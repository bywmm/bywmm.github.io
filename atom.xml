<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>bywmm&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://bywmm.github.io/"/>
  <updated>2019-10-24T03:40:22.240Z</updated>
  <id>https://bywmm.github.io/</id>
  
  <author>
    <name>JF Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>单人检测：stacked hourglass network</title>
    <link href="https://bywmm.github.io/2019/10/24/%E5%8D%95%E4%BA%BA%E6%A3%80%E6%B5%8B-stacked-hourglass-network/"/>
    <id>https://bywmm.github.io/2019/10/24/单人检测-stacked-hourglass-network/</id>
    <published>2019-10-24T00:52:41.000Z</published>
    <updated>2019-10-24T03:40:22.240Z</updated>
    
    <content type="html"><![CDATA[<h1 id="单人检测：stacked-hourglass-network"><a href="#单人检测：stacked-hourglass-network" class="headerlink" title="单人检测：stacked hourglass network"></a>单人检测：stacked hourglass network</h1><p>文章链接：<a href="https://arxiv.org/abs/1603.06937" target="_blank" rel="noopener">https://arxiv.org/abs/1603.06937</a></p><p>文章提出一种网络结构stacked hourglass network，来处理单人的关键点估计问题。</p><p>如下图所示，该网络通过不断堆叠hourglass结构，多次重复bottom-up，top-down的过程。</p><p><img src="/2019/10/24/单人检测-stacked-hourglass-network/stacked-hourglass-network.png"></p><p>图中，每一个形为沙漏的单个模型，称其为hourglass。</p><p>下文将针对几个问题进行分析：</p><ol><li>为什么要用hourglass这种结构</li><li>为什么要堆叠多个hourglass</li><li>多个stage如何进行监督学习</li></ol><h2 id="hourglass"><a href="#hourglass" class="headerlink" title="hourglass"></a>hourglass</h2><p>如上图中，每个hourglass的结构都形如下图。</p><p><img src="/2019/10/24/单人检测-stacked-hourglass-network/hourglass.png"></p><p>图中每一个box都对应一个残差块。</p><p><img src="/2019/10/24/单人检测-stacked-hourglass-network/resblock.png" width="300"></p><p><strong>hourglass的设计是为了获取到多个分辨率下的特征</strong>。我们需要给关键点进行定位(high resolution)；需要对身体有一个整体的理解，理解哪些特征是关键点(low resolution)。也就是说，<strong>我们既需要高层的语义特征来进行检测，也需要底层的高分辨率来进行定位。</strong></p><h3 id="hourglass模型"><a href="#hourglass模型" class="headerlink" title="hourglass模型"></a>hourglass模型</h3><p>hourglass的设计非常简单。</p><p><strong>bottom-up:</strong> 左边是输入是$64\times{64}$的特征图（成为$l_1$层），通过四次降采样(max pooling)变为$4\times4$的特征（称为$l_8$层），通过卷积和池化操作学习到低分辨率下的特征。</p><p><strong>left-right:</strong> 左边学到的最高层的特征$l_8$通过两次$1\times1$的残差块卷积得到右边的最高层特征$r_8$</p><p><strong>top-down:</strong> 然后每次通过对之前学习到的特征进行上采样并于左边的结果进行结合。例如右边最高层的特征$r_8$为$4\times4$的，通过一次上采样的得到$r’_4$为$8\times8$，$r’_4$与$l_4$进行conbine得到$r_4$。</p><p>最终得到的$r_1$为$64\times64$既可作为结果进行预测，也可作为输入进行下一轮的hourglass。</p><h3 id="hourglass理解"><a href="#hourglass理解" class="headerlink" title="hourglass理解"></a>hourglass理解</h3><p>hourglass的模型与<strong>特征金字塔网络</strong>模型类似，想要具有丰富语义的高层特征，又想要具有高分辨率的底层特征。</p><p>因为hourgalss是将上采样的高层特征与原来的特征图向结合，所以进行堆叠的时候，理论上可以使hourglass的输出与输入相等。也就是说如果不能学习到更好的结果的话，至少不会更差。这种设计思想又与<strong>残差网络</strong>类似。</p><h2 id="stacked-hourglass-multi-stages"><a href="#stacked-hourglass-multi-stages" class="headerlink" title="stacked-hourglass(multi-stages)"></a>stacked-hourglass(multi-stages)</h2><p>这种多个stage的方法，其他姿势估计方法也经常会使用，例如openpose，这些方法在多个迭代阶段和中间监督下均表现出出色的性能。</p><p>后续的bottom-up,top-down stages可以对特征进行更深刻的学习。</p><p>原文是使用了8个stage作为最后的模型。</p><h2 id="supervision"><a href="#supervision" class="headerlink" title="supervision"></a>supervision</h2><p>在每个stage中都进行中间监督，使用相同的ground-true计算损失。</p><p>下图是一个完整的stage的结构。</p><p>在经过一个hourglass之后，得到了一个$64\times64$的特征，使用一个$1\times1$的卷积，得到该stage的head。再对head使用一个$1\times1$的卷积得到$64\times64\times{num_class}$的heatmap，即途中蓝色部分。使用该heatmap与gt可以计算损失。</p><p>我们看到，输出的heatmap有使用$1\times1$的卷积对维度进行处理，与原来的head分支进行合并，作为下一次hourglass的输入再进行学习。</p><p><img src="/2019/10/24/单人检测-stacked-hourglass-network/supervision.png" width="500"></p><h2 id="个人实验"><a href="#个人实验" class="headerlink" title="个人实验"></a>个人实验</h2><p><img src="/2019/10/24/单人检测-stacked-hourglass-network/diff_stack.jpg" width="450"></p>]]></content>
    
    <summary type="html">
    
      单人姿态估计算法
    
    </summary>
    
      <category term="pose estimation" scheme="https://bywmm.github.io/categories/pose-estimation/"/>
    
    
      <category term="姿态估计" scheme="https://bywmm.github.io/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Evaluation measure of pose estimation</title>
    <link href="https://bywmm.github.io/2019/10/19/evaluation-measure-of-pose-estimation/"/>
    <id>https://bywmm.github.io/2019/10/19/evaluation-measure-of-pose-estimation/</id>
    <published>2019-10-19T01:38:19.000Z</published>
    <updated>2019-10-24T03:39:30.120Z</updated>
    
    <content type="html"><![CDATA[<h1 id="姿态估计的评估标准"><a href="#姿态估计的评估标准" class="headerlink" title="姿态估计的评估标准"></a>姿态估计的评估标准</h1><h2 id="单人检测"><a href="#单人检测" class="headerlink" title="单人检测"></a>单人检测</h2><p>单人姿态估计的评估标准。</p><h3 id="PCK"><a href="#PCK" class="headerlink" title="PCK"></a>PCK</h3><p>PCK(Percentage of Correct Keypoints)正确关键点的比例。</p><p>PCK的思想是，关键点坐标pred与groundtrue之间的<strong>归一化距离</strong>小于一定阈值时，视为正确估计，以正确估计的关键点的比例作为评估标准。</p><p>从定义可以看出，PCk的变量有两个：</p><ol><li>如何归一化</li><li>阈值是多少</li></ol><p>以<strong>PCKh</strong>为例，PCKh采用头部长度（head segment length）作为归一化参考。</p><p>即：对于每个人</p><ol><li>计算所有关键点pred与groundtrue之间的欧氏距离pg_length；</li><li>计算<strong>头部长度</strong>head_length；</li><li>计算归一化距离norm_length=pg_lenght/head_length；</li><li>当这个距离小于规定的阈值时，比如说0.5，则认为估计正确；否则认为错误。</li></ol><p>参考代码：<a href="https://github.com/yuanyuanli85/Stacked_Hourglass_Network_Keras/blob/master/src/eval/pckh.py" target="_blank" rel="noopener">stacked hourglass network - pckh.py</a></p>]]></content>
    
    <summary type="html">
    
      2D人体姿态估计的评估标准
    
    </summary>
    
      <category term="pose estimation" scheme="https://bywmm.github.io/categories/pose-estimation/"/>
    
    
      <category term="姿态估计" scheme="https://bywmm.github.io/tags/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书笔记6 支持向量机</title>
    <link href="https://bywmm.github.io/2019/10/07/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B06/"/>
    <id>https://bywmm.github.io/2019/10/07/西瓜书笔记6/</id>
    <published>2019-10-07T13:09:37.000Z</published>
    <updated>2019-10-24T03:44:15.582Z</updated>
    
    <content type="html"><![CDATA[<h1 id="西瓜书笔记6-支持向量机"><a href="#西瓜书笔记6-支持向量机" class="headerlink" title="西瓜书笔记6 支持向量机"></a>西瓜书笔记6 支持向量机</h1><h2 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h2><p>存在多个划分超平面将两类训练样本分开</p><p>在样本空间中，划分超平面可通过如下线性方程来描述</p><script type="math/tex; mode=display">\boldsymbol{\omega}^{T}\boldsymbol{x}+b=0</script><p>其中$\boldsymbol{\omega} = (\omega_1;\omega_2;…;\omega_d)$为法向量，决定了超平面的方向；b是位移项，决定了超平面与原点之间的距离。</p><p><strong>样本空间中的任意点$x$到超平面$(\boldsymbol{\omega},b)$的距离：</strong></p><script type="math/tex; mode=display">r = \frac{|\boldsymbol{\omega}^{T}\boldsymbol{x}+b|}{||\boldsymbol{\omega}||}</script><p>假设超平面$(\boldsymbol{\omega},b)$能将训练样本<strong>正确分类</strong>，即对于样本$(\boldsymbol{x}_i,y_i)\in D$，若标签$y_i=+1$，则有$\boldsymbol{\omega}^{T}\boldsymbol{x}_i+b&gt;0$；若$y_i=-1$，则有$\boldsymbol{\omega}^{T}\boldsymbol{x}_i+b&lt;0$。</p><p>进一步可以令：</p><script type="math/tex; mode=display">\left\{\begin{matrix} \boldsymbol{\omega}^{T}\boldsymbol{x}_i+b\geq+1,&y_i=+1; \\  \boldsymbol{\omega}^{T}\boldsymbol{x}_i+b\leq-1,&y_i=-1\end{matrix}\right.</script><blockquote><p>若超平面$(\boldsymbol{\omega}’,b’)$能将训练样本正确分类，则总存在缩放变换$\varsigma\boldsymbol{\omega}\mapsto\boldsymbol{\omega}’$和$\varsigma b\mapsto b’$使得上式成立。</p></blockquote><p>距离超平面最近的几个样本点使上式等号成立，称之为“<strong>支持向量</strong>”。</p><p>两个异类支持向量到超平面的距离之和，称之为“<strong>间隔</strong>”</p><script type="math/tex; mode=display">\gamma=\frac{2}{||\boldsymbol{\omega}||}</script><p>我们的目标就是在确保超平面能够正确分类的同时，最大化间隔：</p><script type="math/tex; mode=display">\begin{matrix}\underset{\omega,b}{max} \frac{2}{||\boldsymbol{\omega}||}& \\ s.t.\ y_i(\boldsymbol{\omega}^T\boldsymbol{x}_i+b)\geq1,&i=1,2,...,m\end{matrix}</script><p>等价于</p><script type="math/tex; mode=display">\begin{matrix}\underset{\omega,b}{min} \frac{1}{2}||\boldsymbol{\omega}||^{2}& \\ s.t.\ y_i(\boldsymbol{\omega}^T\boldsymbol{x}_i+b)\geq1,&i=1,2,...,m\end{matrix}</script><p>这就是支持向量机(Support Vector Machine)的基本型。</p><h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>上式是一个凸二次规划问题。对其使<strong>用拉格朗日乘子法</strong>可得到其“<strong>对偶问题</strong>”。</p><p>该问题的拉格朗日函数可写为</p><script type="math/tex; mode=display">L(\boldsymbol{\omega},b,\boldsymbol{\alpha})=\frac{1}{2}||\boldsymbol{\omega}||^2+\sum_{i=1}^{m}\alpha_i(1-y_i(\boldsymbol{\omega}^T\boldsymbol{x}_i+b))</script><p>其中$\boldsymbol{\alpha}=(\alpha_1;\alpha_2;…;\alpha_m),\alpha_i\geq0$。</p><p>由于引入了拉格朗日乘子，我们的优化目标变为</p><script type="math/tex; mode=display">\underset{\boldsymbol{w},b}{\min}\;\underset{\boldsymbol{\alpha}}{\max}L(\boldsymbol{\omega},b,\boldsymbol{\alpha})</script><p>转化成对偶问题，现在我们要解决的是</p><script type="math/tex; mode=display">\underset{\boldsymbol{\alpha}}{\max}\;\underset{\boldsymbol{w},b}{\min}L(\boldsymbol{\omega},b,\boldsymbol{\alpha})</script><p>首先我们来求令$L(\boldsymbol{\omega},b,\boldsymbol{\alpha})$基于$\boldsymbol{\omega}$和b的最小值，即$\underset{\boldsymbol{w},b}{\min}L(\boldsymbol{\omega},b,\boldsymbol{\alpha})$。这个值，我们可以对$\boldsymbol{\omega}$和b分别求偏导数得到</p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{\boldsymbol{\omega}}}=0\Rightarrow \boldsymbol{\omega}=\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{b}}=0\Rightarrow 0=\sum_{i=1}^m\alpha_iy_i</script><p>将上式带入$L(\boldsymbol{\omega}, b,\boldsymbol{\alpha})$，可得</p><script type="math/tex; mode=display">\underset{\boldsymbol{\alpha}}{\max}\sum_{i=1}^{m}\alpha_i-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j\\s.t.\ \sum_{i=1}^m\alpha_iy_i=0\\\alpha_i\geq0</script><p>解出$\alpha$后，求出$\boldsymbol{\omega}$与b即可得到模型</p><script type="math/tex; mode=display">f(\boldsymbol{x})=\boldsymbol{\omega}^T\boldsymbol{x}+b=\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i^T\boldsymbol{x}+b</script><h2 id="软间隔与正则化"><a href="#软间隔与正则化" class="headerlink" title="软间隔与正则化"></a>软间隔与正则化</h2><p>回顾一下硬间隔支持向量机，最大化的目标：</p><script type="math/tex; mode=display">\underset{\boldsymbol{\omega},b}{\min}\frac{1}{2}||\boldsymbol{\omega}||^2\\s.t.\ y_i(\boldsymbol{\omega}^T\boldsymbol{x}_i+b)\geq1</script><p>“<strong>软间隔</strong>”：允许支持向量机在<strong>一些样本</strong>上出错，即允许某些样本不满足约束$y_i(\boldsymbol{\omega}^T\boldsymbol{x}_i+b)\geq1$。</p><p>对每个样本$(\boldsymbol{x}_i,y_i)$引入一个<strong>松弛变量</strong>$\xi_i\geq0$，使函数间隔加上松弛变量大于等于1，即</p><script type="math/tex; mode=display">y_i(\boldsymbol{\omega}^T\boldsymbol{x}_i+b)\geq1-\xi_i</script><h3 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h3><p>对比硬间隔最大化，可以看到我们对样本到超平面的函数距离的要求放松了，之前是一定要大于等于1，现在只需要加上一个大于等于0的松弛变量能大于等于1就可以了。当然，松弛变量不能白加，这是有成本的，每一个松弛变量$\xi_i$, 对应了一个损失$\xi_i$，这个就得到了我们的软间隔最大化的SVM学习条件如下</p><script type="math/tex; mode=display">\underset{\boldsymbol{\omega},b,\xi_i}{\min}\frac{1}{2}||\boldsymbol{\omega}||^2+C\sum_{i=1}^{m}\xi_i\\s.t.\ y_i(\boldsymbol{\omega}^T\boldsymbol{x}_i+b)\geq1-\xi_i\\\xi_i\geq0</script><p>这里，C&gt;0为惩罚参数，可以理解为我们一般回归和分类问题正则化时候的参数。C越大，对误分类的惩罚越大，C越小，对误分类的惩罚越小。</p><h3 id="最大化函数的优化"><a href="#最大化函数的优化" class="headerlink" title="最大化函数的优化"></a>最大化函数的优化</h3><p>首先将软间隔最大化的约束问题用拉格朗日函数转化为无约束问题</p><script type="math/tex; mode=display">L(\boldsymbol{\omega},b,\boldsymbol{\alpha}, \boldsymbol{\xi},\boldsymbol{\mu})=\frac{1}{2}||\boldsymbol{\omega}||^2+C\sum_{i=1}^{m}\xi_i\\+\sum_{i=1}^{m}\alpha_i(1-\xi_i-y_i(\boldsymbol{\omega}^T\boldsymbol{x}_i+b))-\sum_{i=1}^{m}\mu_i\xi_i</script><p>其中$\alpha_i\geq0,\mu_i\geq0$是拉格朗日乘子</p><p>现在要求得是</p><script type="math/tex; mode=display">\underset{\boldsymbol{\omega},b,\boldsymbol{\xi}}{\min}\;\underset{\boldsymbol{\alpha},\boldsymbol{\mu}}{\max}L(\boldsymbol{\omega},b,\boldsymbol{\alpha}, \boldsymbol{\xi},\boldsymbol{\mu})</script><p>同样转换成等价的对偶问题：</p><script type="math/tex; mode=display">\underset{\boldsymbol{\alpha},\boldsymbol{\mu}}{\max}\;\underset{\boldsymbol{\omega},b,\boldsymbol{\xi}}{\min}L(\boldsymbol{\omega},b,\boldsymbol{\alpha}, \boldsymbol{\xi},\boldsymbol{\mu})</script><p>首先求$\underset{\boldsymbol{\omega},b,\boldsymbol{\xi}}{\min}L(\boldsymbol{\omega},b,\boldsymbol{\alpha}, \boldsymbol{\xi},\boldsymbol{\mu})$，可以通过求偏导数求得</p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{\boldsymbol{\omega}}}=0\Rightarrow \boldsymbol{\omega}=\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{b}}=0\Rightarrow \sum_{i=1}^{m}\alpha_iy_i=0</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{\boldsymbol{\xi}}}=0\Rightarrow \alpha_i+\mu_i=C</script><p>带入对偶问题式可得</p><script type="math/tex; mode=display">\underset{\boldsymbol{\alpha}}{\max}\sum_{i=1}^{m}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j\\s.t. \sum_{i=1}^{m}\alpha_iy_i=0\\0\leq\alpha_i\leq C</script><hr><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>有些训练样本是线性不可分的，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分。<font color="red">（至于为什么，以后再证，参见12章？）</font></p><p>令$\phi(\boldsymbol{x})$表示将$\boldsymbol{x}$映射后的特征向量。在特征空间中划分超平面对应的模型可表示为</p><script type="math/tex; mode=display">f(\boldsymbol{x})=\boldsymbol{\omega}^T\phi(\boldsymbol{x})+b</script><p>SVM的优化目标函数变为</p><script type="math/tex; mode=display">\underset{\boldsymbol{\alpha}}{\min}\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)-\sum_{i=1}^{m}\alpha_i\\s.t.\sum_{i=1}^{m}\alpha_iy_i=0\\0\leq\alpha_i\leq C</script><p>上式中涉及到计算$\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)$，这是样本映射后的内积。由于映射后的特征空间维数可能很高，计算量太大；甚至是无穷维，无法计算。这时候，就需要我们的核函数了。设想有这么一个函数</p><script type="math/tex; mode=display">K(\boldsymbol{x}_i,\boldsymbol{x}_j)=\phi(\boldsymbol{x_i})^T\phi(\boldsymbol{x_j})</script><p>核函数在低维上进行计算，而将实质上的分类效果（利用了内积）表现在了高维上，这样避免了直接在高维空间中的复杂计算，真正解决了SVM的线性不可分问题。</p><h4 id="有哪些核函数呢"><a href="#有哪些核函数呢" class="headerlink" title="有哪些核函数呢"></a>有哪些核函数呢</h4><p><strong>定理（核函数）</strong></p><p>令$\chi$为输入空间，$\kappa(\cdot,\cdot)$是定义在$\chi\times\chi$上的对称函数，则$\kappa$是核函数当且仅当对于任意数据$D=\left \{\boldsymbol{x}_1,\boldsymbol{x}_2,…,\boldsymbol{x}_m\right \}$，“核矩阵”$\boldsymbol{K}$总是半正定的。</p><p>定理表明，<strong>只要是一个对称函数对应的核矩阵半正定，它就能作为核函数使用</strong>。</p><p>常用的几种核函数</p><div class="table-container"><table><thead><tr><th>名称</th><th>表达式</th><th>参数</th></tr></thead><tbody><tr><td>线性核</td><td>$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\boldsymbol{x}_i^T\boldsymbol{x}_j$</td></tr><tr><td>多项式核</td><td>$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=(\boldsymbol{x}_i^T\boldsymbol{x}_j)^d$`</td><td>`$d\geq1$为多项式的次数</td></tr><tr><td>高斯核(RBF核)</td><td>$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\exp(-\frac{\boldsymbol{x}_i-\boldsymbol{x}_j^2}{2\sigma^2})$</td><td>$\sigma&gt;0$为高斯核带宽</td></tr><tr><td>拉普拉斯核</td><td>$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\exp(-\frac{\boldsymbol{x}_i-\boldsymbol{x}_j}{\sigma})$</td><td>$\sigma&gt;0$</td></tr><tr><td>Sigmoid核</td><td>$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\tanh(\beta\boldsymbol{x}_i^T\boldsymbol{x}_j+\theta)$</td><td>$\beta&gt;0,\theta&gt;0$</td></tr></tbody></table></div><p>此外，还可通过函数组合得到</p><ul><li>若$\kappa_1$和$\kappa_2$为核函数，则对于任意正数$\gamma_1$、$\gamma_2$，其线性组合也是核函数</li></ul><script type="math/tex; mode=display">\gamma_1\kappa_1+\gamma_2\kappa_2</script><ul><li>若$\kappa_1$和$\kappa_2$为核函数，则核函数的直积也是核函数<font color="red">（这里不太懂，直积不是集合运算吗？）</font></li></ul><script type="math/tex; mode=display">\kappa_1\otimes \kappa_2(\boldsymbol{x},\boldsymbol{z})=\kappa_1(\boldsymbol{x},\boldsymbol{z})\kappa_2(\boldsymbol{x},\boldsymbol{z})</script><ul><li>若$\kappa_1$为核函数，则对于任意函数$g(\boldsymbol{x})$，$\kappa$也是核函数</li></ul><script type="math/tex; mode=display">\kappa(\boldsymbol{x},\boldsymbol{z})=g(\boldsymbol{x})\kappa_1(\boldsymbol{x},\boldsymbol{z})g(\boldsymbol{z})</script><hr><h2 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法*"></a>SMO算法*</h2><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>目标函数：</p><script type="math/tex; mode=display">\underset{\boldsymbol{\alpha}}{\min}\;\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jK(\boldsymbol{x}_i,\boldsymbol{x}_j)-\sum_{i=1}^{m}\alpha_i\\s.t.\sum_{i=1}^{m}\alpha_iy_i=0\\0\leq\alpha_i\leq C</script><p>==解要满足的KKT条件的对偶互补条件为==</p><script type="math/tex; mode=display">\alpha_i^*(y_i(\boldsymbol{\omega}^T\phi(\boldsymbol{x}_i)+b)-1+\xi_i^*)=0</script><p>即</p><script type="math/tex; mode=display">\alpha_i^*=0\Rightarrow y_i(\boldsymbol{\omega}^T\phi(\boldsymbol{x}_i)+b)\geq1\\0<\alpha_i^*<C\Rightarrow y_i(\boldsymbol{\omega}^T\phi(\boldsymbol{x}_i)+b)=1\\\alpha_i^*=C\Rightarrow y_i(\boldsymbol{\omega}^T\phi(\boldsymbol{x}_i)+b)\leq1</script><p>由于$\boldsymbol{\omega}=\sum_{i=1}^{m}\alpha_i^<em>y_i\phi(\boldsymbol{x}_i)$。$f(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_i^</em>y_iK(\boldsymbol{x},\boldsymbol{x_i})+b$,则有</p><script type="math/tex; mode=display">\alpha_i^*=0\Rightarrow y_if(\boldsymbol{x}_i)\geq1\\0<\alpha_i^*<C\Rightarrow y_if(\boldsymbol{x}_i)=1\\\alpha_i^*=C\Rightarrow y_if(\boldsymbol{x}_i)\leq1</script><h3 id="SMO基本思想"><a href="#SMO基本思想" class="headerlink" title="SMO基本思想"></a>SMO基本思想</h3><p>上面的优化式子比较复杂。是一个二次规划问题，可以使用通用的二次规划算法来求解。优化时间正比于训练样本数m，所以直接优化很难。</p><p>SMO算法，采用了一种<strong>启发式</strong>方法。它每次只选择两个变量，其他变量视为常数（固定）。由于存在约束$\sum_{i=1}^{m}\alpha_iy_i=0$，假如将$\alpha_3,\alpha4,…,\alpha_m$固定，那么$\alpha_1,\alpha_2$之间的关系也确定了。</p><p>SMO不断执行如下两个步骤直至收敛</p><blockquote><ul><li>选取一对需要更新的$\alpha_i$和$\alpha_j$</li><li>固定$\alpha_i$和$\alpha_j$以外的参数，求解目标函数式获得更新后的$\alpha_i$和$\alpha_j$</li></ul></blockquote><h3 id="SMO算法细节"><a href="#SMO算法细节" class="headerlink" title="SMO算法细节"></a>SMO算法细节</h3><h4 id="两个变量的选择"><a href="#两个变量的选择" class="headerlink" title="两个变量的选择"></a>两个变量的选择</h4><p>不妨记$\alpha_1,\alpha_2$为选择的两个变量</p><h5 id="1-第一个变量的选择"><a href="#1-第一个变量的选择" class="headerlink" title="1.第一个变量的选择"></a>1.第一个变量的选择</h5><p>SMO算法称选择第一个变量为外层循环，选择在训练集中<strong>违反KKT条件程度最大的样本点</strong>。对于每个样本点，要满足的KKT条件</p><script type="math/tex; mode=display">\alpha_i^*=0\Rightarrow y_if(\boldsymbol{x}_i)\geq1\\0<\alpha_i^*<C\Rightarrow y_if(\boldsymbol{x}_i)=1\\\alpha_i^*=C\Rightarrow y_if(\boldsymbol{x}_i)\leq1</script><p>一般来说，先选择违反$0&lt;\alpha_i^<em>&lt;C\Rightarrow y_if(\boldsymbol{x}_i)=1$的点。如果这些<em>*支持向量</em></em>都满足KKT条件，在选择其他的</p><h5 id="2-第二个变量的选择"><a href="#2-第二个变量的选择" class="headerlink" title="2.第二个变量的选择"></a>2.第二个变量的选择</h5><p>SMO算法称，选择第二个变量为内层循环，假设外层循环已经找到了$\alpha_1$，第二个变量的选择标准是选择<strong>与其样本间隔最大的点</strong>。<br>为了方便表示，我们定义</p><script type="math/tex; mode=display">E_i=f(\boldsymbol{x_i})-y_i\\=\sum_{i=1}^{m}\alpha_i^*y_iK(\boldsymbol{x},\boldsymbol{x}_i)+b-y_i</script><font color="red">对于E的理解有待更新</font><p>即第二个变量的选择标准是让$|E_1-E_2|$有足够大的变化。由于$\alpha_1$定了的时候$E_1$也定了，所以想要$|E_1-E_2|$最大，只需要在$E_1$为正时，选择最小的$E_i$作为$E_2$，$E_1$为负时，选择最大的$E_i$作为$E_2$，可以将所有的$E_i$保存下来加快迭代。</p><p>==如果内循环找到的点不能让目标函数有足够的下降，可以采用遍历支持向量点来做<code>$\alpha_2$</code>，直至目标函数有足够的下降，如果所有的支持向量做<code>$\alpha_2$</code>都不能让目标函数有足够的下降，可以跳出循环，重新选择<code>$\alpha_1$</code>。==</p><h4 id="两个变量的更新"><a href="#两个变量的更新" class="headerlink" title="两个变量的更新"></a>两个变量的更新</h4><p>定义</p><script type="math/tex; mode=display">K_{ij}=\phi(\boldsymbol{x}_i)\cdot\phi(\boldsymbol{x}_j)</script><p>因为$\alpha_3,\alpha_4,…,\alpha_m$都变成了常量，所以我们的目标优化函数变为</p><script type="math/tex; mode=display">\underset{\alpha_1,\alpha_2}{\min}\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2+y_1y_2K_{12}\alpha_1\alpha_2-(\alpha_1+\alpha_2)+y_1\alpha_1\sum_{i=3}^{m}y_i\alpha_iK_{i1}+y_2\alpha_2\sum_{i=3}^{m}y_i\alpha_iK_{i2}\\s.t.\ \alpha_1y_1+\alpha_2y_2=-\sum_{i=3}^{m}y_i\alpha_i=\zeta\\0\leq\alpha_i\leq C \ i=1,2</script><p>现在的问题是，求解上面含有这两个变量的目标优化问题。</p><p>首先分析约束条件，所有的$\alpha_1,\alpha_1$都要满足约束条件，然后在约束条件下求最小。</p><ol><li>要满足的约束条件</li></ol><p>已知约束条件：</p><script type="math/tex; mode=display">\left\{\begin{matrix}\alpha_1y_1+\alpha_2y_2=\zeta\\ 0\leq\alpha_1\leq C\\0\leq\alpha_2\leq C\\|y1|=|y2|=1\end{matrix}\right.</script><p>若$y_1=y_2$,</p><script type="math/tex; mode=display">\because\alpha_1y_1+\alpha_2y_2=\zeta\\\therefore\alpha_1+\alpha_2=y_1\zeta\\\therefore\alpha_1=y_1\zeta-\alpha_2\\\because\ 0\leq\alpha_1\leq C\\\therefore y_1\zeta-C\leq\alpha_2\leq y_1\zeta\\\because 0\leq\alpha_2\leq C,\alpha_1+\alpha_2=y_1\zeta\\\therefore \max(0, \alpha_1+\alpha_2-C)\leq\alpha_2\leq\min(C,\alpha_1+\alpha_2)</script><p>若$y_1\neq y_2$</p><script type="math/tex; mode=display">\because\alpha_1y_1+\alpha_2y_2=\zeta\\\therefore\alpha_1-\alpha_2=y_1\zeta\\\therefore\alpha_a=y_1\zeta+\alpha_2\\\because 0\leq\alpha_1\leq C\\\therefore -y_1\zeta\leq\alpha_2\leq C-y_1\zeta\\\because 0\leq\alpha_2\leq C,\alpha_1-\alpha_2=y_1\zeta\\\therefore \max(0, \alpha_2-\alpha_1)\leq\alpha_2\leq\min(C,C+\alpha_2-\alpha_1)</script><p>分别用L和H表示$\alpha_2$的上下界约束，则$L\leq\alpha_2\leq H$。也就是说，我们通过求导得到$\alpha_{2}^{new,unc}$,则最终的$\alpha_2^{new}$应该为</p><script type="math/tex; mode=display">\alpha_2^{new}=\left\{\begin{matrix}H&\alpha_2^{new,unc}>H\\ \alpha_2^{new,unc}&L\leq\alpha_2^{new,unc}\leq H\\ L&\alpha_2^{new,unc}<L\end{matrix}\right.</script><ol><li>约束条件下的最优值</li></ol><p>为了简化目标优化函数，我们令</p><script type="math/tex; mode=display">v_i=\sum_{j=3}^{m}y_j\alpha_jK_{ij}=g(\boldsymbol{x}_i)-\sum_{j=1}^2y_j\alpha_jK_{ij}-b</script><p>这样我们的目标优化函数进一步简化为</p><script type="math/tex; mode=display">W(\alpha_1,\alpha_2)=\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2+y_1y_2K_{12}\alpha_1\alpha_2-(\alpha_1+\alpha_2)+y_1\alpha_1v_1+y2\alpha_2v_2</script><p>由于$\alpha_1y_1+\alpha_2y_2=\zeta$，可以得到$\alpha_1$用$\alpha_2$表达的式子为</p><script type="math/tex; mode=display">\alpha_1=y_1(\zeta-\alpha_2y_2)</script><p>将上式代入我们的目标优化函数，就可以消除$\alpha_1$，得到仅包含$\alpha_2$的式子</p><script type="math/tex; mode=display">W(\alpha_2)=\frac{1}{2}K_{11}(\zeta-\alpha_2y_2)^2+\frac{1}{2}K_{22}\alpha_2^2+y_2K_{12}(\zeta-\alpha_2y_2)\alpha_2\\-(y_1(\zeta-\alpha_2y_2)+\alpha_2)+(\zeta-\alpha_2y_2)v_1+y_2\alpha_2v_2</script><p>对W求导数，得到$\alpha_2^{new,unc}$</p><script type="math/tex; mode=display">\frac{\partial{W}}{\partial{\alpha_2}}=K_{11}\alpha_2-K_{11}y_2\zeta+K_{22}\alpha_2+K_{12}y_2\zeta-2K_{12}\alpha_2+y_1y_2-1-v_1y_2+v2y2=0</script><p>整理上式得</p><script type="math/tex; mode=display">(K_{11}+K_{22}-2K_{12})\alpha_2=y_2(y_2-y_1+K_{11}\zeta-K_{12}\zeta+v_1-v_2)\\y_2(y_2-y_1+K_{11}\zeta-K_{12}\zeta+(g(\boldsymbol{x}_1)-\sum_{i=1}^2y_i\alpha_iK_{1i}-b)-(g(\boldsymbol{x}_2)-\sum_{i=1}^2y_i\alpha_iK_{2i}-b)</script><p>将$\zeta=\alpha_1y_1+\alpha_2y_2$带入上式得</p><script type="math/tex; mode=display">(K_{11}+K_{22}-2K_{12})\alpha_2^{new,unc}=y_2((K_{11}+K_{22}-2K_{12})\alpha_2^{old}y_2+y_2-y_1+g(\boldsymbol{x}_1)-g(\boldsymbol{x}_2))\\=(K_{11}+K_{22}-2K_{12})\alpha_2^{old}+y_2(E_1-E_2)</script><p>最终得到$\alpha_2^{new,unc}$的表达式</p><script type="math/tex; mode=display">\alpha_2^{new,unc}=\alpha_2^{old}+\frac{y_2(E_1-E_2)}{K_{11}+K_{22}-2K_{12}}</script><p>求得$\alpha_2^{new}$后，利用$\alpha_2^{new}$与$\alpha_1^{new}$的线性关系，也可以得到$\alpha_1^{new}$</p><h4 id="计算阈值b和差值-E-i"><a href="#计算阈值b和差值-E-i" class="headerlink" title="计算阈值b和差值$E_i$"></a>计算阈值b和差值$E_i$</h4><h4 id="SMO算法总结"><a href="#SMO算法总结" class="headerlink" title="SMO算法总结"></a>SMO算法总结</h4><p>输入m个样本<code>$(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2),...,(\boldsymbol{x}_m,y_m)$</code>，其中<code>$x_i$</code>为n维特征向量，y为二元输出，值为1或-1。</p><ol><li>取初值<code>$\boldsymbol{\alpha}^0=0,k=0$</code></li><li><p>按上述方法选择<code>$\alpha_1^k$</code>和<code>$\alpha_2^k$</code>，求出新的<code>$\alpha_2^{new,unc}$</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\alpha_2^&#123;new,unc&#125;=\alpha_2^k+\frac&#123;y_2(E_1-E_2)&#125;&#123;K_&#123;11&#125;+K_&#123;22&#125;-2K_&#123;12&#125;&#125;</span><br></pre></td></tr></table></figure></li><li><p>按照下式求出<code>$\alpha_2^{k+1}$</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\alpha_2^&#123;k+1&#125;=\left\&#123;\begin&#123;matrix&#125;</span><br><span class="line">H&amp;\alpha_2^&#123;new,unc&#125;&gt;H\\ </span><br><span class="line">\alpha_2^&#123;new,unc&#125;&amp;L\leq\alpha_2^&#123;new,unc&#125;\leq H\\ </span><br><span class="line">L&amp;\alpha_2^&#123;new,unc&#125;&lt;L</span><br><span class="line">\end&#123;matrix&#125;\right.</span><br></pre></td></tr></table></figure></li><li><p>利用<code>$\alpha_2^{k+1}$</code>和<code>$\alpha_1^{k+1}$</code>的线性关系求出<code>$\alpha_1^{k+1}$</code></p></li><li>按上述方法计算<code>$b^{k+1}$</code>和<code>$E_i$</code></li><li><p>在精度e范围内检查是否满足如下的终止条件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">\sum_&#123;i=1&#125;^m\alpha_iy_i=0</span><br><span class="line"></span><br><span class="line">0\leq\alpha_i\leq C\ i=1,2,...,m</span><br><span class="line"></span><br><span class="line">\alpha_i^&#123;k+1&#125;=0\Rightarrow y_if(\boldsymbol&#123;x&#125;_i)\geq1</span><br><span class="line"></span><br><span class="line">0&lt;\alpha_i^&#123;k+1&#125;&lt;C\Rightarrow y_if(\boldsymbol&#123;x&#125;_i)=1</span><br><span class="line"></span><br><span class="line">\alpha_i^&#123;k+1&#125;=C\Rightarrow y_if(\boldsymbol&#123;x&#125;_i)\leq1</span><br></pre></td></tr></table></figure></li><li><p>如满足则结束，返回<code>$\boldsymbol{\alpha}^{k+1}$</code>，否则转到步骤2</p></li></ol><h2 id="支持向量机回归"><a href="#支持向量机回归" class="headerlink" title="支持向量机回归"></a>支持向量机回归</h2><p>考虑一个回归问题，给定训练样本<code>$D=\left\{(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2),...,(\boldsymbol{x}_m,y_m)\right\},y_i\epsilon\mathbb{R}$</code>，希望学得一个形如下式的回归模型，使得<code>$f(\boldsymbol{x})$</code>与<code>$y$</code>尽可能接近，<code>$\boldsymbol{\omega}$</code>和<code>$b$</code>是待确定的模型参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(\boldsymbol&#123;x&#125;)=\boldsymbol&#123;\omega&#125;^T\boldsymbol&#123;x&#125;+b</span><br></pre></td></tr></table></figure><ul><li><strong>传统回归模型</strong>：通常直接基于模型输出<code>$f(\boldsymbol{x})$</code>与真是输出<code>$y$</code>之间的差别来计算损失，当且仅当<code>$f(\boldsymbol{x})$</code>与<code>$y$</code>完全相同时，损失才为零。</li><li><strong>支持向量回归</strong>：可以容忍<code>$f(\boldsymbol{x})$</code>与<code>$y$</code>之间最多有<code>$\epsilon$</code>的偏差，即仅当<code>$f(\boldsymbol{x})$</code>与<code>$y$</code>之间的差别绝对值大约<code>$\epsilon$</code>时才计算损失。</li></ul><p>SVR问题可形式化为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\underset&#123;\boldsymbol&#123;\omega&#125;,b&#125;&#123;\min&#125;\frac&#123;1&#125;&#123;2&#125;||\boldsymbol&#123;\omega&#125;||^2+C\sum_&#123;i=1&#125;^&#123;m&#125;\ell_&#123;\epsilon&#125;(f(\boldsymbol&#123;x&#125;_i)-y_i)</span><br></pre></td></tr></table></figure><p>其中C为正则化常数，<code>$\ell_{\epsilon}$</code>是<code>$\epsilon-$</code>不敏感损失函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\ell_&#123;\epsilon&#125;(z)=\left\&#123;\begin&#123;matrix&#125;</span><br><span class="line">0,&amp;if|z|\leq\epsilon\\ </span><br><span class="line">|z|-\epsilon,&amp;othervise</span><br><span class="line">\end&#123;matrix&#125;\right.</span><br></pre></td></tr></table></figure><p>引入松弛变量<code>$\xi_i$</code>和<code>$\widehat{\xi}_i$</code>（间隔带两侧的松弛程度可有所不同，所以有两个松弛变量）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\underset&#123;\boldsymbol&#123;\omega&#125;,b,\xi_i,\widehat&#123;\xi&#125;_i&#125;&#123;\min&#125;\frac&#123;1&#125;&#123;2&#125;||\boldsymbol&#123;\omega&#125;||^2+C\sum_&#123;i=1&#125;^m(\xi_i,\widehat&#123;\xi&#125;_i)</span><br><span class="line"></span><br><span class="line">s.t.\ -\epsilon-\widehat&#123;\xi&#125;_i\leq f(\boldsymbol&#123;x&#125;_i)-y_i\leq\epsilon+\xi_i</span><br><span class="line"></span><br><span class="line">\xi_i\geq0,\widehat&#123;\xi&#125;_i\geq0,i=1,2,...,m</span><br></pre></td></tr></table></figure><h3 id="目标函数对偶形式"><a href="#目标函数对偶形式" class="headerlink" title="目标函数对偶形式"></a>目标函数对偶形式</h3><p>引入拉格朗日乘子<code>$\mu_i\geq0,\hat{\mu}_i\geq0,\alpha_i\geq0,\hat{\alpha}_i\geq0$</code>，由拉格朗日乘子法可得拉格朗日函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">L(\boldsymbol&#123;\omega&#125;,b,\boldsymbol&#123;\alpha&#125;,\boldsymbol&#123;\hat&#123;\alpha&#125;&#125;,\boldsymbol&#123;\xi&#125;,\boldsymbol&#123;\hat&#123;\xi&#125;&#125;,\boldsymbol&#123;\mu&#125;,\boldsymbol&#123;\hat&#123;\mu&#125;&#125;)</span><br><span class="line"></span><br><span class="line">=\frac&#123;1&#125;&#123;2&#125;||\boldsymbol&#123;\omega&#125;||^2+C\sum_&#123;i=1&#125;^m(\xi_i+\hat&#123;\xi&#125;_i)-\sum_&#123;i=1&#125;^m\mu_i\xi_i-\sum_&#123;i=1&#125;^m\hat&#123;\mu&#125;_i\hat&#123;\xi&#125;_i</span><br><span class="line"></span><br><span class="line">+\sum_&#123;i=1&#125;^m\alpha_i(f(\boldsymbol&#123;x&#125;_i-y_i-\epsilon-\xi_i))+\sum_&#123;i=1&#125;^m\hat&#123;\alpha&#125;_i(y_i-f(\boldsymbol&#123;x&#125;_i)-\epsilon-\hat&#123;\xi&#125;_i)</span><br></pre></td></tr></table></figure></p><p>我们的优化目标是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\underset&#123;\boldsymbol&#123;\omega&#125;,b,\boldsymbol&#123;\xi&#125;,\boldsymbol&#123;\hat&#123;\xi&#125;&#125;&#125;&#123;\min&#125;\underset&#123;\boldsymbol&#123;\mu&#125;,\boldsymbol&#123;\hat&#123;\mu&#125;&#125;,\boldsymbol&#123;\alpha&#125;,\boldsymbol&#123;\hat&#123;\alpha&#125;&#125;&#125;&#123;\max&#125;L(\boldsymbol&#123;\omega&#125;,b,\boldsymbol&#123;\alpha&#125;,\boldsymbol&#123;\hat&#123;\alpha&#125;&#125;,\boldsymbol&#123;\xi&#125;,\boldsymbol&#123;\hat&#123;\xi&#125;&#125;,\boldsymbol&#123;\mu&#125;,\boldsymbol&#123;\hat&#123;\mu&#125;&#125;)</span><br></pre></td></tr></table></figure><p>和SVM分类模型一样，这个优化目标也满足KKT条件，也就是说，我们可以通过拉格朗日对偶将我们的优化问题转化为等价的对偶问题来求解如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\underset&#123;\boldsymbol&#123;\mu&#125;,\boldsymbol&#123;\hat&#123;\mu&#125;&#125;,\boldsymbol&#123;\alpha&#125;,\boldsymbol&#123;\hat&#123;\alpha&#125;&#125;&#125;&#123;\max&#125;\underset&#123;\boldsymbol&#123;\omega&#125;,b,\boldsymbol&#123;\xi&#125;,\boldsymbol&#123;\hat&#123;\xi&#125;&#125;&#125;&#123;\min&#125;L(\boldsymbol&#123;\omega&#125;,b,\boldsymbol&#123;\alpha&#125;,\boldsymbol&#123;\hat&#123;\alpha&#125;&#125;,\boldsymbol&#123;\xi&#125;,\boldsymbol&#123;\hat&#123;\xi&#125;&#125;,\boldsymbol&#123;\mu&#125;,\boldsymbol&#123;\hat&#123;\mu&#125;&#125;)</span><br></pre></td></tr></table></figure></p><p>首先通过求偏导数，求关于<code>$\boldsymbol{\omega},b,\boldsymbol{\xi},\boldsymbol{\hat{\xi}}$</code>的极小值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\frac&#123;\partial&#123;L&#125;&#125;&#123;\partial&#123;\boldsymbol&#123;\omega&#125;&#125;&#125;=0\Rightarrow\boldsymbol&#123;\omega&#125;=\sum_&#123;i=1&#125;^m(\hat&#123;\alpha_i&#125;-\alpha_i)\boldsymbol&#123;x&#125;_i</span><br><span class="line"></span><br><span class="line">\frac&#123;\partial&#123;L&#125;&#125;&#123;\partial&#123;b&#125;&#125;=0\Rightarrow\sum_&#123;i=1&#125;^m(\hat&#123;\alpha&#125;_i-\alpha_i)=0</span><br><span class="line"></span><br><span class="line">\frac&#123;\partial&#123;L&#125;&#125;&#123;\partial&#123;\xi&#125;_i&#125;=0\Rightarrow\alpha_i+\mu_i=C</span><br><span class="line"></span><br><span class="line">\frac&#123;\partial&#123;L&#125;&#125;&#123;\partial&#123;\hat&#123;\xi&#125;_i&#125;&#125;=0\Rightarrow\hat&#123;\alpha&#125;_i+\hat&#123;\mu&#125;_i=C</span><br></pre></td></tr></table></figure><p>带入，得到对偶问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\underset&#123;\boldsymbol&#123;\alpha&#125;,\boldsymbol&#123;\hat&#123;\alpha&#125;&#125;&#125;&#123;\max&#125;\sum_&#123;i=1&#125;^my_i(\hat&#123;\alpha&#125;_i-\alpha_i)-\epsilon(\hat&#123;\alpha&#125;_i+\alpha_i)</span><br><span class="line"></span><br><span class="line">-\frac&#123;1&#125;&#123;2&#125;\sum_&#123;i=1&#125;^m\sum_&#123;j=1&#125;^m(\hat&#123;\alpha&#125;_i-\alpha_i)(\hat&#123;\alpha&#125;_j-\alpha_j)\boldsymbol&#123;x&#125;_i^T\boldsymbol&#123;x&#125;_j</span><br><span class="line"></span><br><span class="line">s.t.\ \sum_&#123;i=1&#125;^m(\hat&#123;\alpha&#125;_i-\alpha_j)=0</span><br><span class="line"></span><br><span class="line">0\leq\alpha_i,\hat&#123;\alpha&#125;_j\leq&#123;C&#125;</span><br></pre></td></tr></table></figure><p>同样可通过SMO算法解出<code>$\boldsymbol{\alpha},\boldsymbol{\hat{\alpha}}$</code>，SVR的解为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(\boldsymbol&#123;x&#125;)=\sum_&#123;i=1&#125;^m(\hat&#123;\alpha&#125;_i-\alpha_i)\boldsymbol&#123;x&#125;_i^T\boldsymbol&#123;x&#125;+b</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      间隔与支持向量；对偶问题；软间隔与正则化；核函数；SMO算法；支持向量回归；核方法
    
    </summary>
    
      <category term="机器学习" scheme="https://bywmm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="西瓜书笔记" scheme="https://bywmm.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器学习" scheme="https://bywmm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>目标检测backbone</title>
    <link href="https://bywmm.github.io/2019/09/30/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bbackbone/"/>
    <id>https://bywmm.github.io/2019/09/30/目标检测backbone/</id>
    <published>2019-09-30T01:32:51.000Z</published>
    <updated>2019-10-24T03:54:15.210Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目标检测的backbone"><a href="#目标检测的backbone" class="headerlink" title="目标检测的backbone"></a>目标检测的backbone</h1><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>Deeper neural networks are more difficult to train.残差网络的提出，可以训练更深的网络。</p><h3 id="A-building-block"><a href="#A-building-block" class="headerlink" title="A building block"></a>A building block</h3><p>残差块使用了一种”shortcut connection”的链接方式，顾名思义，shortcut就是“抄近道”的意思，看下图我们就能大致理解：</p><p><img src="/2019/09/30/目标检测backbone/building-block.png" width="350"></p><p>真正在使用的残差块并不是这么单一，文章中就提出了两种方式：</p><p><img src="/2019/09/30/目标检测backbone/building-block-examples.png" width="800"></p><p>这两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图）。</p><p>其中右图又称为”<strong>bottleneck design</strong>“，目的就是为了<strong>降低参数的数目</strong>。</p><p>第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632；而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。<br>对于常规building blocks，可以用于34层或者更少的网络中，而bottleneck design通常用于更深的如ResNet101这样的网络中，目的是减少计算和参数量。</p><script type="math/tex; mode=display">y = F(\boldsymbol{x}) + \boldsymbol{x}</script><p>我们看到残差块是通过一条 shortcut connection 保留低层网络学到的特征$\boldsymbol{x}$，和这几层新学的特征$F(\boldsymbol{x})$相加。这样理论上网络最差也不会比低层网络的效果差。</p><script type="math/tex; mode=display">y = F(\boldsymbol{x}) + W_s\boldsymbol{x}</script><p>如果$F(\boldsymbol{x})$和$\boldsymbol{x}$维度不同的话，通过一个$W_s$进行线性的维度映射（mask-rcnn <a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="noopener">keras&amp;tf实现</a>中是用的一个ConvNet）。</p><h3 id="ResNet50和ResNet101"><a href="#ResNet50和ResNet101" class="headerlink" title="ResNet50和ResNet101"></a>ResNet50和ResNet101</h3><p>下表一共列出了5种深度ResNet的architecture。所有的ResNet都分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x，之后的其他论文也会专门用这个称呼指代ResNet50或者101的每部分。</p><p><img src="/2019/09/30/目标检测backbone/architecture-of-ResNet-xx.png"></p><p>很多方法都建立在<strong>ResNet50或者ResNet101</strong>的基础上完成的。这里我们关注50-layer和101-layer这两列，可以发现，它们<strong>唯一的不同在于conv4_x</strong>，ResNet50有6个block，而ResNet101有23个block，相差17个block，也就是17 x 3 = 51层。</p><p>注：101层网络仅仅指卷积或者全连接层，而激活层或者Pooling层并没有计算在内；</p><p><strong>ResNet50的keras实现：<a href="https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py" target="_blank" rel="noopener">https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py</a></strong></p><h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><h3 id="FPN解决了什么问题"><a href="#FPN解决了什么问题" class="headerlink" title="FPN解决了什么问题"></a>FPN解决了什么问题</h3><p>在以往的faster r-cnn进行目标检测的时候，roi都只作用在最后一层。这对于大目标的检测没有问题，但对于小目标的检测就会出现问题，因为随着卷积操作会缩小特征图的尺寸，小目标的语义信息到高层已经不存在了。卷积网络在较低层分辨率高，学习到了较多的细节信息；较高层分辨率低，学习到了较多的语义信息。<br>所以<strong>为了解决多尺度检测的问题，引入了特征金字塔网络</strong>。</p><p><img src="http://static.zybuluo.com/BYWMM/yyaz5su9uchaw68gfpf5kqx0/image_1dl8r5jsf1n0hqsveqa6em1bi39.png"></p><ul><li>图（a）使用了图像金字塔来建立一个特征金字塔。通过不同尺度的图像训练得到不同尺度的特征。慢。</li><li>图（b）CNN</li><li>图（c）SSD较早尝试了使用CNN金字塔形的层级特征。理想情况下，SSD风格的金字塔 重利用了前向过程计算出的来自多层的多尺度特征图，因此这种形式是不消耗额外的资源的。<strong>但是SSD为了避免使用low-level的特征，放弃了浅层的feature map</strong>，而是从conv4_3开始建立金字塔，而且加入了一些新的层。因此SSD放弃了重利用更高分辨率的feature map，但是这些feature map对检测小目标非常重要。这就是SSD与FPN的区别。</li><li>图（d）FPN</li></ul><h3 id="如何工作"><a href="#如何工作" class="headerlink" title="如何工作"></a>如何工作</h3><p>FPN为了自然地利用CNN层级特征的金字塔形式，同时生成在所有尺度上都具有强语义信息的特征金字塔。所以FPN的结构设计了top-down结构和横向连接，以此融合具有高分辨率的浅层layer和具有丰富语义信息的深层layer。这样就实现了从单尺度的单张输入图像，快速构建在所有尺度上都具有强语义信息的特征金字塔，同时不产生明显的代价。</p><p><strong>自下而上的路径</strong><br>CNN的前馈计算就是自下而上的路径，特征图经过卷积核计算，通常是越变越小的，也有一些特征层的输出和原来大小一样，称为”same network stage”。对于每个stage选择最后一层的输出作为特征图的参考集。 这种选择是很自然的，因为每个阶段的最深层应该具有最强的特征。<br>具体来说，对于ResNets，作者使用了每个阶段的最后一个残差结构的特征激活输出。将这些残差模块输出表示为{C2, C3, C4, C5}，对应于conv2，conv3，conv4和conv5的输出，并且注意它们相对于输入图像具有{4, 8, 16, 32}像素的步长。考虑到内存占用，没有将conv1包含在金字塔中。</p><p><strong>自上而下的路径和横向连接</strong><br>自上而下的路径（the top-down pathway ）是如何去结合低层高分辨率的特征呢？方法就是，把更抽象，语义更强的高层特征图进行上取样，然后把该特征横向连接（lateral connections ）至前一层特征，因此高层特征得到加强。值得注意的是，横向连接的两层特征在空间尺寸上要相同。这样做应该主要是为了利用底层的定位细节信息。</p><p><img src="http://static.zybuluo.com/BYWMM/eie86ipgtl44mzsty4gvvydf/image_1dl8s0fgs1oma1tnu1ks8m32hlsm.png"></p><p>上图显示连接细节。把高层特征做2倍上采样（最邻近上采样法，可以参考反卷积），然后将其和对应的前一层特征结合（前一层要经过1 <em> 1的卷积核才能用，目的是改变channels，应该是要和后一层的channels相同），结合方式就是做像素间的加法。重复迭代该过程，直至生成最精细的特征图。迭代开始阶段，作者在C5层后面加了一个1 </em> 1的卷积核来产生最粗略的特征图，最后，作者用3 <em> 3的卷积核去处理已经融合的特征图（为了消除上采样的混叠效应），以生成最后需要的特征图。为了后面的应用能够在所有层级共享分类层，这里坐着固定了3</em>3卷积后的输出通道为d,这里设为256.因此所有额外的卷积层（比如P2）具有256通道输出。这些额外层没有用非线性。</p><p>{C2, C3, C4, C5}层对应的融合特征层为{P2, P3, P4, P5}，对应的层空间尺寸是相通的。</p><p>源码可参考mask r-cnn</p>]]></content>
    
    <summary type="html">
    
      读到Mask R-CNN，文中使用多种backbone的结果效果进行对比。这里对目标检测的各种backbone进行总结。
    
    </summary>
    
      <category term="object detection" scheme="https://bywmm.github.io/categories/object-detection/"/>
    
    
      <category term="目标检测" scheme="https://bywmm.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="backbone" scheme="https://bywmm.github.io/tags/backbone/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：Spatial Transformer Networks</title>
    <link href="https://bywmm.github.io/2019/09/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ASpatial-Transformer-Networks/"/>
    <id>https://bywmm.github.io/2019/09/26/论文笔记：Spatial-Transformer-Networks/</id>
    <published>2019-09-26T15:45:15.000Z</published>
    <updated>2019-10-24T03:50:38.519Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文笔记：Spatial-Transformer-Networks"><a href="#论文笔记：Spatial-Transformer-Networks" class="headerlink" title="论文笔记：Spatial Transformer Networks"></a>论文笔记：Spatial Transformer Networks</h1><h2 id="STN的作用"><a href="#STN的作用" class="headerlink" title="STN的作用"></a>STN的作用</h2><p>CNN本身对平移、旋转、放缩等许多基础变换是不具有不变性的。</p><p>举个例子，如果一个CNN一直在学习如何识别一直猫，训练样本都是一整只猫放满整张图片，如果我们输入一只小猫，那它很有可能不能正确识别。</p><p>当然我们可以通过data argumentation来手动使CNN学习到各种基础变换后的猫。</p><p>但attention model告诉我们，与其让网络隐式的学习到某种能力，不如为网络设计一个显式的处理模块，专门处理以上的各种变换。</p><h2 id="两个概念"><a href="#两个概念" class="headerlink" title="两个概念"></a>两个概念</h2><p><a href="https://bywmm.github.io/2019/09/26/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E5%92%8C%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC/">仿射变换和双线性插值</a></p><h2 id="STN的基本构架"><a href="#STN的基本构架" class="headerlink" title="STN的基本构架"></a>STN的基本构架</h2><p><img src="/2019/09/26/论文笔记：Spatial-Transformer-Networks/stn.png" alt="stn结构"></p><p>如图是空间转换模型（spatial transformer module）的基本结构，包括三部分：</p><ul><li>localisation net</li><li>grid generator</li><li>sampler</li></ul><h3 id="如何工作"><a href="#如何工作" class="headerlink" title="如何工作"></a>如何工作</h3><p>有了第2部分的基础，可以很容易理解STN的做法：</p><ol><li>生成$V$的meshgrid，然后通过一个$(2\times3)$的变换矩阵$M$得到每个坐标在$U$中对应的像素位置。</li><li>这个位置可能不是整数，使用双线性插值对该位置的像素值进行估计。</li></ol><p>通过<strong>仿射变换+双线性插值</strong>就可以得到我们需要的图像的相关变形。唯一未知的参数就是变换矩阵$M$，需要什么样的仿射由$M$的值确定。</p><h3 id="三部分"><a href="#三部分" class="headerlink" title="三部分"></a>三部分</h3><p>这就是<strong>localisation net</strong>的作用：</p><script type="math/tex; mode=display">\theta=f_{loc}(U)</script><p>通过前一层的特征$U$找到一个合适的$M$。这个$f_{loc}$可以是任何形式的，通常用一个全连接网络。</p><p>而grid generator，就是通过localisation net得到的$\theta$，也就是变换矩阵$M$，来对$V$的meshgrid进行仿射变换。</p><p>最后一步$Sampler$使用双线性插值得到最终的$V$。</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>模块化：<strong>STN可以插入到模型的任意位置with small tweaking。</strong></p><p>可微分：可以使用反向传播进行收敛操作。</p><p>动态变换：STN对每个输入样本的特征图都进行不同的仿射变换。相比，pooling layer对所有输入样本的作用相同。</p><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><p><a href="https://github.com/bywmm/Spatia-Transformer-Networks-Demo" target="_blank" rel="noopener">https://github.com/bywmm/Spatia-Transformer-Networks-Demo</a></p>]]></content>
    
    <summary type="html">
    
      Make CNN have translation invariance
    
    </summary>
    
      <category term="module" scheme="https://bywmm.github.io/categories/module/"/>
    
    
      <category term="论文笔记" scheme="https://bywmm.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>仿射变换&amp;双线性插值</title>
    <link href="https://bywmm.github.io/2019/09/26/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E5%92%8C%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC/"/>
    <id>https://bywmm.github.io/2019/09/26/仿射变换和双线性插值/</id>
    <published>2019-09-26T15:41:15.000Z</published>
    <updated>2019-10-17T05:21:56.632Z</updated>
    
    <content type="html"><![CDATA[<h1 id="仿射变换-amp-双线性插值"><a href="#仿射变换-amp-双线性插值" class="headerlink" title="仿射变换&amp;双线性插值"></a>仿射变换&amp;双线性插值</h1><h2 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h2><p>在学习仿射变换之前，先看一下线性变换。<br>我们定义：</p><ul><li>点$k$二维空间内一点，用列向量$\left[<br>\begin{matrix}<br>x \\y<br>\end{matrix}<br>\right]<br>$表示</li><li>矩阵$M=\left[<br>\begin{matrix}<br>a &amp; b \\c &amp; d<br>\end{matrix}<br>\right]$表示一个$(2\times2)$的一个方阵</li></ul><p>通过对$M$取不同的值，可以对$k$完成不同的线性变换。</p><script type="math/tex; mode=display">K'=MK</script><p>例如，当$M=\left[<br> \begin{matrix}<br>  1 &amp; 0 \\0 &amp; 1<br>  \end{matrix}<br>  \right]$时，</p><script type="math/tex; mode=display">K'= \left[\begin{matrix}1 & 0 \\ 0 & 1  \end{matrix}\right]  \left[\begin{matrix}x \\ y  \end{matrix}\right]=  \left[\begin{matrix}x \\ y  \end{matrix}\right]=K</script><p>下面来看典型的基本线性变换是如何实现的。</p><h3 id="放缩-Scaling"><a href="#放缩-Scaling" class="headerlink" title="放缩(Scaling)"></a>放缩(Scaling)</h3><p><img src="/2019/09/26/仿射变换和双线性插值/scale.png" height="150"></p><p>我们取$b=c=0$，$a$和$d$取任意整数：</p><script type="math/tex; mode=display">M=\left[\begin{matrix}p & 0 \\ 0 & q  \end{matrix}\right]</script><p>可以得到</p><script type="math/tex; mode=display">K'= \left[\begin{matrix}p & 0 \\ 0 & q  \end{matrix}\right]  \left[\begin{matrix}x \\ y  \end{matrix}\right]=  \left[\begin{matrix}px \\ qy  \end{matrix}\right]</script><h3 id="旋转-Rotation"><a href="#旋转-Rotation" class="headerlink" title="旋转(Rotation)"></a>旋转(Rotation)</h3><p><img src="/2019/09/26/仿射变换和双线性插值/rot.png" height="150"></p><p>如图，假设我们想旋转$\theta$，那我们需要设置$a=d=\cos\theta,b=-\sin\theta,c=\sin\theta$</p><script type="math/tex; mode=display">M=\left[\begin{matrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta  \end{matrix}\right]</script><p>旋转后的坐标</p><script type="math/tex; mode=display">K'= \left[\begin{matrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta  \end{matrix}\right]  \left[\begin{matrix}x \\ y  \end{matrix}\right]=  \left[\begin{matrix}x\cos\theta-y\sin\theta \\ x\sin\theta+y\cos\theta  \end{matrix}\right]</script><p><a href="https://blog.csdn.net/wodownload2/article/details/72637897" target="_blank" rel="noopener">旋转矩阵推导</a></p><h3 id="裁剪-Shear"><a href="#裁剪-Shear" class="headerlink" title="裁剪(Shear)"></a>裁剪(Shear)</h3><p><img src="/2019/09/26/仿射变换和双线性插值/shear.png" height="150"></p><p>（好吧，这个变换我根本不懂什么意义）</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>我们定义了3个基本的线性变换：</p><ul><li>放缩：按标量缩放x和y方向。</li><li>旋转：将点绕原点旋转角度θ。</li><li>裁剪：将x偏移与y成正比的数字，并将x偏移与x成正比的数字。</li></ul><p>由于矩阵乘法的交换律，我们可以把多个线性变换合成一个线性变换。<br>比如，我们要对$k$<strong>依次</strong>进行进行一次放缩变换$S$，一次裁剪变换$H$，一次旋转变换$R$。</p><script type="math/tex; mode=display">K'=R[H(SK)]</script><p>由矩阵乘法的结合律得</p><script type="math/tex; mode=display">K'=(RHS)K=MK</script><p>所以矩阵乘法有个好处：当我们进行多次线性变换的时候，不需要对进行多次矩阵乘法运算，只需要找到对的$M$，就可以一步到位！</p><h2 id="仿射变换"><a href="#仿射变换" class="headerlink" title="仿射变换"></a>仿射变换</h2><p>简单来说，<strong>仿射变换就是：线性变换+平移</strong></p><p>上面的变换可以将图像扭成任意形状，但在二维坐标系内，用2x2的矩阵所不能表示的变换就是<strong>平移</strong>操作。而平移是图像转换的重要变换.</p><p>所以，我们用一个三维的<strong>齐次坐标</strong>来表示二维向量：</p><ul><li>点$k$用一个$(3\times1)$列向量$\left[\begin{matrix}x \\y \\1<br>\end{matrix}\right]$表示</li><li>矩阵$M=\left[<br>\begin{matrix}<br>a &amp; b &amp; 0\\c &amp; d &amp; 0 \\0 &amp; 0 &amp; 1<br>\end{matrix}<br>\right]$表示一个$(3\times3)$的一个方阵</li></ul><p>为了表示<strong>平移变换</strong>，我们只需要在$M$的第三列放两个新的参数$e,f$</p><script type="math/tex; mode=display">M=\left[\begin{matrix}  a & b & e\\ c & d & f \\0 & 0 & 1  \end{matrix}\right]</script><p>如果只需为二维输出，那么只要用一个$2\times3$的矩阵来表示$M$，就可以进行我们的仿射变换。</p><script type="math/tex; mode=display">M=\left[\begin{matrix}  a & b & e\\ c & d & f  \end{matrix}\right]</script><h2 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h2><p>当我们进行了仿射变换后，比如旋转或者放缩，得到左图各像素点在右图中对应的位置，但转换后的坐标往往不是整数。我们需要得到右图每个整数点的像素值才能渲染出这张image。所以要想点办法。</p><p><img src="/2019/09/26/仿射变换和双线性插值/stickman.png"></p><p>考虑到，我们将左图乘了一个$M$得到变换后的坐标，同样存在一种逆变换$M’$可以让我们将右图各像素点还原到左图相对应的位置。同样，这些位置虽然不一定是整数，但我们可以使用<strong>双线性插值</strong>等方法对其像素值进行估计。</p><p><strong>所以在实际操作中所求的$M$其实是一种从后到前的逆变换。</strong><br>（个人理解）</p><h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p><img src="/2019/09/26/仿射变换和双线性插值/interpol.png" width="400"></p><p>先做$x$轴方向的线性插值得到$R_2$和$R_1$。</p><script type="math/tex; mode=display">R_1=\frac{x_2-x}{x_2-x_1}Q_{11}+\frac{x-x_1}{x_2-x_1}Q_{21}</script><script type="math/tex; mode=display">R_2=\frac{x_2-x}{x_2-x_1}Q_{12}+\frac{x-x_1}{x_2-x_1}Q_{22}</script><p>再做$y$轴方向的线性插值</p><script type="math/tex; mode=display">P=\frac{y_2-y}{y_2-y_1}R_1+\frac{y-y_1}{y_2-y_1}R_2</script><p>当然也可以先做$y$轴的插值在做$x$轴的插值。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>搬运：<a href="https://kevinzakka.github.io/2017/01/10/stn-part1/" target="_blank" rel="noopener">kevin’s blog</a></p>]]></content>
    
    <summary type="html">
    
      仿射变换=线性变换+平移
    
    </summary>
    
      <category term="数学" scheme="https://bywmm.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="线性代数" scheme="https://bywmm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书笔记7 贝叶斯分类器</title>
    <link href="https://bywmm.github.io/2019/09/22/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B07/"/>
    <id>https://bywmm.github.io/2019/09/22/西瓜书笔记7/</id>
    <published>2019-09-22T03:01:30.000Z</published>
    <updated>2019-10-24T03:44:24.397Z</updated>
    
    <content type="html"><![CDATA[<h1 id="西瓜书笔记7-贝叶斯分类器"><a href="#西瓜书笔记7-贝叶斯分类器" class="headerlink" title="西瓜书笔记7 贝叶斯分类器"></a>西瓜书笔记7 贝叶斯分类器</h1><p><strong>先验概率</strong>：事情还没有发生，要求这件事情发生的可能性的大小</p><p><strong>后验概率</strong>：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小</p><p><strong>贝叶斯定理</strong>:</p><script type="math/tex; mode=display">P(A|B) = \frac{P(AB)}{P(B)}=\frac{P(B|A)P(A)}{P(B)}</script><h2 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h2><p>假设有$N$个可能的类别标记，即 $\gamma=\lbrace{c}_{1},{c}_{2},…,{c}_{N}\rbrace$</p><p>${\lambda}_{ij}$是将一个真实标记为${c}_{j}$样本误分类为${c}_{i}$的概率。</p><p>将样本$\boldsymbol x$分类为${c}_{i}$产生的期望损失为</p><script type="math/tex; mode=display">R({C}_{i}|\boldsymbol{x})=\sum_{j=1}^{N} {\lambda}_{ij}P(c_{i}|\boldsymbol{x})</script><blockquote><p>后验概率$P({c}_{j}|\boldsymbol{x})$理解：$\boldsymbol{x}$已经存在的情况下，$c_{j}$发生的概率，即$\boldsymbol{x}$分类为$c_{j}$的概率</p></blockquote><p>我们的任务是寻找一个判定准则$h:\chi \rightarrow \gamma$以最小化总体风险</p><script type="math/tex; mode=display">R(h)=E_{\boldsymbol{x}}[R(h(\boldsymbol{x})|\boldsymbol{x})]</script><p>贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险$R(c|\boldsymbol{x})$最小的标记，即</p><script type="math/tex; mode=display">h^{*}(\boldsymbol{x})=\underset{c\epsilon{\gamma}}{argmin}R(c|\boldsymbol{x})</script><p>若目标是最小化分类错误率，可采用<strong>0/1损失函数</strong>，则误判损失${\lambda}_{ij}$可写为</p><script type="math/tex; mode=display">\lambda_{ij} = \left\{\begin{matrix} 0,&if\ i=j; \\  1,&otherwise\end{matrix}\right.</script><p>此时条件风险</p><script type="math/tex; mode=display">R(c|\boldsymbol{x})=1-P(c|\boldsymbol{x})</script><p>目标最小化风险$R(c|\boldsymbol{x})$，即最大化$P(c|\boldsymbol{x})$<br>于是最小化分类错误率的贝叶斯最有分类器为</p><script type="math/tex; mode=display">h^{*}(\boldsymbol{x})=\underset{c\epsilon{\gamma}}{argmax}P(c|\boldsymbol{x})</script><p>大体来说，有两种测率来估计后验概率$P(c|\boldsymbol{x})$：</p><p><strong>判别式模型</strong>：给定$\boldsymbol{x}$，可通过直接建模$P(c|\boldsymbol{x})$来预测c。</p><p><strong>生成式模型</strong>：先对联合概率分布$P(c, \boldsymbol{x})$建模，然后在由此获得$P(c|\boldsymbol{x})$。</p><blockquote><p>常见判别式模型：</p><ul><li>线性回归、逻辑回归</li><li>线性判别分析</li><li>支持向量机(SVM)</li><li>神经网络(Neural Networks)</li><li>随机森林、决策树</li><li>Boosting</li></ul><p>常见生成式模型:</p><ul><li>隐马尔科夫模型(HMM)</li><li>朴素贝叶斯模型</li><li>高斯混合模型(GMM)</li><li>AODE(SPODE的集成)</li><li>Latent Dirichlet allocation</li><li>Restricted Boltzmann Machine</li></ul></blockquote><p>对于生成式模型来说，必然考虑</p><script type="math/tex; mode=display">P(c|\boldsymbol{x})=\frac{P(c,\boldsymbol{x})}{P(\boldsymbol{x})}</script><p>根据贝叶斯定理</p><script type="math/tex; mode=display">P(c|\boldsymbol{x})=\frac{P(c)P(\boldsymbol{x}|c)}{P(\boldsymbol{x})}</script><p>$P(c)$是类“先验”概率。表达了样本空间中各类样本所占的比例，根据<strong>大数定律</strong>，当训练集包含<strong>充足的独立同分布</strong>样本时，P(c)可通过各类样本出现的<strong>频率</strong>来估计。</p><p>$P(\boldsymbol{x})$是用于归一化的“证据”因子，对所有类标记均相同。个人认为是$\boldsymbol{x}$在所有样本空间里出现的概率。</p><p>$P(\boldsymbol{x}|c)$是样本$\boldsymbol{x}$相对于标记c的类条件概率，或称为“<strong>似然</strong>”，标记为c的样本中标记为$\boldsymbol{x}$的概率。</p><p><strong>$P(\boldsymbol{x}|c)$怎么估计？</strong></p><blockquote><p><strong>不可用频率估计</strong></p><p>由于$P(\boldsymbol{x}|c)$涉及关于$\boldsymbol{x}$的所有属性的联合概率。假设样本有d个属性，每个属性都是二值，则样本空间将有$2^{d}$种可能的取值。现实应用中这个值往往大于样本数，也就是说，很多样本取值在训练集中根本没有出现。直接用频率估计$P(\boldsymbol{x}|c)$显然是不同的，因为“未被观测到”和“出现概率为零”通常是不同的。</p><p><strong>极大似然估计：先假设其具有某种确定的概率分布形式，再基于训练样本对参数进行估计。</strong></p><p>假设$P(\boldsymbol{x}|c)$具有确定的形式并且被参数向量$\boldsymbol{\theta_{c}}$唯一确定</p><p>我们的任务就是用训练集D估计参数$\boldsymbol{\theta_{c}}$。为明确起见，可将$P(\boldsymbol{x}|c)$记为$P(\boldsymbol{x}|\boldsymbol{\theta_{c}})$。</p><p>令$D_{c}$表示训练集D中第c类样本组成的集合，假设这些样本是独立同分布的，则参数$\boldsymbol{\theta_{c}}$对于$D_{c}$的似然是</p><script type="math/tex; mode=display">P(D_{c}|\boldsymbol{\theta_{c}}) =\prod_{\boldsymbol{x}\epsilon{D_{c}}}P(\boldsymbol{x}|\boldsymbol{\theta_{c}})</script><p>避免练成操作造成的下溢，通常使用对数似然</p><script type="math/tex; mode=display">LL(\boldsymbol{\theta_{c}}) = logP(D_{c}|\boldsymbol{\theta_{c}})=\sum_{\boldsymbol{x}\epsilon{D_{c}}}logP(\boldsymbol{x}|\boldsymbol{\theta_{c}})</script><p>参数$\boldsymbol{\theta_{c}}$的极大似然估计$\hat{\boldsymbol{\theta}}_{c}$为</p><script type="math/tex; mode=display">\hat{\boldsymbol{\theta}}_{c}=\underset{\boldsymbol{\theta_{c}}}{argmax}LL(\boldsymbol{\theta_{c}})</script><p>需要注意的是，这种参数化的方法虽然能使类条件概率估计变得相对简单，<strong>但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真是数据分布。</strong></p></blockquote><h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><h3 id="属性条件独立假设"><a href="#属性条件独立假设" class="headerlink" title="属性条件独立假设"></a>属性条件独立假设</h3><p><strong>属性条件独立性假设：对已知类别，假设所有属性相互独立。</strong></p><p>基于属性条件独立性假设：</p><script type="math/tex; mode=display">P(c|\boldsymbol{x})=\frac{P(c)P(\boldsymbol{x}|c)}{P(\boldsymbol{x})}=\frac{P(c)}{P({\boldsymbol{x}})}\prod_{i=1}^{d}P(x_{i}|c)</script><p>其中$d$为属性数目，$x_{i}$为$\boldsymbol{x}$在第$i$个属性上的取值。</p><p>对所有类别来说$P(\boldsymbol{x})$相同，所以朴素贝叶斯的表达式为：</p><script type="math/tex; mode=display">h_{nb}(\boldsymbol{x}) = \underset{c\epsilon{\gamma}}{argmax}P(c)\prod_{i=1}^{d}P(x_{i}|c)</script><p>令$D_{c}$表示训练集$D$中第$c$类样本组成的集合，若有重组的独立同分布样本，则可容易地估计出类先验概率</p><script type="math/tex; mode=display">P(c) = \frac{|D_{c}|}{|D|}</script><p>对<strong>离散属性</strong>而言，令$D_{c,x_{i}}$表示$D_{c}$中在第i个属性上取值为$x_{i}$的样本组成的集合，条件概率$P(x_{i}|c)$可估计为</p><script type="math/tex; mode=display">p(x_{i}|c)=\frac{|D_{c,x_{i}}|}{|D_{c}|}</script><p>对<strong>连续属性</strong>而言，可考虑<strong>概率密度函数</strong>。假定$p(x_{i}|C)\sim N(\mu_{c,i},\sigma_{c,i}^{2})$，其中$\mu_{c,i}$和$\sigma_{c,i}^{2}$分别是第c类样本在第i个属性上取值的均值和方差(易求)，则有</p><script type="math/tex; mode=display">p(x_i|c)=\frac{1}{\sqrt{2\pi}\sigma_{c,i}}exp(-\frac{(x_i-\mu_{c,i})^2}{2 \sigma_{c,i}^2})</script><h3 id="拉普拉斯修正"><a href="#拉普拉斯修正" class="headerlink" title="拉普拉斯修正"></a>拉普拉斯修正</h3><p>需注意，若某个属性值在训练集中没有与某个类同时出现过，则直接基于上式进行估计，则会出现问题。例如，</p><script type="math/tex; mode=display">p(x_{i}|c)=\frac{|D_{c,x_{i}}|}{|D_{c}|} = \frac{0}{D_c} = 0</script><p>则其与其他属性的乘积都为0。为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，在估计概率值是通常需要“平滑”，常用“<strong>拉普拉斯修正</strong>”。</p><p>令$N$表示训练集$D$中可能的属性数，$N_i$表示第$i$个属性可能的取值数，则</p><script type="math/tex; mode=display">\hat{P}(c)=\frac{|D_c|+1}{|D|+N}</script><script type="math/tex; mode=display">\hat{P}(x_i|c)=\frac{|D_{c,xi}|+1}{|D_c|+N}</script><p><strong>拉普拉斯修正实质上假设了属性值与类别均匀分布。</strong></p><blockquote><p>现实任务中朴素贝叶斯分类器有多种使用方式。例如：</p><ul><li>若任务队预测速度要求较高，可对所有概率估值事件预处理储存，预测时“查表”</li><li>若任务数据更替频繁，可采用“懒惰学习”</li><li>若数据不断增加，可实现“增量学习”</li></ul></blockquote><h2 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h2><p>思想：<strong>适当考虑一部分属性间的相互依赖信息</strong>，从而既不需要进行完全联合概率计算，又不至于彻底忽略了较强的属性依赖关系。</p><h3 id="独立依赖估计"><a href="#独立依赖估计" class="headerlink" title="独立依赖估计"></a>独立依赖估计</h3><p>“独依赖估计”(One-Dependent Estimator)是最常用的一种策略：假设每个属性在类别之外最多仅依赖于一个其他属性，即</p><script type="math/tex; mode=display">P(c|\boldsymbol{x})\propto P(c)\prod_{i}^{d}P(x_i|c,pa_{i})</script><p>其中$\propto$表示成正比，$pa_i$为属性$x_i$所依赖的属性，为$x_i$的父属性。</p><p>所以，问题的关键在如何确定每个属性的父属性。</p><h3 id="SPODE"><a href="#SPODE" class="headerlink" title="SPODE"></a>SPODE</h3><p>最直接的做法是，假设所有属性都依赖于同一个属性，称为“超父”。然后通过交叉验证等模型选择方法来确定超父属性，由此形成了SPODE(Super-Parent ODE)方法。</p><p><img src="http://static.zybuluo.com/BYWMM/prpt04mukomox26l5ev3r2cc/image_1dl8u7lhserp1tsp144813kef679.png"></p><h3 id="TAN"><a href="#TAN" class="headerlink" title="TAN"></a>TAN</h3><p>基于<strong>最大权生成树</strong>算法：</p><ol><li><p>计算任意两个属性之间的<strong>条件互信息</strong></p><script type="math/tex; mode=display">I(x_i, x_j|y)=\sum_{x_i,x_j;c\epsilon \gamma}log\frac{P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)} ;</script></li><li><p>以属性为结点构建完全图，任意两个结点之间边的权重设为 $I(x_i,x_j|y)$;</p></li><li><p>构建此完全图的最大权生成树，挑选跟变量，将边置为有向；</p></li><li><p>加入类别结点，增加y到每个属性的有向边。</p></li></ol><p>条件互信息$I(x_i,x_j|y)$刻画了属性$x_i$和$x_j$在已知类别情况下的相关性，因此，通过最大生成树算法，TAN仅保留了强相关属性之间的依赖性</p><h3 id="AODE"><a href="#AODE" class="headerlink" title="AODE"></a>AODE</h3><p>AODE(Averaged ODE)尝试将每个属性作为超父来构建SPODE，将具有足够数据支撑的SPODE集成起来作为最终结果，</p><script type="math/tex; mode=display">P(c|\boldsymbol x)\propto \sum_{i=1,|D_{x_i}| \geq m^{'}}^{d}P(c,x_i)\prod_{j=1}^{d}P(x_j|c,x_i)</script><p>其中$D_{x_i}$是在第i个属性上取值为$x_i$的样本集合$m^{‘}$为阈值常数(默认30)。</p><script type="math/tex; mode=display">\hat{P}(c,x_i)=\frac{|D_{c,x_i}|+1}{|D|+N+N_i}</script><script type="math/tex; mode=display">\hat{P}(x_j|c,x_i) = \frac{|D_{c,x_i,x_j}|+1}{|D_{c,x_i}|+N_j}</script><p>其中N是D中可能的类别数，$N_i$是第i个属性可能的取值数，$D_{c,x_i}$是类别为c且在第i个属性上取值为$x_i$的样本集合，$D_{c, x_i, x_j}$是类别为c且在第i和第j个属性上分别为$x_i$和$x_j$的样本集合。</p><h2 id="贝叶斯网-信念网"><a href="#贝叶斯网-信念网" class="headerlink" title="贝叶斯网(信念网)"></a>贝叶斯网(信念网)</h2><h2 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h2><p><strong>隐变量</strong>：属性值未知的变量；未观测的变量。</p><p>令$\boldsymbol X$表示已观测变量集，$\boldsymbol Z$表示隐变量集，$\Theta$表示模型参数。</p><p>若对$\Theta$做极大似然估计，则应最大化对数似然</p><script type="math/tex; mode=display">LL(\Theta|\boldsymbol X, \boldsymbol Z) = lnP(\boldsymbol X, \boldsymbol Z|\Theta)</script><p>由于$\boldsymbol Z$是隐变量，上式无法直接求解。可以对$\boldsymbol Z$计算期望，来最大化已观测数据的对数“==边际似然==”</p><script type="math/tex; mode=display">LL(\Theta|\boldsymbol X)=lnP(\boldsymbol X|\Theta) = ln \sum _{\boldsymbol Z}P(\boldsymbol X,\boldsymbol Z|\Theta)</script><p><strong>EM(Expectation-Maximization)算法(期望最大化算法)</strong>:<br>是一种<strong>迭代式</strong>的方法</p><p>EM使用两个步骤交替计算，直至收敛到最优解。</p><p>算法原型：</p><ul><li>E步(Expectation)：基于$\Theta^t$推断隐变量$\boldsymbol Z$的值，记为$\boldsymbol Z^t$;</li><li>M步(Maximization)：基于已观测变量$\boldsymbol X$和$\boldsymbol{Z^t}$对参数$\Theta$做极大似然估计，记为$\Theta^{t+1}$</li></ul><p>若不是，取$\boldsymbol{Z}$的期望，而是基于$\Theta^t$计算隐变量$\boldsymbol Z$的概率分布$P(\boldsymbol Z|\boldsymbol X, \Theta^t)$,则EM算法的两个步骤是：</p><ul><li>E步(Expectation)：以当前参数$\Theta^t$推断隐变量分布$P(\boldsymbol Z|\boldsymbol{X},\Theta)$，并计算对数似然$LL(\Theta|\boldsymbol{X},\boldsymbol{Z})$关于$\boldsymbol{Z}$的期望</li></ul><script type="math/tex; mode=display">Q(\Theta|\Theta^t)=E_{\boldsymbol{Z}|\boldsymbol{X},\Theta^t}LL(\Theta|\boldsymbol{X},\boldsymbol{Z})</script><ul><li>M步(Maximization):寻找参数<strong>最大化期望似</strong>然，即</li></ul><script type="math/tex; mode=display">\Theta^{t+1}=\underset{\Theta}{argmax}Q(\Theta|\Theta^t)</script><p>EM算法可看作用坐标下降法来最大化对数似然下界的过程，是一种非梯度的优化方法。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><blockquote><p>根据对属性间依赖的涉及程度，贝叶斯分类器形成了一个“谱”：朴素贝叶斯分类器不考虑属性见的依赖性，贝叶斯网能表示任意属性间的依赖性，二者分别位于“谱”的两端；介于两者之间的则是一系列半朴素贝叶斯分类器，它们基于各种假设和约束来对属性间的部分依赖性进行建模。</p><p>EM算法是最常见的<strong>隐变量估计方法</strong>，在机器学习中有极为广泛的用途，例如常被用来学习高斯混合模型(GMM)的参数。K均值聚类算法就是一个典型的EM算法。</p></blockquote>]]></content>
    
    <summary type="html">
    
      贝叶斯决策论；朴素贝叶斯；半朴素贝叶斯；贝叶斯网；EM算法
    
    </summary>
    
      <category term="机器学习" scheme="https://bywmm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="西瓜书笔记" scheme="https://bywmm.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器学习" scheme="https://bywmm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>cs231n assignment3(LSTM_Captioning)</title>
    <link href="https://bywmm.github.io/2018/12/17/cs231n_assignment3(LSTM_Captioning)/"/>
    <id>https://bywmm.github.io/2018/12/17/cs231n_assignment3(LSTM_Captioning)/</id>
    <published>2018-12-17T09:03:37.000Z</published>
    <updated>2019-10-24T03:43:11.040Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment3-LSTM-Captioning"><a href="#cs231n-assignment3-LSTM-Captioning" class="headerlink" title="cs231n assignment3(LSTM_Captioning)"></a>cs231n assignment3(LSTM_Captioning)</h1><p>上个实验中的virtual RNN，存在一个问题，就是无法解决长期依赖问题（long-term dependencies）。当时间序列变得很长的时候，前后信息的关联度会越来越小，直至消失，即所谓的梯度消失现象。</p><h2 id="GRU（Gated-Recurrent-Unit）单元"><a href="#GRU（Gated-Recurrent-Unit）单元" class="headerlink" title="GRU（Gated Recurrent Unit）单元"></a>GRU（Gated Recurrent Unit）单元</h2><p>GRU用了个<strong>更新门</strong>的东西（它出现的时间比LSTM要晚），来维持一个长时间的记忆。</p><blockquote><p><strong>看一个例子</strong>：</p><p>The <strong>cat</strong>, which already ate …, <strong>was</strong> full.</p></blockquote><p>这里我们看到，cat和was之间隔了一个很长的定语从句，在这之间RNN可能记不到之前的信息。</p><p>于是这里用一个c = memory cell来表示长期的记忆；一个更新门$f_u$来确定是否对c进行更新，因为可能并不是每次都对c进行改变，可以达到一个长期记忆的效果。</p><p>公式如下：</p><script type="math/tex; mode=display">\tilde{c}^{<t>} = \tanh(W_c[c^{t-1}, x^{t}]+b_c)</script><script type="math/tex; mode=display">f_u=\sigma(W_u[c^{t-1}, x^{t}]+b_u)</script><script type="math/tex; mode=display">c^{<t>}=f_u*\tilde{c}^{t}+(1-f_u)*c^{t-1}</script><p>Sigma函数使$f_u$很容易就非常接近1或0。</p><p><img src="http://static.zybuluo.com/BYWMM/uadt151yqca6128xov6tj3il/zheng1.png" width="350"></p><h2 id="LSTM原理"><a href="#LSTM原理" class="headerlink" title="LSTM原理"></a>LSTM原理</h2><p>有了门的概念之后，理解LSTM会比较容易了。</p><p>一个LSTM单元有三个门，后面介绍。</p><p><img src="http://static.zybuluo.com/BYWMM/eacrvvivhjujpf7ji86yxet1/2301760-555de74a5ecfa8de.png" width="700"></p><h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><p>遗忘门决定对上次的$c^{<t-1>}$“遗忘程度”。</t-1></p><script type="math/tex; mode=display">f_t=\sigma(W_t[c^{<t-1>}, x^{<t>}]+b_t)</script><p><img src="http://static.zybuluo.com/BYWMM/wotfoetiaia9bjgegerts4u3/zheng3.png" width="350"></p><h3 id="更新门"><a href="#更新门" class="headerlink" title="更新门"></a>更新门</h3><p>更新门决定对本次计算新生成的$\tilde{c}^{<t>}$“保留程度”</t></p><script type="math/tex; mode=display">f_i=\sigma(W_i[c^{<t-1>}, x^{<t>}]+b_i)</script><p>同时计算一下memory cell的值</p><script type="math/tex; mode=display">\tilde{c}^{<t>} = \tanh(W_c[c^{<t-1>}, x^{<t>}]+b_c)</script><p><img src="http://static.zybuluo.com/BYWMM/aowf1gccc13c4e2rw14aqurj/zheng4.png" width="350"></p><p>有了更新门和遗忘门还有新的memory cell，就可以对memory cell进行更新</p><script type="math/tex; mode=display">c^{<t>}=f_t*c^{<t-1>}+f_i*\tilde{c}^{<t>}</script><p><img src="http://static.zybuluo.com/BYWMM/8oun9hx0pmcwmw01dkzn8iv2/zheng6.png" width="350"></p><h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><p>更新门和遗忘门都是针对$c^{<t>}$的更新而言的，输出门则是针对$f^{<t>}$来说的。</t></t></p><script type="math/tex; mode=display">o_t=\sigma(W_o[c^{<t-1>}, x^{<t>}]+b_o)</script><script type="math/tex; mode=display">h^{<t>} = o_t * \tanh(c^{<t>})</script><p><img src="http://static.zybuluo.com/BYWMM/jzimd71jg604cxslck3jar26/zheng5.png" width="350"></p><p>以上就是标准的LSTM，当然它还有很多变体这里就不再介绍。下面是作业内容。</p><h2 id="LSTM-Step"><a href="#LSTM-Step" class="headerlink" title="LSTM Step"></a>LSTM Step</h2><h3 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - prev_h: Previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - prev_c: previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - next_c: Next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Tuple of values needed for backward pass.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    A = np.dot(x, Wx) + np.dot(prev_h, Wh) + b</span><br><span class="line">    A = A.reshape(N, <span class="number">4</span>, <span class="number">-1</span>)</span><br><span class="line">    next_c = sigmoid(A[:,<span class="number">1</span>,:]) * prev_c + sigmoid(A[:,<span class="number">0</span>,:]) * np.tanh(A[:,<span class="number">3</span>,:])</span><br><span class="line">    next_h = sigmoid(A[:,<span class="number">2</span>,:]) * np.tanh(next_c)</span><br><span class="line"></span><br><span class="line">    cache = (A, x, prev_h, prev_c, Wx, Wh, b, next_h, next_c)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_h, next_c, cache</span><br></pre></td></tr></table></figure><h3 id="Backward-Pass"><a href="#Backward-Pass" class="headerlink" title="Backward Pass"></a>Backward Pass</h3><p>不用推公式，用计算图递归求导就好了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for a single timestep of an LSTM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dnext_h: Gradients of next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dnext_c: Gradients of next cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dprev_c: Gradient of previous cell state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A, x, prev_h, prev_c, Wx, Wh, b, next_h, next_c = cache</span><br><span class="line"></span><br><span class="line">    dsigmoid_a2 = dnext_h * np.tanh(next_c)</span><br><span class="line">    dnext_c += dnext_h * sigmoid(A[:, <span class="number">2</span>, :]) * (<span class="number">1</span> - np.tanh(next_c) ** <span class="number">2</span>)</span><br><span class="line">    dsigmoid_a1 = dnext_c * prev_c</span><br><span class="line">    dsigmoid_a0 = dnext_c * np.tanh(A[:, <span class="number">3</span>, :])</span><br><span class="line">    dtanh_a3 = dnext_c * sigmoid(A[:, <span class="number">0</span>, :])</span><br><span class="line">    dA = np.zeros(A.shape)</span><br><span class="line">    dA[:, <span class="number">0</span>, :] = dsigmoid_a0 * sigmoid(A[:, <span class="number">0</span>, :]) * (<span class="number">1</span> - sigmoid(A[:, <span class="number">0</span>, :]))</span><br><span class="line">    dA[:, <span class="number">1</span>, :] = dsigmoid_a1 * sigmoid(A[:, <span class="number">1</span>, :]) * (<span class="number">1</span> - sigmoid(A[:, <span class="number">1</span>, :]))</span><br><span class="line">    dA[:, <span class="number">2</span>, :] = dsigmoid_a2 * sigmoid(A[:, <span class="number">2</span>, :]) * (<span class="number">1</span> - sigmoid(A[:, <span class="number">2</span>, :]))</span><br><span class="line">    dA[:, <span class="number">3</span>, :] = dtanh_a3 * (<span class="number">1</span> - np.tanh(A[:, <span class="number">3</span>, :]) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    dprev_c = dnext_c * sigmoid(A[:, <span class="number">1</span>, :])</span><br><span class="line">    N, D = x.shape</span><br><span class="line">    dA = dA.reshape(N, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    dx = dA.dot(Wx.T)</span><br><span class="line">    dWx = x.T.dot(dA)</span><br><span class="line">    dprev_h = dA.dot(Wh.T)</span><br><span class="line">    dWh = prev_h.T.dot(dA)</span><br><span class="line">    db = np.ones((N,)).dot(dA)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</span><br></pre></td></tr></table></figure></p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>这一部分与RNN差不多。</p><h3 id="Forward-Pass-1"><a href="#Forward-Pass-1" class="headerlink" title="Forward Pass"></a>Forward Pass</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for an LSTM over an entire sequence of data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - h0: Initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - b: Biases of shape (4H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N, T, D = x.shape</span><br><span class="line">    H = h0.shape[<span class="number">1</span>]</span><br><span class="line">    cache = []</span><br><span class="line">    h = np.zeros((N, T, H))</span><br><span class="line">    prev_h = h0</span><br><span class="line">    prev_c = np.zeros((N, H))</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        prev_h, prev_c, cache_t = lstm_step_forward(x[:, t, :], prev_h, prev_c, Wx, Wh, b)</span><br><span class="line">        cache.append(cache_t)</span><br><span class="line">        h[:, t, :] = prev_h</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br></pre></td></tr></table></figure><h3 id="Backward-Pass-1"><a href="#Backward-Pass-1" class="headerlink" title="Backward Pass"></a>Backward Pass</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(dh, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for an LSTM over an entire sequence of data.]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of input data of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - dh0: Gradient of initial hidden state of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    _, x, prev_h, prev_c, Wx, Wh, b, next_h, next_c = cache[<span class="number">0</span>]</span><br><span class="line">    N, T, H = dh.shape</span><br><span class="line">    D = x.shape[<span class="number">1</span>]</span><br><span class="line">    dx = np.zeros((N, T, D))</span><br><span class="line">    dh0 = np.zeros((N, H))</span><br><span class="line">    dWx = np.zeros((D, <span class="number">4</span> * H))</span><br><span class="line">    dWh = np.zeros((H, <span class="number">4</span> * H))</span><br><span class="line">    db = np.zeros((<span class="number">4</span>*H))</span><br><span class="line">    dprev_c = np.zeros((N, H))</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        dx_t, dh0, dprev_c, dWx_t, dWh_t, db_t = lstm_step_backward(dh[:, t, :] + dh0, dprev_c, cache[t])</span><br><span class="line">        dx[:,t,:] = dx_t</span><br><span class="line">        dWx += dWx_t</span><br><span class="line">        dWh += dWh_t</span><br><span class="line">        db += db_t</span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><p>loss,grads,sample稍微修改一点就好。</p>]]></content>
    
    <summary type="html">
    
      GRU（Gated Recurrent Unit）；LSTM原理；LSTM实验代码；
    
    </summary>
    
      <category term="cs231n" scheme="https://bywmm.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="https://bywmm.github.io/tags/cs231n/"/>
    
      <category term="计算机视觉" scheme="https://bywmm.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="RNN" scheme="https://bywmm.github.io/tags/RNN/"/>
    
      <category term="Image Caption" scheme="https://bywmm.github.io/tags/Image-Caption/"/>
    
  </entry>
  
  <entry>
    <title>cs231n assignment3(RNN_Captioning)</title>
    <link href="https://bywmm.github.io/2018/12/16/cs231n_assignment3(RNN_Captioning)/"/>
    <id>https://bywmm.github.io/2018/12/16/cs231n_assignment3(RNN_Captioning)/</id>
    <published>2018-12-16T09:03:37.000Z</published>
    <updated>2019-10-24T03:43:22.017Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment3-RNN-Captioning"><a href="#cs231n-assignment3-RNN-Captioning" class="headerlink" title="cs231n assignment3(RNN_Captioning)"></a>cs231n assignment3(RNN_Captioning)</h1><p>这次的作业内容是从 <strong>Image Caption</strong> 这个问题入手，即给定一张图片，生成对图片的文字描述。</p><p>大概的做法是这样的，用一个预训练的 CNN 把图片提取特征，然后那这个特征初始化 RNN(LSTM) 的 hidden state，用 RNN(LSTM) 生成一句话。</p><p>这里的 CNN 主要就是一个<strong>encoder</strong>，负责把图片压缩成一个语义向量，而 RNN(LSTM) 则是一个<strong>decoder</strong>，也是一个语言模型（language model），负责从这个语义向量解码出自然语言。</p><p><img src="http://static.zybuluo.com/BYWMM/9mytf272s2vcuceh5u7cbru7/20170427160521318.png"></p><h2 id="COCO-dataset"><a href="#COCO-dataset" class="headerlink" title="COCO dataset"></a>COCO dataset</h2><p>本次实验用的数据是微软2014年发布的<a href="http://mscoco.org/" target="_blank" rel="noopener">COCO dataset</a>，这也是用来测试image caption的标准数据集。</p><p>COCO数据集包含80,000个训练集图像40,000验证集图像, each annotated with 5 captions written by workers on Amazon Mechanical Turk.</p><p>下面看其中一张样本。</p><h2 id><a href="#" class="headerlink" title></a><img src="http://static.zybuluo.com/BYWMM/liqdo84ndl5s0c27z1c8w24p/image_1cuqmbeltm8laah1vqf17tt15rj11.png" width="500"></h2><h2 id="Vanilla-RNN-Step"><a href="#Vanilla-RNN-Step" class="headerlink" title="Vanilla RNN Step"></a>Vanilla RNN Step</h2><h3 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h3><p><img src="http://static.zybuluo.com/BYWMM/gw0atcmm5ocqocp8vrrxcykz/zfasf.png" width="800"><br>RNN 的 step_forward 公式很简单，</p><script type="math/tex; mode=display">h_t=\tanh(W_xx_t+W_hh_{t−1}+b)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data for this timestep, of shape (N, D).</span></span><br><span class="line"><span class="string">    - prev_h: Hidden state from previous timestep, of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></span><br><span class="line"><span class="string">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></span><br><span class="line"><span class="string">    - b: Biases of shape (H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Tuple of values needed for the backward pass.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    Z = np.dot(x, Wx) + np.dot(prev_h, Wh) + b</span><br><span class="line">    next_h = np.tanh(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> next_h, cache</span><br></pre></td></tr></table></figure><h3 id="Backward-Pass"><a href="#Backward-Pass" class="headerlink" title="Backward Pass"></a>Backward Pass</h3><p>反向传播和之前的全连接层也差不多，也挺简单。注意$\tanh(x)$的导数</p><script type="math/tex; mode=display">\tanh'(x)=1-\tanh^2(x)</script><p><img src="http://static.zybuluo.com/BYWMM/l228xa9kbsogncfgnc3odxkf/zheng2.png" width="350"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for a single timestep of a vanilla RNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - cache: Cache object from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradients of input data, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dprev_h: Gradients of previous hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradients of input-to-hidden weights, of shape (D, H)</span></span><br><span class="line"><span class="string">    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)</span></span><br><span class="line"><span class="string">    - db: Gradients of bias vector, of shape (H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x, Wx, prev_h, Wh, b, next_h= cache</span><br><span class="line">    N, H = dnext_h.shape</span><br><span class="line">    <span class="comment"># the derivative of tanh(x) is (1 - tanh(x)**2)</span></span><br><span class="line">    dZ = (<span class="number">1</span> - next_h ** <span class="number">2</span>) * dnext_h   <span class="comment"># of shape (N, H)</span></span><br><span class="line"></span><br><span class="line">    dx = np.dot(dZ, Wx.T)   <span class="comment"># of shape (N, D)</span></span><br><span class="line">    dWx = np.dot(x.T, dZ)   <span class="comment"># of shape (D, H)</span></span><br><span class="line">    dprev_h = np.dot(dZ, Wh.T)   <span class="comment"># of shape (N, H)</span></span><br><span class="line">    dWh = np.dot(prev_h.T, dZ)   <span class="comment"># of shape (H, H)</span></span><br><span class="line">    db = np.dot(np.ones((<span class="number">1</span>, N)), dZ).flatten() <span class="comment"># of shape (H,)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</span><br></pre></td></tr></table></figure><h2 id="Vanilla-RNN"><a href="#Vanilla-RNN" class="headerlink" title="Vanilla RNN"></a>Vanilla RNN</h2><p>如图，RNN的部分就是将前面的RNN step用for loop简单组装一下<br><img src="http://static.zybuluo.com/BYWMM/2vcb33s8sog6yv9fjoimj8wn/zheng4.png"></p><h3 id="Forward-Pass-1"><a href="#Forward-Pass-1" class="headerlink" title="Forward Pass"></a>Forward Pass</h3><p>把前面写的forward step组合一下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Run a vanilla RNN forward on an entire sequence of data. We assume an input</span></span><br><span class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The RNN uses a hidden</span></span><br><span class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></span><br><span class="line"><span class="string">    the RNN forward, we return the hidden states for all timesteps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data for the entire timeseries, of shape (N, T, D).</span></span><br><span class="line"><span class="string">    - h0: Initial hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></span><br><span class="line"><span class="string">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></span><br><span class="line"><span class="string">    - b: Biases of shape (H,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - h: Hidden states for the entire timeseries, of shape (N, T, H).</span></span><br><span class="line"><span class="string">    - cache: Values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N, T, D = x.shape</span><br><span class="line">    _, H = h0.shape</span><br><span class="line">    cache = []</span><br><span class="line">    h = np.zeros((N, T, H))</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        h0, cache_tmp = rnn_step_forward(x[:, t, :], h0, Wx, Wh, b)</span><br><span class="line">        h[:, t, :] = h0</span><br><span class="line">        cache.append(cache_tmp)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> h, cache</span><br></pre></td></tr></table></figure></p><h3 id="Backward-Pass-1"><a href="#Backward-Pass-1" class="headerlink" title="Backward Pass"></a>Backward Pass</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the backward pass for a vanilla RNN over an entire sequence of data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dh: Upstream gradients of all hidden states, of shape (N, T, H). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    NOTE: 'dh' contains the upstream gradients produced by the </span></span><br><span class="line"><span class="string">    individual loss functions at each timestep, *not* the gradients</span></span><br><span class="line"><span class="string">    being passed between timesteps (which you'll have to compute yourself</span></span><br><span class="line"><span class="string">    by calling rnn_step_backward in a loop).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient of inputs, of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - dh0: Gradient of initial hidden state, of shape (N, H)</span></span><br><span class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, H)</span></span><br><span class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)</span></span><br><span class="line"><span class="string">    - db: Gradient of biases, of shape (H,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N, T, H = dh.shape</span><br><span class="line">    x, _, _, _, _, _ = cache[<span class="number">0</span>]</span><br><span class="line">    _, D = x.shape</span><br><span class="line">    dx = np.zeros((N, T, D))</span><br><span class="line">    dWx = np.zeros((D, H))</span><br><span class="line">    dWh = np.zeros((H, H))</span><br><span class="line">    db = np.zeros((H,))</span><br><span class="line">    dh0 = np.zeros((N, H))</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        dx_tmp, dh0, dWx_tmp, dWh_tmp, db_tmp = rnn_step_backward(dh[:, t, :] + dh0, cache[t])</span><br><span class="line">        dx[:,t,:] = dx_tmp</span><br><span class="line">        dWx += dWx_tmp</span><br><span class="line">        dWh += dWh_tmp</span><br><span class="line">        db += db_tmp</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</span><br></pre></td></tr></table></figure><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>另外还要实现 word embedding 层，现在我们的就是给定词表（词表有V个词）中的下标（下标的范围是0 &lt;= idx &lt; V），映射到 D 维向量。</p><p>我们的RNN在时间序列的长度是T，也就是说要循环T次。所以，我们输入的一条样本$x_i$应该是shape =（T，）可以理解成一个有T个字符。但我们每次输入的是一个batch（batch size用N表示），即每次输入的样本shape =（N，T)。</p><p>这N*T每个都是一个字符（字符的种类应该不超过V种），word embedding就是把这些字符用相应的编码来替换（如本来是”a”，用一个长度为D的编码假如是”00001”来替换)，经过word embdding后的x就是shape = （N，T，D）。</p><h3 id="Forward-Pass-2"><a href="#Forward-Pass-2" class="headerlink" title="Forward Pass"></a>Forward Pass</h3><p>把X中的每个元素（每个元素都属于0 &lt;= idx &lt; V的范围），用w中的编码表示。</p><p>实现可以先用for loop的写法，加深理解。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">N, T = x.shape</span><br><span class="line">V, D = W.shape</span><br><span class="line">out = np.zeros((N, T, D))</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        tmp = W[x[n,t]]</span><br><span class="line">        out[n, t, :] = tmp</span><br></pre></td></tr></table></figure></p><p>简洁写法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_embedding_forward</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for word embeddings. We operate on minibatches of size N where</span></span><br><span class="line"><span class="string">    each sequence has length T. We assume a vocabulary of V words, assigning each</span></span><br><span class="line"><span class="string">    word to a vector of dimension D.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Integer array of shape (N, T) giving indices of words. Each element idx</span></span><br><span class="line"><span class="string">      of x muxt be in the range 0 &lt;= idx &lt; V.</span></span><br><span class="line"><span class="string">    - W: Weight matrix of shape (V, D) giving word vectors for all words.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Array of shape (N, T, D) giving word vectors for all input words.</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = W[x]</span><br><span class="line">    cache = x, W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure></p><h3 id="Backward-Pass-2"><a href="#Backward-Pass-2" class="headerlink" title="Backward Pass"></a>Backward Pass</h3><p>backword要用到<code>np.add.at()</code>函数，参考：<a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ufunc.at.html" target="_blank" rel="noopener">numpy.ufunc.at</a></p><p>同样可以先写出for loop的版本<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x, W = cache</span><br><span class="line">dW = np.zeros(W.shape)</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        dW[x[n, t]] += dout[n, t, :]</span><br></pre></td></tr></table></figure></p><p>简洁写法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_embedding_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for word embeddings. We cannot back-propagate into the words</span></span><br><span class="line"><span class="string">    since they are integers, so we only return gradient for the word embedding</span></span><br><span class="line"><span class="string">    matrix.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    HINT: Look up the function np.add.at</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream gradients of shape (N, T, D)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dW: Gradient of word embedding matrix, of shape (V, D).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    N, T, D = dout.shape</span><br><span class="line">    x, W = cache</span><br><span class="line">    dW = np.zeros(W.shape)</span><br><span class="line">    np.add.at(dW, x, dout)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dW</span><br></pre></td></tr></table></figure></p><h2 id="Loss-and-grads"><a href="#Loss-and-grads" class="headerlink" title="Loss and grads"></a>Loss and grads</h2><p>loss和grads就按照他给的层结构进行组装，然后再backward就ok了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">h0, cache1 = affine_forward(features, W_proj, b_proj)   <span class="comment"># h0 (N, H)</span></span><br><span class="line">x, cache2 = word_embedding_forward(captions_in, W_embed)  <span class="comment"># x (N, T, D)</span></span><br><span class="line">h, cache3 = rnn_forward(x, h0, Wx, Wh, b)</span><br><span class="line">scores, cache4 = temporal_affine_forward(h, W_vocab, b_vocab)</span><br><span class="line">loss, dscores = temporal_softmax_loss(scores, captions_out, mask)</span><br><span class="line"></span><br><span class="line">dh, grads[<span class="string">'W_vocab'</span>], grads[<span class="string">'b_vocab'</span>] = temporal_affine_backward(dscores, cache4)</span><br><span class="line">dx, dh0, grads[<span class="string">'Wx'</span>], grads[<span class="string">'Wh'</span>], grads[<span class="string">'b'</span>] = rnn_backward(dh, cache3)</span><br><span class="line">grads[<span class="string">'W_embed'</span>] = word_embedding_backward(dx, cache2)</span><br><span class="line">_, grads[<span class="string">'W_proj'</span>], grads[<span class="string">'b_proj'</span>] = affine_backward(dh0, cache1)</span><br></pre></td></tr></table></figure></p><h2 id="Sample"><a href="#Sample" class="headerlink" title="Sample"></a>Sample</h2><p>Sample是再测试的时候，根据输入的图片特征，自己生成captions。</p><p>具体过程如下：</p><p><strong>1.输入：</strong></p><p><code>features</code> 是经过CNN训练后得出来一个batch图片的特征，维度时$[N\times D]$<br><code>max_length</code> 是RNN训练的轮数（时间序列的长度）</p><p><strong>2.预处理：获取$h_0$，$x_1$</strong></p><p><img src="http://static.zybuluo.com/BYWMM/2vcb33s8sog6yv9fjoimj8wn/zheng4.png"><br>第一步，对特征维度进行处理，变成$h_0$（N,H），即RNN中h的初始值。之后将进行对图像特征的“解码”工作。<br>第二步，RNN的第一个$x_1$是<code>&lt;start&gt;</code>这个字符，这是$x_1$的原始形态(N,)的一个向量，每个向量包含<code>&lt;start&gt;</code>这个字符，但这不是$x_1$，需要一个embedding操作，进行编码得到$x_1$（N,D）。</p><p><strong>3.循环<code>max_length</code>次rnn step:</strong></p><p>这个就很简单了，如图根据$h_{t-1}$和$x_t$算出$h_t$；<br>再根据$h_t$算出预测的$y_t$;<br>对$y_t$进行embedding就是下次的输入$x_{t+1}$;</p><p><img src="http://static.zybuluo.com/BYWMM/12udgmgwix26ttoc5gd1tjvl/zheng1.png" width="350"></p><p>4.最后的caption就是我们所有层输入的字符（进行embedding之前的字符）。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, features, max_length=<span class="number">30</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: Array of input image features of shape (N, D).</span></span><br><span class="line"><span class="string">    - max_length: Maximum length T of generated captions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - captions: Array of shape (N, max_length) giving sampled captions,where each element is an integer in the range [0, V). The first element of captions should be the first sampled word, not the &lt;START&gt; token.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N = features.shape[<span class="number">0</span>]</span><br><span class="line">    captions = self._null * np.ones((N, max_length), dtype=np.int32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Unpack parameters</span></span><br><span class="line">    W_proj, b_proj = self.params[<span class="string">'W_proj'</span>], self.params[<span class="string">'b_proj'</span>]</span><br><span class="line">    W_embed = self.params[<span class="string">'W_embed'</span>]</span><br><span class="line">    Wx, Wh, b = self.params[<span class="string">'Wx'</span>], self.params[<span class="string">'Wh'</span>], self.params[<span class="string">'b'</span>]</span><br><span class="line">    W_vocab, b_vocab = self.params[<span class="string">'W_vocab'</span>], self.params[<span class="string">'b_vocab'</span>]</span><br><span class="line"></span><br><span class="line">    h_prev, _ = affine_forward(features, W_proj, b_proj)  <span class="comment"># h0 (N, H)</span></span><br><span class="line">    cap_prev = np.repeat(self._start, N)</span><br><span class="line">    <span class="comment"># rnn时间序列的第一层，我们输入了batch size为N的N个字母（编码），所以输入为(N,D)</span></span><br><span class="line">    <span class="comment"># 当然，以后每层的输入都这样</span></span><br><span class="line">    captions[:, <span class="number">0</span>] = cap_prev</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, max_length):</span><br><span class="line">        <span class="comment"># 根据前一层的预测出的N个字母，作为本层的输入。首先进行编码</span></span><br><span class="line">        x = W_embed[cap_prev]   <span class="comment"># (N, D)</span></span><br><span class="line">        <span class="comment"># 编码之后，作为x_t进行输入，计算本层的输出h_t</span></span><br><span class="line">        h_next, _ = rnn_step_forward(x, h_prev, Wx, Wh, b)</span><br><span class="line">        <span class="comment"># 根据本层的h_t计算本层的预测字母y_t（每个batch各一个）</span></span><br><span class="line">        scores, _ = affine_forward(h_next, W_vocab, b_vocab)</span><br><span class="line">        cap_prev = np.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 把预测出来的字母保存下来</span></span><br><span class="line">        captions[:, t] = cap_prev</span><br><span class="line">        h_prev = h_next</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> captions</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      COCO dataset；Vanilla RNN实现;
    
    </summary>
    
      <category term="cs231n" scheme="https://bywmm.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="https://bywmm.github.io/tags/cs231n/"/>
    
      <category term="计算机视觉" scheme="https://bywmm.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="RNN" scheme="https://bywmm.github.io/tags/RNN/"/>
    
      <category term="Image Caption" scheme="https://bywmm.github.io/tags/Image-Caption/"/>
    
  </entry>
  
  <entry>
    <title>Keras samples:LeNet-5 on cifar10 dataset</title>
    <link href="https://bywmm.github.io/2018/12/14/keras%20samples-%20LeNet-5%20on%20cifar10%20dataset/"/>
    <id>https://bywmm.github.io/2018/12/14/keras samples- LeNet-5 on cifar10 dataset/</id>
    <published>2018-12-14T12:06:27.000Z</published>
    <updated>2019-10-24T03:48:36.881Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Keras-Samples-LeNet-5-on-cifar10-dataset"><a href="#Keras-Samples-LeNet-5-on-cifar10-dataset" class="headerlink" title="Keras Samples: LeNet-5 on cifar10 dataset"></a>Keras Samples: LeNet-5 on cifar10 dataset</h1><h2 id="从keras导入cifar10数据库"><a href="#从keras导入cifar10数据库" class="headerlink" title="从keras导入cifar10数据库"></a>从keras导入cifar10数据库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_cifar10</span><span class="params">(num_training=<span class="number">49000</span>, num_validation=<span class="number">1000</span>, num_test=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Fetch the CIFAR-10 dataset from the web</span></span><br><span class="line">    cifar10 = keras.datasets.cifar10.load_data()</span><br><span class="line">    (X_train, y_train), (X_test, y_test) = cifar10</span><br><span class="line">    X_train = np.asarray(X_train, dtype=np.float32)</span><br><span class="line">    y_train = np.asarray(y_train, dtype=np.int32).flatten()</span><br><span class="line">    X_test = np.asarray(X_test, dtype=np.float32)</span><br><span class="line">    y_test = np.asarray(y_test, dtype=np.int32).flatten()</span><br><span class="line">    <span class="comment"># Subsample the data</span></span><br><span class="line">    mask = range(num_training, num_training + num_validation)</span><br><span class="line">    X_val = X_train[mask]</span><br><span class="line">    y_val = y_train[mask]</span><br><span class="line">    mask = range(num_training)</span><br><span class="line">    X_train = X_train[mask]</span><br><span class="line">    y_train = y_train[mask]</span><br><span class="line">    mask = range(num_test)</span><br><span class="line">    X_test = X_test[mask]</span><br><span class="line">    y_test = y_test[mask]</span><br><span class="line">    <span class="comment"># Normaliza the data: subtract the mean pixel and divide by std</span></span><br><span class="line">    mean_pixel = X_train.mean(axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>), keepdims=<span class="keyword">True</span>)</span><br><span class="line">    std_pixel = X_train.std(axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>), keepdims=<span class="keyword">True</span>)</span><br><span class="line">    X_train = (X_train - mean_pixel) / std_pixel</span><br><span class="line">    X_val = (X_val - mean_pixel) / std_pixel</span><br><span class="line">    X_test = (X_test - mean_pixel) / std_pixel</span><br><span class="line">    <span class="comment"># one-hot the labels</span></span><br><span class="line">    y_train = keras.utils.to_categorical(y_train, <span class="number">10</span>)</span><br><span class="line">    y_val = keras.utils.to_categorical(y_val, <span class="number">10</span>)</span><br><span class="line">    y_test = keras.utils.to_categorical(y_test, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_val, y_val, X_test, y_test</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Train data shape:  (49000, 32, 32, 3)</span></span><br><span class="line"><span class="string">Train labels shape:  (49000, 10) float32</span></span><br><span class="line"><span class="string">Validation data shape:  (1000, 32, 32, 3)</span></span><br><span class="line"><span class="string">Validation labels shape:  (1000, 10)</span></span><br><span class="line"><span class="string">Test data shape:  (1000, 32, 32, 3)</span></span><br><span class="line"><span class="string">Test labels shape:  (1000, 10)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>labels需要进行one-hot因为预测采用softmax函数，需要每个标签的shape为(num_classes,)。</p><h2 id="LeNet-5模型"><a href="#LeNet-5模型" class="headerlink" title="LeNet-5模型"></a>LeNet-5模型</h2><p>LeNet-5，一个7层的卷积神经网络，被很多银行用于识别支票上的手写数字。</p><p><img src="http://static.zybuluo.com/BYWMM/v5xxuzkoohvw0cvowrnu059p/zheng.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_model</span><span class="params">(input_shape, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Define the input placeholder as a tensor with shape input_shape.</span></span><br><span class="line">    X_input = layers.Input(input_shape)</span><br><span class="line"></span><br><span class="line">    X = layers.Conv2D(filters=<span class="number">20</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                      activation=<span class="string">'relu'</span>)(X_input)</span><br><span class="line">    X = layers.MaxPool2D()(X)</span><br><span class="line">    X = layers.Conv2D(filters=<span class="number">50</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                      activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = layers.MaxPool2D()(X)</span><br><span class="line">    X = layers.Flatten()(X)</span><br><span class="line">    X = layers.Dense(<span class="number">500</span>, activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)(X)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(X_input, X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个模型实例</span></span><br><span class="line">model = simple_model((<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">model.compile(optimizer=keras.optimizers.SGD(lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="keyword">True</span>),</span><br><span class="line">              loss=keras.losses.categorical_crossentropy, metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">history = model.fit(X_train, y_train, epochs=<span class="number">15</span>, batch_size=<span class="number">64</span>, verbose=<span class="number">2</span>, validation_data=(X_val, y_val))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Train Loss = 0.0719343089648</span></span><br><span class="line"><span class="string">Train Accuracy = 0.976734693878</span></span><br><span class="line"><span class="string">Val Loss = 1.77683620453</span></span><br><span class="line"><span class="string">Val Accuracy = 0.708</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>通过训练集和验证集的差距可以看出，模型对训练集过拟合了。</p><p>训练曲线如下：</p><p><img src="http://static.zybuluo.com/BYWMM/jvu18jji8rxi5wj70aayw6j6/zhengxinyueshididi2.png" width="400"><br><img src="http://static.zybuluo.com/BYWMM/ha04z40gvcvd73t8b9snqva9/zhengxinyueshididi1.png" width="400"></p><p>可以看到训练中，虽然train的loss曲线逐渐下降，但是val的loss曲线却有上升。初以为有可能是学习率的问题。但是将学习率将为1e-3后，15个epoch后的结果如下：</p><p><img src="http://static.zybuluo.com/BYWMM/flwkpya9972374xnbuaeq4lj/zheng3.png" width="400"></p><p>貌似不错？其实train的loss曲线还比较直，还没有训练好，再把训练时间拉长，45个epoch后的结果如下：</p><p><img src="http://static.zybuluo.com/BYWMM/kqtzug2k4l1aigymmurlv4g8/zheng4.png" width="400"></p><p><strong>所以，val的loss曲线上升，并不是学习率的问题，而是模型过拟合产生的！</strong></p><h2 id="LeNet-5-Batch-Normalization"><a href="#LeNet-5-Batch-Normalization" class="headerlink" title="LeNet-5 + Batch Normalization"></a>LeNet-5 + Batch Normalization</h2><p>为减少模型的过拟合，希望加入各种正则化手段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_model</span><span class="params">(input_shape, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Define the input placeholder as a tensor with shape input_shape.</span></span><br><span class="line">    X_input = layers.Input(input_shape)</span><br><span class="line"></span><br><span class="line">    X = layers.Conv2D(filters=<span class="number">20</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                      activation=<span class="string">'relu'</span>)(X_input)</span><br><span class="line">    X = layers.BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = layers.MaxPool2D(strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line">    X = layers.Conv2D(filters=<span class="number">50</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                      activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = layers.BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = layers.MaxPool2D(strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line">    X = layers.Flatten()(X)</span><br><span class="line">    X = layers.Dense(<span class="number">500</span>)(X)</span><br><span class="line">    X = layers.BatchNormalization(scale=<span class="keyword">False</span>)(X)</span><br><span class="line">    X = layers.Activation(activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)(X)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(X_input, X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Train Loss = 0.00337406960365</span></span><br><span class="line"><span class="string">Train Accuracy = 1.0</span></span><br><span class="line"><span class="string">Val Loss = 0.965107679367</span></span><br><span class="line"><span class="string">Val Accuracy = 0.767</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>加上BN之后有点点好转，尝试把batchsize变小一点，以增加的随机性强度，无明显效果。</p><p><img src="http://static.zybuluo.com/BYWMM/t5rfqxwv7d20d9xutex5zs9c/zheng5.png" width="400"><br><img src="http://static.zybuluo.com/BYWMM/0bv8mim10mxazyn4xbhtbgx2/zheng6.png" width="400"></p><h2 id="LeNet-5-BN-Dropout"><a href="#LeNet-5-BN-Dropout" class="headerlink" title="LeNet-5 + BN + Dropout"></a>LeNet-5 + BN + Dropout</h2><p>只有两个全连接层，所以，只能在之间防止一个Dropout层。试着把rate调的高一点（rate是失活神经元比例）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_model</span><span class="params">(input_shape, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Define the input placeholder as a tensor with shape input_shape.</span></span><br><span class="line">    X_input = layers.Input(input_shape)</span><br><span class="line"></span><br><span class="line">    X = layers.Conv2D(filters=<span class="number">20</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                      activation=<span class="string">'relu'</span>)(X_input)</span><br><span class="line">    X = layers.BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = layers.MaxPool2D(strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line">    X = layers.Conv2D(filters=<span class="number">50</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                      activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = layers.BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = layers.MaxPool2D(strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line">    X = layers.Flatten()(X)</span><br><span class="line">    X = layers.Dense(<span class="number">500</span>)(X)</span><br><span class="line">    X = layers.BatchNormalization(scale=<span class="keyword">False</span>)(X)</span><br><span class="line">    X = layers.Activation(activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = layers.Dropout(rate=<span class="number">0.7</span>)(X)</span><br><span class="line">    X = layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)(X)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(X_input, X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Train Loss = 0.36407692194</span></span><br><span class="line"><span class="string">Train Accuracy = 0.879408163265</span></span><br><span class="line"><span class="string">Val Loss = 0.750349837303</span></span><br><span class="line"><span class="string">Val Accuracy = 0.76</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p><img src="http://static.zybuluo.com/BYWMM/hly08orjvpdvx5xjtxh8abvy/zheng8.png" width="400"><br><img src="http://static.zybuluo.com/BYWMM/lki14uux0i48el6ih1vwwi4o/zheng7.png" width="400"></p><p>结果还是差强人意，后觉得是网络结构太复杂了？尝试减少层数，train结果反而也将下来了。</p><p>把epoch增加到50：</p><p><img src="http://static.zybuluo.com/BYWMM/nxsi0igaa0381ocetrqclerp/zheng2.png" width="400"><br><img src="http://static.zybuluo.com/BYWMM/gyu41fc67ytbu9jsg2al2dff/zheng1.png" width="400"><br>模型还是很容易拟合训练集，10个epoch之后，训练对val的损失或准确度的贡献就不大了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Train Loss = 0.029131395354</span></span><br><span class="line"><span class="string">Train Accuracy = 0.997387755102</span></span><br><span class="line"><span class="string">Val Loss = 0.905600978374</span></span><br><span class="line"><span class="string">Val Accuracy = 0.775</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_cifar10</span><span class="params">(num_training=<span class="number">49000</span>, num_validation=<span class="number">1000</span>, num_test=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Fetch the CIFAR-10 dataset from the web</span></span><br><span class="line">    cifar10 = keras.datasets.cifar10.load_data()</span><br><span class="line">    (X_train, y_train), (X_test, y_test) = cifar10</span><br><span class="line">    X_train = np.asarray(X_train, dtype=np.float32)</span><br><span class="line">    y_train = np.asarray(y_train, dtype=np.int32).flatten()</span><br><span class="line">    X_test = np.asarray(X_test, dtype=np.float32)</span><br><span class="line">    y_test = np.asarray(y_test, dtype=np.int32).flatten()</span><br><span class="line">    <span class="comment"># Subsample the data</span></span><br><span class="line">    mask = range(num_training, num_training + num_validation)</span><br><span class="line">    X_val = X_train[mask]</span><br><span class="line">    y_val = y_train[mask]</span><br><span class="line">    mask = range(num_training)</span><br><span class="line">    X_train = X_train[mask]</span><br><span class="line">    y_train = y_train[mask]</span><br><span class="line">    mask = range(num_test)</span><br><span class="line">    X_test = X_test[mask]</span><br><span class="line">    y_test = y_test[mask]</span><br><span class="line">    <span class="comment"># Normaliza the data: subtract the mean pixel and divide by std</span></span><br><span class="line">    mean_pixel = X_train.mean(axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>), keepdims=<span class="keyword">True</span>)</span><br><span class="line">    std_pixel = X_train.std(axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>), keepdims=<span class="keyword">True</span>)</span><br><span class="line">    X_train = (X_train - mean_pixel) / std_pixel</span><br><span class="line">    X_val = (X_val - mean_pixel) / std_pixel</span><br><span class="line">    X_test = (X_test - mean_pixel) / std_pixel</span><br><span class="line">    <span class="comment"># one-hot the labels</span></span><br><span class="line">    y_train = keras.utils.to_categorical(y_train, <span class="number">10</span>)</span><br><span class="line">    y_val = keras.utils.to_categorical(y_val, <span class="number">10</span>)</span><br><span class="line">    y_test = keras.utils.to_categorical(y_test, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_val, y_val, X_test, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Invoke the above function to get our data.</span></span><br><span class="line">X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()</span><br><span class="line">print(<span class="string">'Train data shape: '</span>, X_train.shape)</span><br><span class="line">print(<span class="string">'Train labels shape: '</span>, y_train.shape, y_train.dtype)</span><br><span class="line">print(<span class="string">'Validation data shape: '</span>, X_val.shape)</span><br><span class="line">print(<span class="string">'Validation labels shape: '</span>, y_val.shape)</span><br><span class="line">print(<span class="string">'Test data shape: '</span>, X_test.shape)</span><br><span class="line">print(<span class="string">'Test labels shape: '</span>, y_test.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_model</span><span class="params">(input_shape, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Define the input placeholder as a tensor with shape input_shape.</span></span><br><span class="line">    X_input = layers.Input(input_shape)</span><br><span class="line"></span><br><span class="line">    X = layers.Conv2D(filters=<span class="number">16</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                      activation=<span class="string">'relu'</span>)(X_input)</span><br><span class="line">    X = layers.BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = layers.MaxPool2D(strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line">    X = layers.Conv2D(filters=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                      activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = layers.BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = layers.MaxPool2D(strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line">    X = layers.Flatten()(X)</span><br><span class="line">    X = layers.Dense(<span class="number">500</span>)(X)</span><br><span class="line">    X = layers.BatchNormalization(scale=<span class="keyword">False</span>)(X)</span><br><span class="line">    X = layers.Activation(activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = layers.Dropout(rate=<span class="number">0.7</span>)(X)</span><br><span class="line">    X = layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)(X)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(X_input, X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = simple_model((<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), <span class="number">10</span>)</span><br><span class="line">model.compile(optimizer=keras.optimizers.SGD(lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="keyword">True</span>),</span><br><span class="line">              loss=keras.losses.categorical_crossentropy, metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line">history = model.fit(X_train, y_train, epochs=<span class="number">50</span>, batch_size=<span class="number">64</span>, verbose=<span class="number">2</span>, validation_data=(X_val, y_val))</span><br><span class="line">plt.plot(history.history[<span class="string">'loss'</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">'val_loss'</span>])</span><br><span class="line">plt.title(<span class="string">"model loss"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"loss"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"epoch"</span>)</span><br><span class="line">plt.legend([<span class="string">"train"</span>,<span class="string">"val"</span>],loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(history.history[<span class="string">'acc'</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">'val_acc'</span>])</span><br><span class="line">plt.title(<span class="string">"model acc"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"acc"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"epoch"</span>)</span><br><span class="line">plt.legend([<span class="string">"train"</span>,<span class="string">"val"</span>],loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">preds = model.evaluate(x=X_train, y=y_train)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Train Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"Train Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">preds = model.evaluate(x=X_val, y=y_val)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Val Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"Val Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      目标是训练一个简单的ConvNet来训练cifar10数据集。这里先采用了LeNet-5，在此基础上进行修改。
    
    </summary>
    
      <category term="DL&amp;ML Coding" scheme="https://bywmm.github.io/categories/DL-ML-Coding/"/>
    
    
      <category term="CNN" scheme="https://bywmm.github.io/tags/CNN/"/>
    
      <category term="cifar数据集" scheme="https://bywmm.github.io/tags/cifar%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>如何训练一个网络</title>
    <link href="https://bywmm.github.io/2018/12/10/%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C/"/>
    <id>https://bywmm.github.io/2018/12/10/如何训练一个网络/</id>
    <published>2018-12-10T05:32:46.000Z</published>
    <updated>2018-12-14T15:58:26.677Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何训练一个网络"><a href="#如何训练一个网络" class="headerlink" title="如何训练一个网络"></a>如何训练一个网络</h1><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>关于数据预处理我们有3个常用的符号，数据矩阵$X$，假设其尺寸是$[N\times{D}]$（$N$是数据样本的数量，D是数据的维度）。</p><h3 id="均值减法（Mean-subtraction）"><a href="#均值减法（Mean-subtraction）" class="headerlink" title="均值减法（Mean subtraction）"></a>均值减法（Mean subtraction）</h3><p><strong>均值减法</strong>是预处理最常用的形式。它对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。</p><p>在numpy中，该操作可以通过以下代码实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X -= np.mean(X, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p>而对于图像，更常用的是对所有像素都减去一个值，可以用以下代码实现，也可以在3个颜色通道上分别操作。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X -= np.mean(X)</span><br></pre></td></tr></table></figure></p><h3 id="归一化（Normalization）"><a href="#归一化（Normalization）" class="headerlink" title="归一化（Normalization）"></a>归一化（Normalization）</h3><p><strong>归一化</strong>是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。</p><p>第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X /= np.std(X, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p>第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。</p><p><img src="http://static.zybuluo.com/BYWMM/j5fc79pdauwc8pft45wsez28/e743b6777775b1671c3b5503d7afbbc4_r.jpg"></p><p>一般数据预处理流程：<strong>左边：</strong>原始的2维输入数据。<strong>中间：</strong>在每个维度上都减去平均值后得到零中心化数据，现在数据是以原点为中心的。<strong>右边：</strong>每个维度都除以其标准差来调整其数值范围。</p><h3 id="PCA和白化（Whitening）"><a href="#PCA和白化（Whitening）" class="headerlink" title="PCA和白化（Whitening）"></a>PCA和白化（Whitening）</h3><p><strong>PCA</strong>中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设输入数据矩阵X的尺寸为[N x D]</span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># 对数据进行零中心化(重要)</span></span><br><span class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># 得到数据的协方差矩阵</span></span><br></pre></td></tr></table></figure></p><p>数据<strong>协方差矩阵</strong>的第$(i,j)$个元素是数据第$i$个和第$j$个维度的协方差。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和半正定的，可以对数据协方差矩阵进行<strong>SVD（奇异值分解）</strong>运算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">U,S,V = np.linalg.svd(cov)</span><br></pre></td></tr></table></figure></p><p>U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了<strong>去除数据相关性</strong>，将已经零中心化处理过的原始数据投影到特征基准上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xrot = np.dot(X,U) <span class="comment"># 对数据去相关性</span></span><br></pre></td></tr></table></figure><p>注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算Xrot的协方差矩阵，将会看到它是对角对称的。np.linalg.svd的一个良好性质是在它的返回值U中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有方差的维度。 这个操作也被称为主成分分析（ Principal Component Analysis 简称PCA）降维：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment"># Xrot_reduced 变成 [N x 100]</span></span><br></pre></td></tr></table></figure></p><p>经过上面的操作，将原始的数据集的大小由$[N\times{D}]$降到了$[N\times100]$，留下了数据中包含<strong>最大方差</strong>的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。</p><p><strong>白化（whitening）</strong>的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：<strong>如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。</strong>该操作的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据进行白化操作:</span></span><br><span class="line"><span class="comment"># 除以特征值 </span></span><br><span class="line">Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure><p>注意：该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将<strong>所有维度都拉伸到相同的数值范围</strong>，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的<code>epsilon</code>值来解决（例如：采用比1e-5更大的值）。</p><p><img src="http://static.zybuluo.com/BYWMM/dmwy85ti1fubvf32hid4yc95/aae11de6e6a29f50d46b9ea106fbb02a_r.jpg"></p><p><strong>左边：</strong>二维的原始数据。<strong>中间：</strong>经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。<strong>右边：</strong>每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。</p><p>我们可以使用CIFAR-10数据将这些变化可视化出来。CIFAR-10训练集的大小是50000x3072，其中每张图片都可以拉伸为3072维的行向量。我们可以计算$[3072\times3072]$的协方差矩阵然后进行奇异值分解（比较耗费计算性能），那么经过计算的特征向量看起来是什么样子呢？</p><ul><li>首先是一个用于演示的集合，含49张图片：</li></ul><p><img src="http://static.zybuluo.com/BYWMM/35mlegzeh5iynqdzxjxrzx83/12afsdjlfask%20%283%29.png" width="300"></p><ul><li>第二张是3072个特征值向量中的前144个。靠前面的特征向量解释了数据中大部分的方差，可以看见它们与图像中较低的频率相关。</li></ul><p><img src="http://static.zybuluo.com/BYWMM/8w0wxrollbdj5cu2yfnyak3r/12afsdjlfask%20%281%29.png" width="300"></p><ul><li>第三张是49张经过了PCA降维处理的图片，展示了144个特征向量。这就是说，展示原始图像是每个图像用3072维的向量，向量中的元素是图片上某个位置的像素在某个颜色通道中的亮度值。而现在每张图片只使用了一个144维的向量，其中每个元素表示了特征向量对于组成这张图片的贡献度。为了让图片能够正常显示，需要将144维度重新变成基于像素基准的3072个数值。因为U是一个旋转，可以通过乘以U.transpose()[:144,:]来实现，然后将得到的3072个数值可视化。可以看见图像变得有点模糊了，这正好说明前面的特征向量获取了较低的频率。然而，大多数信息还是保留了下来。</li></ul><p><img src="http://static.zybuluo.com/BYWMM/yqsjhxzmjmaqse8ohf6bum98/12afsdjlfask%20%284%29.png" width="300"></p><ul><li>第四张是将“白化”后的数据进行显示。其中144个维度中的方差都被压缩到了相同的数值范围。然后144个白化后的数值通过乘以U.transpose()[:144,:]转换到图像像素基准上。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。</li></ul><p><img src="http://static.zybuluo.com/BYWMM/xib9jemewo8uezzlaimnttf6/12afsdjlfask%20%282%29.png" width="300"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p><strong>实践操作：</strong>这里提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行<strong>零中心化</strong>操作还是非常重要的，对每个像素进行<strong>归一化</strong>也很常见。</p><p><strong>常见错误：</strong>进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。应该怎么做呢？应该先分成训练/验证/测试集，<strong>只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong></p><h2 id="选择网络结构"><a href="#选择网络结构" class="headerlink" title="选择网络结构"></a>选择网络结构</h2><p><img src="http://static.zybuluo.com/BYWMM/kuutwq5bvbc383rislzdrzy3/%E9%80%89%E6%8B%A9%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png"></p><p>例如这个单隐层网络，一开始有50个隐层神经元，但基本上，我们可以选择任何我们想要的网络结构。</p><h2 id="初始化网络"><a href="#初始化网络" class="headerlink" title="初始化网络"></a>初始化网络</h2><h3 id="错误：全零初始化"><a href="#错误：全零初始化" class="headerlink" title="错误：全零初始化"></a>错误：全零初始化</h3><p>在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。</p><h3 id="小随机数初始化"><a href="#小随机数初始化" class="headerlink" title="小随机数初始化"></a>小随机数初始化</h3><p>因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)</span><br></pre></td></tr></table></figure></p><p>其中randn函数是基于零均值和标准差的一个高斯分布来生成随机数的。根据这个式子，<strong>每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布</strong>，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。</p><blockquote><p><strong>warn：</strong>并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。</p></blockquote><h3 id="使用1-sqrt-n-校准方差"><a href="#使用1-sqrt-n-校准方差" class="headerlink" title="使用1/sqrt(n)校准方差"></a>使用1/sqrt(n)校准方差</h3><p>上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(n) / sqrt(n)</span><br></pre></td></tr></table></figure></p><p>其中n是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。</p><h3 id="稀疏初始化（Sparse-initialization）"><a href="#稀疏初始化（Sparse-initialization）" class="headerlink" title="稀疏初始化（Sparse initialization）"></a>稀疏初始化（Sparse initialization）</h3><p>另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。</p><h3 id="偏置（biases）的初始化"><a href="#偏置（biases）的初始化" class="headerlink" title="偏置（biases）的初始化"></a>偏置（biases）的初始化</h3><p>通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。</p><p>实践中，当前的推荐是使用ReLU激活函数，并且使用w = np.random.randn(n) * sqrt(2.0/n)来进行权重初始化，关于这一点，<a href="http://link.zhihu.com/?target=http://arxiv-web3.library.cornell.edu/abs/1502.01852" target="_blank" rel="noopener">这篇文章</a>有讨论。</p><h2 id="学习之前：合理性检查（sanity-checks）"><a href="#学习之前：合理性检查（sanity-checks）" class="headerlink" title="学习之前：合理性检查（sanity checks）"></a>学习之前：合理性检查（sanity checks）</h2><p>在进行费时费力的最优化之前，最好进行一些合理性检查：</p><h3 id="寻找特定情况的正确损失值"><a href="#寻找特定情况的正确损失值" class="headerlink" title="寻找特定情况的正确损失值"></a>寻找特定情况的正确损失值</h3><p>在使用小参数进行初始化时，确保得到的损失值与期望一致。</p><p><img src="http://static.zybuluo.com/BYWMM/767fnlxme7p9yquq2h8d47vs/image_1ctibg5kl1k9qb79eteh8vfdt11.png" width="600"></p><p>当我们weight很小且很分散的时候，趋近于随机分类。所以Softmax损失（让<strong>正则化强度为0</strong>），约等于$-\log{\frac{1}{C}}$。</p><p>例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。</p><h3 id="提高正则化强度时导致损失值变大"><a href="#提高正则化强度时导致损失值变大" class="headerlink" title="提高正则化强度时导致损失值变大"></a>提高正则化强度时导致损失值变大</h3><p>提高正则化强度，看损失值是否变大</p><p><img src="http://static.zybuluo.com/BYWMM/dksv9zm09dkgqsrurr7fep50/123jlkjagksgrkas.png" width="600"></p><h3 id="对小数据子集过拟合"><a href="#对小数据子集过拟合" class="headerlink" title="对小数据子集过拟合"></a>对小数据子集过拟合</h3><p>最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个<strong>很小的数据集上进行训练（比如20个数据）</strong>，然后确保能到达0的损失值。</p><p>进行这个实验的时候，最好让<strong>正则化强度为0</strong>，不然它会阻止得到0的损失。</p><blockquote><p>但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。</p></blockquote><p>现在就全部完成了 完整性检查操作（sanity checks）</p><h2 id="超参数调参"><a href="#超参数调参" class="headerlink" title="超参数调参"></a>超参数调参</h2><h3 id="首先调学习率"><a href="#首先调学习率" class="headerlink" title="首先调学习率"></a>首先调学习率</h3><p>全部数据集，小的正则化项，首先进行学习率的调参。</p><p>可以试一些学习率的值。这里用了1e-6可以看到<strong>损失值几乎不变，原因可能是学习率太小</strong>。</p><p><img src="http://static.zybuluo.com/BYWMM/ugytwpn9obz36we4v6yalaif/barelychangelearningrate.png" width="700"></p><p>但是，<strong>注意到虽然cost几乎不变，但准确度却提高到了20%左右，是为什么呢？</strong></p><p><strong>原因：</strong>学习率太小了，梯度下降步幅非常小，因此我们的损失项很接近。但是所有的分布都在朝着正确的方向轻微的移动，weight也在朝着正确的方向改变。正确率有较大提升是因为，我们认为分数最大的为该样本的分类，虽然分布还是很分散，但是已经向正确方向有所偏移，大小关系应该有所改变。</p><p>然后选择了另一个极端，1e6一个非常大的学习率</p><p><img src="http://static.zybuluo.com/BYWMM/luulnjg4svsemb5fqemygc8o/V7L_NXF2PR126SNSW_SD4VI.png" width="600"></p><p><strong>损失值爆炸变成NaN，原因可能是学习率太大</strong>，可以试试更小的学习率</p><p>一般我们的学习率设为[1e-3,1e-5]，这是一个我们想要交叉验证的粗略范围。可以在这个范围里试一试不同的学习率。</p><h3 id="超参数优化"><a href="#超参数优化" class="headerlink" title="超参数优化"></a>超参数优化</h3><p><strong>调参的目标：</strong> 找到交叉验证中表现最好的参数（组合），即在训练集上训练，验证集上测试，选验证集高的参数。</p><p>大概分为两步。</p><p><strong>第一步，粗略选择好的参数来确定一个区间</strong></p><p>首先选几个比较分散的数值，然后用几个epochs来训练。通过几个epochs就可以知道那些是比较好的参数，那些是比较差的。通常只要这样做就可以发现一个较好的区间。例如首先用5个epochs进行搜索</p><p><img src="http://static.zybuluo.com/BYWMM/2tlrul7g9peg8ky3n157n7mj/bukensngasldjflaskjg.png" width="600"></p><p>这些区间就是我们想要进一步细化的区域。</p><p><strong>第二步，是在区间内进行精确地搜索</strong></p><p>接下来就可以调整变化范围，在第一步找到的区间中，精确的搜索</p><p><img src="http://static.zybuluo.com/BYWMM/jhwr27cpojz912zc4io9xce0/8JWV6OINT%28OWT%5BFC322O5XA.png" width="600"></p><p>需要注意的一点是，<strong>通常采用对数来优化</strong>，效果会更好。</p><blockquote><p>比如用在[0.01,100]与其用均匀采样，不如用10的幂次进行采样。<br>因为学习率是乘以梯度进行更新，具有乘法效应，所以考虑学习率的时候用一些值得乘或除的值更合理。</p></blockquote><p>这就产生了一个问题，这里所有好的学习率都在1e-4左右，如果我们一开始的调参区间就是[0，1e-4]的话，这样最后选出的学习率都在这个区间的边缘。这样不太好，因为我们没法在全部的空间上充分寻找，可能1e-5或者1e-6中也有我们想要最好的结果。</p><h3 id="参数搜索时的采样"><a href="#参数搜索时的采样" class="headerlink" title="参数搜索时的采样"></a>参数搜索时的采样</h3><p><img src="http://static.zybuluo.com/BYWMM/vq1lp11zxyzy6a7979vh4oft/klgajfdkjasdf.png" width="600"></p><p>网格采样不如用一种随机排列的方式进行采样。<br>随机采样是考虑对一个超过一个变量的函数而言，算是一种稍微有效的降维，可以获得更多有用的信息。</p><h2 id="检查整个学习过程"><a href="#检查整个学习过程" class="headerlink" title="检查整个学习过程"></a>检查整个学习过程</h2><p>在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。</p><p>在下面的图表中，x轴通常都是表示周期（epochs）单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>训练期间第一个要跟踪的数值就是损失值，它在前向传播时对每个独立的批数据进行计算。下图展示的是随着损失值随时间的变化，尤其是曲线形状会给出关于学习率设置的情况：</p><p><img src="http://static.zybuluo.com/BYWMM/rr2w1ve77pmzqce7cbmgvsqe/753f398b46cc28c1916d6703cf2080f5_r.jpg"></p><p><strong>左图</strong>展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。<br><strong>右图</strong>显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。</p><p>损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。</p><p>有的研究者喜欢用对数域对损失函数值作图。因为学习过程一般都是采用指数型的形状，图表就会看起来更像是能够直观理解的直线，而不是呈曲棍球一样的曲线状。还有，如果多个交叉验证模型在一个图上同时输出图像，它们之间的差异就会比较明显。</p><p>有时候损失函数看起来很有意思：lossfunctions.tumblr.com。</p><p>当在你观察学习率曲线的时候，如果它在一定时间内很平滑，然后突然开始下降，可能是初始值没有设好。<br>从图上可以看出，刚开始的时候梯度变化并不太好，什么也没学到；到达某点后突然开始下降，就像刚开始训练一样<br><img src="http://static.zybuluo.com/BYWMM/11lbsac9nvxv3yrqvfjdlzas/fhjsagjksahg.png" width="400"></p><h3 id="训练集和验证集准确率"><a href="#训练集和验证集准确率" class="headerlink" title="训练集和验证集准确率"></a>训练集和验证集准确率</h3><p>在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。这个图表能够展现知道模型过拟合的程度：</p><p><img src="http://static.zybuluo.com/BYWMM/x51clwpraa14lu5uso1cmrcf/jkdhfa.png"></p><p>在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。<br>————————————————————————————————————————</p><h3 id="权重更新比例"><a href="#权重更新比例" class="headerlink" title="权重更新比例"></a>权重更新比例</h3><p>最后一个应该跟踪的量是权重中更新值的数量和全部值的数量之间的比例。注意：是更新的，而不是原始梯度（比如，在普通sgd中就是梯度乘以学习率）。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设参数向量为W，其梯度向量为dW</span></span><br><span class="line">param_scale = np.linalg.norm(W.ravel())</span><br><span class="line">update = -learning_rate*dW <span class="comment"># 简单SGD更新</span></span><br><span class="line">update_scale = np.linalg.norm(update.ravel())</span><br><span class="line">W += update <span class="comment"># 实际更新</span></span><br><span class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># 要得到1e-3左右</span></span><br></pre></td></tr></table></figure><p>相较于跟踪最大和最小值，有研究者更喜欢计算和跟踪梯度的范式及其更新。这些矩阵通常是相关的，也能得到近似的结果。</p><h3 id="每层的激活数据及梯度分布"><a href="#每层的激活数据及梯度分布" class="headerlink" title="每层的激活数据及梯度分布"></a>每层的激活数据及梯度分布</h3><p>一个不正确的初始化可能让学习过程变慢，甚至彻底停止。还好，这个问题可以比较简单地诊断出来。其中一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。</p><h3 id="第一层可视化"><a href="#第一层可视化" class="headerlink" title="第一层可视化"></a>第一层可视化</h3><p>最后，如果数据是图像像素数据，那么把第一层特征可视化会有帮助：</p><p><img src="http://static.zybuluo.com/BYWMM/iw44xxd3emfrs04lcg0kz6hi/96573094f9d7f4b3b188069726840a2e_r.jpg"></p><p>将神经网络第一层的权重可视化的例子。左图中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。右图的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。</p>]]></content>
    
    <summary type="html">
    
      数据预处理；选择网络结构；初始化网络；合理性检查；超参数调参；检查整个学习过程
    
    </summary>
    
      <category term="神经网络" scheme="https://bywmm.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://bywmm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>cs231n assignment2(ConvolutionalNetworks)</title>
    <link href="https://bywmm.github.io/2018/12/05/cs231n_assignment2(ConvolutionalNetworks)/"/>
    <id>https://bywmm.github.io/2018/12/05/cs231n_assignment2(ConvolutionalNetworks)/</id>
    <published>2018-12-05T13:18:25.000Z</published>
    <updated>2019-10-24T03:42:46.334Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment2-ConvolutionalNetworks"><a href="#cs231n-assignment2-ConvolutionalNetworks" class="headerlink" title="cs231n assignment2(ConvolutionalNetworks)"></a>cs231n assignment2(ConvolutionalNetworks)</h1><h2 id="Convolution-Naive-forward-pass"><a href="#Convolution-Naive-forward-pass" class="headerlink" title="Convolution: Naive forward pass"></a>Convolution: Naive forward pass</h2><p><strong>Input data of shape：</strong> $(N, C, H_{prev}, W_{prev})$<br>其中$N$是样本数，$C$是channel数。<br>下一层的$H,W$可由以下公式得出：</p><script type="math/tex; mode=display">H = \lfloor \frac{H_{prev} - HH + 2 \times pad}{stride} \rfloor +1</script><script type="math/tex; mode=display">W = \lfloor \frac{W_{prev} - WW + 2 \times pad}{stride} \rfloor +1</script><p><strong>Output data of shape：</strong> $(N, F, H, W)$<br>其中$F$是本层filters的个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward_naive</span><span class="params">(x, w, b, conv_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input consists of N data points, each with C channels, height H and</span></span><br><span class="line"><span class="string">    width W. We convolve each input with F different filters, where each filter</span></span><br><span class="line"><span class="string">    spans all C channels and has height HH and width WW.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - w: Filter weights of shape (F, C, HH, WW)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (F,)</span></span><br><span class="line"><span class="string">    - conv_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - 'stride': The number of pixels between adjacent receptive fields in the</span></span><br><span class="line"><span class="string">        horizontal and vertical directions.</span></span><br><span class="line"><span class="string">      - 'pad': The number of pixels that will be used to zero-pad the input. </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)</span></span><br><span class="line"><span class="string">    along the height and width axes of the input. Be careful not to modfiy the original</span></span><br><span class="line"><span class="string">    input x directly.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, F, H', W') where H' and W' are given by</span></span><br><span class="line"><span class="string">      H' = 1 + (H + 2 * pad - HH) / stride</span></span><br><span class="line"><span class="string">      W' = 1 + (W + 2 * pad - WW) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, w, b, conv_param)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the convolutional forward pass.                         #</span></span><br><span class="line">    <span class="comment"># Hint: you can use the function np.pad for padding.                      #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H_prev, W_prev = x.shape</span><br><span class="line">    F, _, HH, WW = w.shape</span><br><span class="line">    stride = conv_param[<span class="string">'stride'</span>]   <span class="comment"># stride</span></span><br><span class="line">    pad = conv_param[<span class="string">'pad'</span>]   <span class="comment"># pad</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># python3除法的结果都是float</span></span><br><span class="line">    H_out = <span class="number">1</span> + (H_prev + <span class="number">2</span> * pad - HH) // stride</span><br><span class="line">    W_out = <span class="number">1</span> + (W_prev + pad * <span class="number">2</span> - WW) // stride</span><br><span class="line">    Z = np.zeros((N, F, H_out, W_out))</span><br><span class="line">    <span class="comment"># zero padding</span></span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), <span class="string">'constant'</span>, constant_values=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N): </span><br><span class="line">        <span class="keyword">for</span> h_i <span class="keyword">in</span> range(H_out):  </span><br><span class="line">            <span class="keyword">for</span> w_i <span class="keyword">in</span> range(W_out): </span><br><span class="line">                </span><br><span class="line">                h_start = h_i * stride</span><br><span class="line">                h_end = h_start + HH</span><br><span class="line">                w_start = w_i * stride</span><br><span class="line">                w_end = w_start + WW</span><br><span class="line">                </span><br><span class="line">                xi_slice = x[i, :, h_start:h_end, w_start:w_end]</span><br><span class="line">                <span class="keyword">for</span> f <span class="keyword">in</span> range(F):                   <span class="comment"># loop over channels (= #filters) of the output volume</span></span><br><span class="line">                    Z[i, f, h_i, w_i] = np.sum(np.multiply(xi_slice, w[f])) + b[f]</span><br><span class="line">    out = Z</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = (x, w, b, conv_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><h2 id="Convolution-Naive-backward-pass"><a href="#Convolution-Naive-backward-pass" class="headerlink" title="Convolution: Naive backward pass"></a>Convolution: Naive backward pass</h2><p>在Forward Pass中，最关键的就这一句代码了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z[i, f, h_i, w_i] = np.sum(np.multiply(xi_slice, w[f])) + b[f]</span><br></pre></td></tr></table></figure><p>反向传播中，就根据这个来进行求导：</p><script type="math/tex; mode=display">dX\ += \sum _{h=0} ^{H} \sum_{w=0} ^{W} W_c \times dZ_{hw}</script><script type="math/tex; mode=display">dW_c\ += \sum _{h=0} ^{H} \sum_{w=0} ^ {W} x_{slice} \times dZ_{hw}</script><script type="math/tex; mode=display">db\ += \sum_h \sum_w dZ_{hw}</script><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward_naive</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives.</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the convolutional backward pass.                        #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    x, w, b, conv_param = cache</span><br><span class="line">    stride = conv_param[<span class="string">'stride'</span>]  <span class="comment"># stride</span></span><br><span class="line">    pad = conv_param[<span class="string">'pad'</span>]  <span class="comment"># pad</span></span><br><span class="line"></span><br><span class="line">    N, C, H_prev, W_prev = x.shape</span><br><span class="line">    F, _, HH, WW = w.shape</span><br><span class="line">    N, F, H_out, W_out = dout.shape</span><br><span class="line">    <span class="comment"># zero padding</span></span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), <span class="string">'constant'</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    dx = np.zeros(x.shape)</span><br><span class="line">    dx_pad = np.zeros(x_pad.shape)</span><br><span class="line">    dw = np.zeros(w.shape)</span><br><span class="line">    db = np.zeros(b.shape)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):  <span class="comment"># loop over the batch of training examples</span></span><br><span class="line">        xi = x_pad[i]  <span class="comment"># Select ith training example's padded activation</span></span><br><span class="line">        <span class="keyword">for</span> h_i <span class="keyword">in</span> range(H_out):  <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w_i <span class="keyword">in</span> range(W_out):  <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                h_start = h_i * stride</span><br><span class="line">                h_end = h_start + HH</span><br><span class="line">                w_start = w_i * stride</span><br><span class="line">                w_end = w_start + WW</span><br><span class="line">                xi_slice = xi[:, h_start:h_end, w_start:w_end]</span><br><span class="line">                <span class="keyword">for</span> f <span class="keyword">in</span> range(F):  <span class="comment"># loop over channels (= #filters) of the output volume</span></span><br><span class="line">                    dx_pad[i, :, h_start:h_end, w_start:w_end] += w[f] * dout[i, f, h_i, w_i]</span><br><span class="line">                    dw[f] += xi_slice * dout[i, f, h_i, w_i]</span><br><span class="line">                    db[f] += dout[i, f, h_i, w_i]</span><br><span class="line">        dx[i, :, :, :] = dx_pad[i,:, pad:-pad, pad:-pad]</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure></p><h2 id="Max-Pooling-Naive-forward"><a href="#Max-Pooling-Naive-forward" class="headerlink" title="Max-Pooling: Naive forward"></a>Max-Pooling: Naive forward</h2><p>Max-Pooling就对Convolution层简化一下就可以了。</p><p><strong>Input data of shape：</strong> $(N, C, H_{prev}, W_{prev})$<br>其中$N$是样本数，$C$是channel数。</p><p>下一层的$H,W$：（没有考虑padding）</p><script type="math/tex; mode=display">H = \lfloor \frac{H_{prev} - HH }{stride} \rfloor +1</script><script type="math/tex; mode=display">W = \lfloor \frac{W_{prev} - WW }{stride} \rfloor +1</script><p><strong>Output data of shape：</strong> $(N, C, H, W)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_forward_naive</span><span class="params">(x, pool_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - pool_param: dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - 'pool_height': The height of each pooling region</span></span><br><span class="line"><span class="string">      - 'pool_width': The width of each pooling region</span></span><br><span class="line"><span class="string">      - 'stride': The distance between adjacent pooling regions</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    No padding is necessary here. Output size is given by </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H', W') where H' and W' are given by</span></span><br><span class="line"><span class="string">      H' = 1 + (H - pool_height) / stride</span></span><br><span class="line"><span class="string">      W' = 1 + (W - pool_width) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, pool_param)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the max-pooling forward pass                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H_prev, W_prev = x.shape</span><br><span class="line">    pool_height = pool_param[<span class="string">'pool_height'</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">'pool_width'</span>]</span><br><span class="line">    stride = pool_param[<span class="string">'stride'</span>]  <span class="comment"># stride</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># python3除法的结果都是float</span></span><br><span class="line">    H_out = <span class="number">1</span> + (H_prev - pool_height) // stride</span><br><span class="line">    W_out = <span class="number">1</span> + (W_prev - pool_width) // stride</span><br><span class="line">    Z = np.zeros((N, C, H_out, W_out))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):  <span class="comment"># loop over the batch of training examples</span></span><br><span class="line">        <span class="keyword">for</span> h_i <span class="keyword">in</span> range(H_out):  <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w_i <span class="keyword">in</span> range(W_out):  <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                h_start = h_i * stride</span><br><span class="line">                h_end = h_start + pool_height</span><br><span class="line">                w_start = w_i * stride</span><br><span class="line">                w_end = w_start + pool_width</span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(C):  <span class="comment"># loop over channels (= #filters) of the output volume</span></span><br><span class="line">                    Z[i, c, h_i, w_i] = np.max(x[i, c, h_start:h_end, w_start:w_end])</span><br><span class="line">    out = Z</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = (x, pool_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><h2 id="Max-Pooling-Naive-backward"><a href="#Max-Pooling-Naive-backward" class="headerlink" title="Max-Pooling: Naive backward"></a>Max-Pooling: Naive backward</h2><p>对Max-Pooling的求导类似于max函数，最大为1，否则为0.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mask = (x == np.max(x))</span><br><span class="line"><span class="comment">##########等价于########</span></span><br><span class="line">mask[i,j] = <span class="keyword">True</span> <span class="keyword">if</span> X[i,j] = x</span><br><span class="line">mask[i,j] = <span class="keyword">False</span> <span class="keyword">if</span> X[i,j] != x</span><br></pre></td></tr></table></figure></p><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_backward_naive</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, pool_param) as in the forward pass.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the max-pooling backward pass                           #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    x, pool_param = cache</span><br><span class="line">    pool_height = pool_param[<span class="string">'pool_height'</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">'pool_width'</span>]</span><br><span class="line">    stride = pool_param[<span class="string">'stride'</span>]  <span class="comment"># stride</span></span><br><span class="line"></span><br><span class="line">    N, C, H_out, W_out = dout.shape</span><br><span class="line"></span><br><span class="line">    dx = np.zeros(x.shape)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):  <span class="comment"># loop over the batch of training examples</span></span><br><span class="line">        <span class="keyword">for</span> h_i <span class="keyword">in</span> range(H_out):  <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w_i <span class="keyword">in</span> range(W_out):  <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                h_start = h_i * stride</span><br><span class="line">                h_end = h_start + pool_height</span><br><span class="line">                w_start = w_i * stride</span><br><span class="line">                w_end = w_start + pool_width</span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(C):  <span class="comment"># loop over channels (= #filters) of the output volume</span></span><br><span class="line">                    x_slice = x[i, c, h_start:h_end, w_start:w_end]</span><br><span class="line">                    mask = (x_slice == np.max(x_slice))</span><br><span class="line">                    dx[i, c, h_start:h_end, w_start:w_end] += mask * dout[i, c, h_i, w_i]</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure></p><h2 id="Spatial-batch-normalization-forward"><a href="#Spatial-batch-normalization-forward" class="headerlink" title="Spatial batch normalization: forward"></a>Spatial batch normalization: forward</h2><p>由于维度的差别，卷积网络的Batch Normalization和全连接网络略有不同，卷积层的输入是$(N,C,H,W)$，BN是对每个batch的每个channel进行Normalization。及将每个channel看成全连接的一个属性:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_reshaped = x.transpose(0, 2, 3, 1).reshape(N * H * W, C)</span><br></pre></td></tr></table></figure></p><p><code>x_reshaped</code>就是新的$(N’,D’)$的输入。<br>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for spatial batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    - beta: Shift parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance. momentum=0 means that</span></span><br><span class="line"><span class="string">        old information is discarded completely at every time step, while</span></span><br><span class="line"><span class="string">        momentum=1 means that new information is never incorporated. The</span></span><br><span class="line"><span class="string">        default of momentum=0.9 should work well in most situations.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out, cache = <span class="keyword">None</span>, []</span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for spatial batch normalization.       #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># HINT: You can implement spatial batch normalization by calling the      #</span></span><br><span class="line">    <span class="comment"># vanilla version of batch normalization you implemented above.           #</span></span><br><span class="line">    <span class="comment"># Your implementation should be very short; ours is less than five lines. #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    x_reshaped = x.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(N * H * W, C)</span><br><span class="line">    out_tmp, cache = batchnorm_forward(x_reshaped, gamma, beta, bn_param)</span><br><span class="line">    out = out_tmp.reshape(N, H, W, C).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure></p><h2 id="Spatial-batch-normalization-backward"><a href="#Spatial-batch-normalization-backward" class="headerlink" title="Spatial batch normalization: backward"></a>Spatial batch normalization: backward</h2><p>一样的思想应用于反向传播。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the backward pass for spatial batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for spatial batch normalization.      #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># HINT: You can implement spatial batch normalization by calling the      #</span></span><br><span class="line">    <span class="comment"># vanilla version of batch normalization you implemented above.           #</span></span><br><span class="line">    <span class="comment"># Your implementation should be very short; ours is less than five lines. #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = dout.shape</span><br><span class="line">    dout_reshaped = dout.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(N * H * W, C)</span><br><span class="line">    dx_reshaped, dgamma, dbeta = batchnorm_backward(dout_reshaped, cache)</span><br><span class="line">    dx = dx_reshaped.reshape(N, H, W, C).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure></p><h2 id="Spatial-group-normalization-forward"><a href="#Spatial-group-normalization-forward" class="headerlink" title="Spatial group normalization: forward"></a>Spatial group normalization: forward</h2><p>Group Normalization是18年才提出来的一个方法，算是Layer Normalization的一种变形把，不同点是，将C个channels分成G个Group，对每个Group分别Normalizaiton。</p><p><img src="http://static.zybuluo.com/BYWMM/rpkjafixzz9tl2kwcpxiinbb/12522150-02baf749c0d7a33b.png" width="600"></p><p>paper给的效果图，CNN中也一般，可能batch小一点的时候可以选用一下。</p><p><img src="http://static.zybuluo.com/BYWMM/r2c9dldwzff2qmpa7ois3m0t/12522150-771831a611f4cf37.jpg" width="400"></p><p>实现上，首先考类似上面那个BN的卷积版，每次要把一个batch$(N,C,H,W)$的哪些数据进行Normalization？</p><p>对于group normalization来说，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num = C // G</span><br><span class="line">x_group = x.reshape(N*G, num*H*W)</span><br></pre></td></tr></table></figure></p><p>就可以类似BN（或者LN）的应用了。<br>另外要注意，gamma和beta是，所以当涉及它们的运算的时候还要变成$(N, C,H,W)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_groupnorm_forward</span><span class="params">(x, gamma, beta, G, gn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for spatial group normalization.</span></span><br><span class="line"><span class="string">    In contrast to layer normalization, group normalization splits each entry </span></span><br><span class="line"><span class="string">    in the data into G contiguous pieces, which it then normalizes independently.</span></span><br><span class="line"><span class="string">    Per feature shifting and scaling are then applied to the data, in a manner identical to that of batch normalization and layer normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    - beta: Shift parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    - G: Integer number of groups to split into, should be a divisor of C</span></span><br><span class="line"><span class="string">    - gn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    eps = gn_param.get(<span class="string">'eps'</span>,<span class="number">1e-5</span>)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for spatial group normalization.       #</span></span><br><span class="line">    <span class="comment"># This will be extremely similar to the layer norm implementation.        #</span></span><br><span class="line">    <span class="comment"># In particular, think about how you could transform the matrix so that   #</span></span><br><span class="line">    <span class="comment"># the bulk of the code is similar to both train-time batch normalization  #</span></span><br><span class="line">    <span class="comment"># and layer normalization!                                                # </span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    num = C // G</span><br><span class="line">    x_group = x.reshape(N*G, num*H*W)</span><br><span class="line">    <span class="comment"># adapt gamma/beta to the function</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Same as LayerNorm Forward Pass</span></span><br><span class="line">    x_T = x_group.T</span><br><span class="line">    x_mean_T = np.mean(x_T, axis=<span class="number">0</span>)</span><br><span class="line">    x_var_T = np.var(x_T, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    x_group_hat_T = (x_T - x_mean_T) / np.sqrt(x_var_T + eps)</span><br><span class="line">    x_group_hat = x_group_hat_T.T <span class="comment"># shape of (N*G, num*H*W)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># shape of (N, C, H, W) to cal the gamma and beta</span></span><br><span class="line">    x_hat = x_group_hat.reshape(N, C, H, W)</span><br><span class="line"></span><br><span class="line">    out = gamma * x_hat + beta</span><br><span class="line">    cache = (G, x_T, x_hat, x_mean_T, x_var_T, gamma, beta, eps)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><h2 id="Spatial-group-normalization-backward"><a href="#Spatial-group-normalization-backward" class="headerlink" title="Spatial group normalization: backward"></a>Spatial group normalization: backward</h2><p>反向传播稍微有点难？？调的时候忽略了把LN代码中的<code>N</code>改为<code>tmp_N = num * H * W</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spatial_groupnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the backward pass for spatial group normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for spatial group normalization.      #</span></span><br><span class="line">    <span class="comment"># This will be extremely similar to the layer norm implementation.        #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = dout.shape</span><br><span class="line">    G, x, x_hat, x_mean, x_var, gamma, beta, eps = cache</span><br><span class="line">    num = C // G</span><br><span class="line">    <span class="comment"># dx_hat (N, C, H, W)</span></span><br><span class="line">    dx_hat = dout * gamma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculation with the shape of(N*G, num*H*W)</span></span><br><span class="line">    dx_hat = dx_hat.reshape(N*G, num*H*W)</span><br><span class="line">    dx_hat_T = dx_hat.T</span><br><span class="line">    tmp_N = num * H * W</span><br><span class="line"></span><br><span class="line">    dx_var = <span class="number">-0.5</span> * np.sum(dx_hat_T * (x - x_mean) * np.power(x_var + eps, <span class="number">-3</span> / <span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">    dx_mean = np.sum(dx_hat_T * (<span class="number">-1</span> / np.sqrt(x_var + eps)), axis=<span class="number">0</span>) + np.sum(<span class="number">-2</span> * dx_var * (x - x_mean), axis=<span class="number">0</span>) / tmp_N</span><br><span class="line"></span><br><span class="line">    dx_group_T = dx_hat_T / np.sqrt(x_var + eps) + dx_var * <span class="number">2</span> * (x - x_mean) / tmp_N + dx_mean / tmp_N</span><br><span class="line">    dx = dx_group_T.T.reshape(N, C, H, W)</span><br><span class="line"></span><br><span class="line">    dgamma = np.sum(dout * x_hat, axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>)).reshape(<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    dbeta = np.sum(dout, axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>)).reshape(<span class="number">1</span>, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      实现几个层：卷积层、最大池化、BN、GN(new)
    
    </summary>
    
      <category term="cs231n" scheme="https://bywmm.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="https://bywmm.github.io/tags/cs231n/"/>
    
      <category term="计算机视觉" scheme="https://bywmm.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="https://bywmm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>cs231n assignment2(Dropout)</title>
    <link href="https://bywmm.github.io/2018/12/03/cs231n_assignment2(Dropout)/"/>
    <id>https://bywmm.github.io/2018/12/03/cs231n_assignment2(Dropout)/</id>
    <published>2018-12-03T14:49:37.000Z</published>
    <updated>2019-10-24T03:42:53.107Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment2-Dropout"><a href="#cs231n-assignment2-Dropout" class="headerlink" title="cs231n assignment2(Dropout)"></a>cs231n assignment2(Dropout)</h1><h2 id="随机失活（Dropout）笔记"><a href="#随机失活（Dropout）笔记" class="headerlink" title="随机失活（Dropout）笔记"></a>随机失活（Dropout）笔记</h2><p>Dropout是一个简单又极其有效的正则化方法。该方法由Srivastava在论文<a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0。</p><p><img src="http://static.zybuluo.com/BYWMM/pqk98tsytplmjt25pd22yh53/xixijifojahjgas.png"></p><p>在训练过程中，<strong>随机失活可以被认为是对完整的神经网络抽样出一些子集</strong>，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为是<strong>对数量巨大的子网络们做了模型集成（model ensemble）</strong>，以此来计算出一个平均的预测。</p><p>一个3层神经网络的普通版随机失活可以用下面代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 普通版随机失活: 不推荐实现 (看下面笔记) """</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="string">""" X中是输入数据 """</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  U1 = np.random.rand(*H1.shape) &lt; p <span class="comment"># 第一个随机失活遮罩</span></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  U2 = np.random.rand(*H2.shape) &lt; p <span class="comment"># 第二个随机失活遮罩</span></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure><p>注意：<strong>在predict函数中不进行随机失活，但是对于两个隐层的输出都要乘以p，调整其数值范围</strong>。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以p=0.5为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。</p><p>上述操作不好的性质是必须在测试时对激活数据要按照p进行数值范围调整。既然测试性能如此关键，实际更倾向使用<strong>反向随机失活（inverted dropout）</strong>，它是<strong>在训练时就进行数值范围调整</strong>，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" </span></span><br><span class="line"><span class="string">反向随机失活: 推荐实现方式.</span></span><br><span class="line"><span class="string">在训练的时候drop和调整数值范围，测试时不做任何事.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 第一个随机失活遮罩. 注意/p!</span></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># 第二个随机失活遮罩. 注意/p!</span></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># 不用数值范围调整了</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  out = np.dot(W3, H2) + b3**</span><br></pre></td></tr></table></figure></p><h2 id="Add-Random-Noise"><a href="#Add-Random-Noise" class="headerlink" title="Add Random Noise"></a>Add Random Noise</h2><p>Add Random Noise是一种常见的正则化思想。<br>在训练中，给网络添加一些随机性，在一定程度上扰乱它过拟合训练数据；<br>在测试中，要抵消掉所有的随机性，希望能够提高我们的泛化能力。<br>Dropout可能是最常见的使用这种策略的例子。</p><ul><li><strong>Dropout：</strong>随机失活（超参数p控制随机失活的力度）</li><li><strong>Batch Normalization：</strong>训练中，一个样本可能与其他出现在不同的batch中，对于单个样本来说，如何正则化，具有一定随机性。（这种随机性不可控制力度）</li><li><strong>Data Augmentation：</strong>在训练中，以某种方式随机的转换图像，使得标签可以不变，用转换过的图像进行训练。</li></ul><p>水平翻转：<br>训练：反转后的图像<br>测试：原图像（？）<br><img src="http://static.zybuluo.com/BYWMM/p309pf5aboo6h369wrex88gm/maomixxiixhaighsdi.png" width="500"></p><p>裁剪图像：<br>训练：从图像中随机抽取不同尺寸大小的裁剪图象。<br>测试：固定的裁剪图像<br><img src="http://static.zybuluo.com/BYWMM/bsw70sszkw5kk6ldxsc0nokl/09e596hgdfkgkaef.png" width="200"></p><p>色彩抖动（Color Jitter）：<br>训练：随机改变图片的对比度和亮度或者其他更加复杂的色彩抖动的操作</p><blockquote><p><strong>More Complex:</strong></p><ol><li>Apply PCA to all [R,G,B] pixels in training set</li><li>Sample a “color offset” along priciple component directions</li><li>Add offset to all pixels of a training image</li></ol></blockquote><p><img src="http://static.zybuluo.com/BYWMM/t70dbnp7gbzk0d1uhckb6pal/foaijgawergfaj.png" width="500"></p><ul><li><strong>DropConnect：</strong>随机将权重矩阵的一些值置零<br><img src="http://static.zybuluo.com/BYWMM/pcrq7lm1zl4gtypvzptsh68d/dropconnect.png" width="500"></li><li><strong>Fractional Max Pooling：</strong>训练时随机池化filters的大小，下图是可能的三种池化结果；测试时有很多方法抵消随机性。<br><img src="http://static.zybuluo.com/BYWMM/r86vowzynib7rjfcbrhp9vrx/bufenzuidachihua.png" width="600"></li><li><strong>Stochastic Depth：</strong>从网络中随机丢弃部分层；测试时用全部网络<br><img src="http://static.zybuluo.com/BYWMM/52yfomuok05otlf60s9byk57/aifjadsighja.png" width="200"></li></ul><h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><p><strong>roward pass</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">    mask = (np.random.rand(*x.shape) &lt; p) / p</span><br><span class="line">    out = x * mask  <span class="comment"># drop!</span></span><br><span class="line"><span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">    out = x</span><br><span class="line">    </span><br><span class="line">cache = (dropout_param, mask)</span><br><span class="line">out = out.astype(x.dtype, copy=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p><p><strong>backward pass</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dropout_param, mask = cache</span><br><span class="line">mode = dropout_param[<span class="string">'mode'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">    dx = dout * mask</span><br><span class="line"><span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">    dx = dout</span><br></pre></td></tr></table></figure></p><p><strong>Fully-connected nets with Dropout</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># froward pass</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line">    <span class="comment"># cal Z</span></span><br><span class="line">    <span class="comment"># cal A</span></span><br><span class="line">    A, params[<span class="string">'cache3'</span>+str(i)] = relu_forward(Z)</span><br><span class="line">    <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">        A, params[<span class="string">'cache4'</span>+str(i)] = dropout_forward(A, self.dropout_param)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># backward pass</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">    <span class="comment"># dropout</span></span><br><span class="line">    <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">        dA = dropout_backward(dA, params[<span class="string">'cache4'</span>+str(i)])</span><br><span class="line">    <span class="comment"># dA to dZ</span></span><br><span class="line">    dZ = relu_backward(dA, params[<span class="string">'cache3'</span>+str(i)])</span><br><span class="line">    <span class="comment"># batchnorm</span></span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure><p><strong>训练结果</strong></p><p><img src="http://static.zybuluo.com/BYWMM/1fum4aovro9q925z1ns1rig4/flkjasghfewac.png" width="700"></p><p>dropout（p=0.25）的网络，虽然训练收敛会变慢，但是在验证集中可以得出好的结果。</p>]]></content>
    
    <summary type="html">
    
      Dropout；Add Random Noise；
    
    </summary>
    
      <category term="cs231n" scheme="https://bywmm.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="https://bywmm.github.io/tags/cs231n/"/>
    
      <category term="计算机视觉" scheme="https://bywmm.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="https://bywmm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>cs231n assignment2(BatchNormalization)</title>
    <link href="https://bywmm.github.io/2018/12/02/cs231n_assignment2(BatchNormalization)/"/>
    <id>https://bywmm.github.io/2018/12/02/cs231n_assignment2(BatchNormalization)/</id>
    <published>2018-12-02T12:28:13.000Z</published>
    <updated>2019-10-24T03:42:37.083Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment2-BatchNormalization"><a href="#cs231n-assignment2-BatchNormalization" class="headerlink" title="cs231n assignment2(BatchNormalization)"></a>cs231n assignment2(BatchNormalization)</h1><p>参考论文：<a href="https://arxiv.org/pdf/1502.03167v3.pdf" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p><p>论文笔记：<a href="https://bywmm.github.io/2018/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ABatch%20Normalization/">Batch Normalization</a></p><p>在机器学习中，当机器学习方法的输入数据包含零均值和单位方差的不相关特征时，机器学习方法往往更有效。<br>Batch Normalization即将这个想法，应用到了每层网络的输入中。</p><h2 id="BatchNorMalization实现"><a href="#BatchNorMalization实现" class="headerlink" title="BatchNorMalization实现"></a>BatchNorMalization实现</h2><h3 id="BN-forward"><a href="#BN-forward" class="headerlink" title="BN forward"></a>BN forward</h3><p>归一化之后添加了两个scale and shift参数，使其压缩范围可以灵活变化。<br>公式如下，<br><img src="http://static.zybuluo.com/BYWMM/lch9k8tgu9qrx5pymm44inz5/fkklaglk.png" width="500"></p><p><strong>注意：</strong>因为训练中计算均值和方差的时候，用的是batch来算的；测试时的mean和var使用训练时的<strong>指数加权平均数</strong>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">    <span class="comment"># cal the x_hat and y</span></span><br><span class="line">    x_mean = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">    x_var = np.var(x, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    x_hat = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">    out = gamma * x_hat + beta</span><br><span class="line">    <span class="comment"># cal the running_paras</span></span><br><span class="line">    running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * x_mean</span><br><span class="line">    running_var = momentum * running_var + (<span class="number">1</span> - momentum) * x_var</span><br><span class="line">    cache = (x, x_hat, x_mean, x_var, gamma, beta, eps)</span><br><span class="line"><span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">    x_hat = (x - running_mean) / np.sqrt(running_var + eps)</span><br><span class="line">    out = gamma * x_hat + beta</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br></pre></td></tr></table></figure></p><h3 id="BN-backward"><a href="#BN-backward" class="headerlink" title="BN backward"></a>BN backward</h3><p>参考博文：<a href="https://blog.csdn.net/pjia_1008/article/details/75073097" target="_blank" rel="noopener">传送门</a><br>对于每个batch，采用计算图求导的方法，进行求导。</p><p>计算图如下：</p><p><img src="http://static.zybuluo.com/BYWMM/zipyeel2c8xqkebme9sks9ce/20170713154211034.png"></p><p><strong>forward pass</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">N, D = x.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1: calculate mean</span></span><br><span class="line">mu = <span class="number">1.</span>/N * np.sum(x, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step2: subtract mean vector of every trainings example</span></span><br><span class="line">xmu = x - mu</span><br><span class="line"></span><br><span class="line"><span class="comment"># step3: following the lower branch - calculation denominator</span></span><br><span class="line">sq = xmu ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># step4: calculate variance</span></span><br><span class="line">var = <span class="number">1.</span>/N * np.sum(sq, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step5: add eps for numerical stability, then sqrt</span></span><br><span class="line">sqrtvar = np.sqrt(var + eps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step6: invert sqrtvar</span></span><br><span class="line">ivar = <span class="number">1.</span>/sqrtvar</span><br><span class="line"></span><br><span class="line"><span class="comment"># step7: execute normalization</span></span><br><span class="line">xhat = xmu * ivar</span><br><span class="line"></span><br><span class="line"><span class="comment"># step8: Nor the two transformation steps</span></span><br><span class="line">gammax = gamma * xhat</span><br><span class="line"></span><br><span class="line"><span class="comment"># step9</span></span><br><span class="line">out = gammax + beta</span><br><span class="line"></span><br><span class="line"><span class="comment"># store intermediate</span></span><br><span class="line">cache = (xhat, gamma, xmu, ivar, sqrtvar, var, eps)</span><br></pre></td></tr></table></figure></p><p><strong>backward pass</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># unfold the variables stored in cache</span></span><br><span class="line">xhat, gamma, xmu, ivar, sqrtvar, var, eps = cache</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the dimensions of the input/output</span></span><br><span class="line">N, D = dout.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># step9</span></span><br><span class="line">dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">dgammax = dout <span class="comment"># not necessary, but more understandable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># step8</span></span><br><span class="line">dgamma = np.sum(dgammax * xhat, axis = <span class="number">0</span>)</span><br><span class="line">dxhat = dgammax * gamma</span><br><span class="line"></span><br><span class="line"><span class="comment"># step7</span></span><br><span class="line">divar = np.sum(dxhat * xmu, axis=<span class="number">0</span>)</span><br><span class="line">dxmu1 = dxhat * ivar</span><br><span class="line"></span><br><span class="line"><span class="comment"># step6</span></span><br><span class="line">dsqrtvar = <span class="number">-1.</span>/(sqrtvar**<span class="number">2</span>) * divar</span><br><span class="line"></span><br><span class="line"><span class="comment"># step5</span></span><br><span class="line">dvar = <span class="number">0.5</span> * <span class="number">1.</span>/np.sqrt(var+eps) * dsqrtvar</span><br><span class="line"></span><br><span class="line"><span class="comment"># step4</span></span><br><span class="line">dsq = <span class="number">1.</span>/N * np.ones((N, D)) * dvar</span><br><span class="line"></span><br><span class="line"><span class="comment"># step3</span></span><br><span class="line">dxmu2 = <span class="number">2</span> * xmu * dsq</span><br><span class="line"></span><br><span class="line"><span class="comment"># step2</span></span><br><span class="line">dx1 = (dxmu1 + dxmu2)</span><br><span class="line">dmu = <span class="number">-1</span> * np.sum(dxmu1 + dxmu2, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1</span></span><br><span class="line">dx2 = <span class="number">1.</span>/N * np.ones((N, D)) * dmu</span><br><span class="line"></span><br><span class="line"><span class="comment"># step0</span></span><br><span class="line">dx = dx1 + dx2</span><br></pre></td></tr></table></figure></p><h3 id="BN-alternative-backward"><a href="#BN-alternative-backward" class="headerlink" title="BN alternative backward"></a>BN alternative backward</h3><p>将上文的梯度进行总和就得出了，链式求导的最终公式，论文也有给出。</p><p><img src="http://static.zybuluo.com/BYWMM/smfpv59hpra5ddhwtsyqw2s5/bpdjsflja.png" width="500"></p><p>关键代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x, x_hat, x_mean, x_var, gamma, beta, eps = cache</span><br><span class="line">N, D = x.shape</span><br><span class="line"></span><br><span class="line">dx_hat = dout * gamma</span><br><span class="line">dx_var = <span class="number">-0.5</span> * np.sum(dx_hat * (x - x_mean) * np.power(x_var + eps, <span class="number">-3</span> / <span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">dx_mean = np.sum(dx_hat * (<span class="number">-1.</span> / np.sqrt(x_var + eps)), axis=<span class="number">0</span>) + np.sum(<span class="number">-2</span> * dx_var * (x - x_mean), axis=<span class="number">0</span>) / N</span><br><span class="line"></span><br><span class="line">dx = dx_hat / np.sqrt(x_var + eps) + dx_var * <span class="number">2</span> * (x - x_mean) / N + dx_mean / N</span><br><span class="line">dgamma = np.sum(dout * x_hat, axis=<span class="number">0</span>)</span><br><span class="line">dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><h3 id="Fully-Connected-Nets-with-BN"><a href="#Fully-Connected-Nets-with-BN" class="headerlink" title="Fully Connected Nets with BN"></a>Fully Connected Nets with BN</h3><p>实现了BN模块，接着就在上节实验中的Fully Connected Nets类中加入BN层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __init__ function</span></span><br><span class="line"><span class="keyword">if</span> self.normalization==<span class="string">'batchnorm'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        self.params[<span class="string">'gamma'</span>+str(i)] = np.ones(hidden_dims[i<span class="number">-1</span>])</span><br><span class="line">        self.params[<span class="string">'beta'</span>+str(i)] = np.zeros(hidden_dims[i<span class="number">-1</span>])</span><br><span class="line">        </span><br><span class="line"><span class="comment"># loss function</span></span><br><span class="line">A = X</span><br><span class="line">L = self.num_layers</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">    Z, params[<span class="string">'cache1'</span>+str(i)] = affine_forward(A, self.params[<span class="string">'W'</span>+str(i)], self.params[<span class="string">'b'</span>+str(i)])</span><br><span class="line">    <span class="keyword">if</span> self.normalization==<span class="string">'batchnorm'</span>:</span><br><span class="line">        Z, params[<span class="string">'cache2'</span>+str(i)] = batchnorm_forward(Z, self.params[<span class="string">'gamma'</span>+str(i)], self.params[<span class="string">'beta'</span>+str(i)], self.bn_params[i<span class="number">-1</span>])</span><br><span class="line">    A, params[<span class="string">'cache3'</span>+str(i)] = relu_forward(Z)</span><br><span class="line">    scores, cache = affine_forward(A, self.params[<span class="string">'W'</span>+str(L)], self.params[<span class="string">'b'</span>+str(L)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># If test mode return early</span></span><br><span class="line"><span class="keyword">if</span> mode == <span class="string">'test'</span>:</span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">loss, dscores = softmax_loss(scores, y)</span><br><span class="line">sum_W_norm = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">    sum_W_norm += np.sum(self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)]**<span class="number">2</span>)</span><br><span class="line">loss += <span class="number">0.5</span> * self.reg * sum_W_norm</span><br><span class="line"></span><br><span class="line">dA, grads[<span class="string">'W'</span>+str(L)], grads[<span class="string">'b'</span>+str(L)] = affine_backward(dscores, cache)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">    dZ = relu_backward(dA, params[<span class="string">'cache3'</span>+str(i)])</span><br><span class="line">    <span class="keyword">if</span> self.normalization == <span class="string">'batchnorm'</span>:</span><br><span class="line">        dZ, grads[<span class="string">'gamma'</span>+str(i)], grads[<span class="string">'beta'</span>+str(i)] =  batchnorm_backward_alt(dZ, params[<span class="string">'cache2'</span>+str(i)])</span><br><span class="line">    dA, grads[<span class="string">'W'</span>+str(i)], grads[<span class="string">'b'</span>+str(i)] = affine_backward(dZ, params[<span class="string">'cache1'</span> + str(i)])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">    grads[<span class="string">'W'</span>+str(i+<span class="number">1</span>)] += self.reg * self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)]</span><br></pre></td></tr></table></figure><h2 id="Batch-Normalization的实验结果"><a href="#Batch-Normalization的实验结果" class="headerlink" title="Batch Normalization的实验结果"></a>Batch Normalization的实验结果</h2><p>激活函数：ReLU<br>优化方法：Adam</p><h3 id="with-or-without-batch-normalization"><a href="#with-or-without-batch-normalization" class="headerlink" title="with or without batch normalization"></a>with or without batch normalization</h3><p>在5个隐层，每个隐层100个神经元的网络中，用1000个训练数据进行测试。分别用或者不用batch normalization<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">weight_scale = <span class="number">2e-2</span></span><br><span class="line">bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=<span class="string">'batchnorm'</span>)</span><br><span class="line">model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">bn_solver = Solver(bn_model, small_data,</span><br><span class="line">                num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                verbose=<span class="keyword">True</span>,print_every=<span class="number">20</span>)</span><br><span class="line">bn_solver.train()</span><br><span class="line"></span><br><span class="line">solver = Solver(model, small_data,</span><br><span class="line">                num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                verbose=<span class="keyword">True</span>, print_every=<span class="number">20</span>)</span><br><span class="line">solver.train()</span><br></pre></td></tr></table></figure></p><p><img src="http://static.zybuluo.com/BYWMM/7c45tnc0j926k6rldq5xc0jq/wjfwudikuailedkgjlasd%20%281%29.png"><br><img src="http://static.zybuluo.com/BYWMM/5c1ojb9lixl2s3j393h9n427/wjfwudikuailedkgjlasd%20%282%29.png"><br><img src="http://static.zybuluo.com/BYWMM/q98z1t2vudmdd4s7dbrz0fq9/wjfwudikuailedkgjlasd%20%283%29.png"></p><p>可以看到Adam优化，同样的学习率下，用batch normalization的网络loss下降更为快速，致使训练集拟合的更好。（两个泛化都很低）</p><h3 id="different-weight-initialization-scale"><a href="#different-weight-initialization-scale" class="headerlink" title="different weight initialization scale"></a>different weight initialization scale</h3><p>采用20种不同的weight initialization scale，查看with or without normalization的表现。</p><p><img src="http://static.zybuluo.com/BYWMM/88md43o6jw0uqhvoixify9rc/zzzzzzzgkjkjjiri34t%20%281%29.png"><br><img src="http://static.zybuluo.com/BYWMM/d5dr5uq845tw84i7biq7c3df/zzzzzzzgkjkjjiri34t%20%282%29.png"><br><img src="http://static.zybuluo.com/BYWMM/zk93bh0b6hqghhiv1vsvlcl2/zzzzzzzgkjkjjiri34t%20%283%29.png"></p><p>可以看出，在用较小的weight scale时，with norm的表现要更好。</p><h3 id="different-batch-size"><a href="#different-batch-size" class="headerlink" title="different batch size"></a>different batch size</h3><p>因为Batch Normalization是对每个batch单独进行标准化，所以当batch size较小的时候，batch均值和方差不能很好的代表整体的均值和方差。<br>可以看到，当batch size到达一定值是，Batch Normalization才能显示出效果来。（同样，因为数据比较少，只观察拟合训练集的效果就好了）<br><img src="http://static.zybuluo.com/BYWMM/l3ysq6i1o1kqcsv048hkff3a/hfsauidhgfksfgaf.png"><br><img src="http://static.zybuluo.com/BYWMM/w2eb0t0fy3eu9i58qkmd7ba4/VI03%5D2ZPH%7DMRL1GS%2882L8AA.png"></p><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>参见论文：<a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">Layer Normalization</a></p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>Batch Normalization是对输入数据在batch上做操作。比如输入为（N,D）的数据，求均值是对N个样本的各个维度分别求均值。所以需要较大的batch size。<br>而Layer Normalization是对输入的数据的每一个样本（1，D），求均值方差等等，不依赖batch。</p><h3 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h3><p>因为其原理与BN只是标准化的维度不同，代码只需在BN的基础上稍作修改。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cal the x_hat and y</span></span><br><span class="line">x_T = x.T</span><br><span class="line">x_mean_T = np.mean(x_T, axis=<span class="number">0</span>)</span><br><span class="line">x_var_T = np.var(x_T, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x_hat_T = (x_T - x_mean_T) / np.sqrt(x_var_T + eps)</span><br><span class="line">x_hat = x_hat_T.T</span><br><span class="line">out = gamma * x_hat + beta</span><br><span class="line"></span><br><span class="line">cache = (x_T, x_hat_T, x_mean_T, x_var_T, gamma, beta, eps)</span><br></pre></td></tr></table></figure></p><h3 id="Backward-Pass"><a href="#Backward-Pass" class="headerlink" title="Backward Pass"></a>Backward Pass</h3><p>同样，在BN的BP上稍作修改<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x, x_hat, x_mean, x_var, gamma, beta, eps = cache</span><br><span class="line">N, D = x.shape</span><br><span class="line"></span><br><span class="line">dx_hat = dout * gamma</span><br><span class="line">dx_hat_T = dx_hat.T</span><br><span class="line">dx_var = <span class="number">-0.5</span> * np.sum(dx_hat_T * (x - x_mean) * np.power(x_var + eps, <span class="number">-3</span> / <span class="number">2</span>), axis=<span class="number">0</span>)</span><br><span class="line">dx_mean = np.sum(dx_hat_T * (<span class="number">-1</span> / np.sqrt(x_var + eps)), axis=<span class="number">0</span>) + np.sum(<span class="number">-2</span> * dx_var * (x - x_mean), axis=<span class="number">0</span>) / N</span><br><span class="line"></span><br><span class="line">dx_T = dx_hat_T / np.sqrt(x_var + eps) + dx_var * <span class="number">2</span> * (x - x_mean) / N + dx_mean / N</span><br><span class="line">dx = dx_T.T</span><br><span class="line"></span><br><span class="line">dgamma = np.sum(dout * x_hat.T, axis=<span class="number">0</span>)</span><br><span class="line">dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>Layer Normalization因为不与batch size相关，可以看到其性能与batch size似乎关系不大。对比baseline有比较好的效果。</p><p><img src="http://static.zybuluo.com/BYWMM/i2768aeb3r7zaf7ccl9wosto/layernormklfdjgkasf.png"></p>]]></content>
    
    <summary type="html">
    
      Batch Normalization；Layer Normalization；
    
    </summary>
    
      <category term="cs231n" scheme="https://bywmm.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="https://bywmm.github.io/tags/cs231n/"/>
    
      <category term="计算机视觉" scheme="https://bywmm.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="https://bywmm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：Batch Normalization</title>
    <link href="https://bywmm.github.io/2018/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ABatch%20Normalization/"/>
    <id>https://bywmm.github.io/2018/11/30/论文笔记：Batch Normalization/</id>
    <published>2018-11-30T13:37:07.000Z</published>
    <updated>2019-10-24T03:50:18.351Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文笔记：Batch-Normalization"><a href="#论文笔记：Batch-Normalization" class="headerlink" title="论文笔记：Batch Normalization"></a>论文笔记：Batch Normalization</h1><p>参考论文：<br><a href="https://arxiv.org/pdf/1502.03167v3.pdf" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><strong>Internal Covariate Shift：</strong><br>在深度网络的训练期间，由于网络参数变化而引起的网络激活（网络内部结点）分布的变化。</p><p><strong>ICS导致的问题：</strong></p><p>深层网络训练时，由于模型参数在不断修改，所以各层的输入的概率分布在不断变化，所以须使用<strong>较小的学习率及较好的权重初值</strong>，导致训练很慢，同时也导致使用saturating nonlinearities 激活函数（如sigmoid，正负两边都会饱和）时训练很困难。</p><p><strong>解决办法是：Batch Normalization</strong></p><p><strong>BN是：</strong> 使正则化层称为模型结构的一部分，在训练每个mini_batch时，单独进行Normalization 。</p><p><strong>BN的好处：</strong> 让我们可以使用更大的学习率，初值可以更随意；它起到了正则项的作用，在某些情况下，有它就不需要使用Dropout了。</p><p>在Imagenet上， achieves the same accuracy with 14 times fewertraining steps</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>随机梯度下降（SGD）已被证明是训练深度网络的一种有效方式，而动量）和Adagrad等SGD的变体也取得了非常好的效果。下文以SGD为例进行讨论。</p><h3 id="SGD遇到的问题"><a href="#SGD遇到的问题" class="headerlink" title="SGD遇到的问题"></a>SGD遇到的问题</h3><p>SGD目标</p><script type="math/tex; mode=display">\theta=\underset{\theta}{\arg\min}\frac{1}{N}\sum_{i=1}^N\ell(x_i,\theta)</script><p>用mini_batch去近似整个训练集的梯度</p><script type="math/tex; mode=display">\frac{1}{m}\frac{\partial\ell(x_i,\theta)}{\partial\theta}</script><p>（在并行计算下，m个合成一个batch计算比单独计算m次快很多；且用m个样本去估计整个训练集的梯度方向比用单个样本要准确。所以用一个mini_batch下降一次要比用一个样本下降mini_batch次要好。）</p><p>虽然SGD简单高效，但它需要调整模型超参数，特别是优化中使用的学习速率以及模型参数的初始值。每个层的输入都受到前面所有层的参数的影响，因此随着网络变得更深，网络参数的微小变化就会放大。</p><p>这就带来一个问题：<strong>各层输入分布的变动导致模型需要不停地去拟合新的分布。</strong></p><h3 id="Covariate-Shift扩展到子网络"><a href="#Covariate-Shift扩展到子网络" class="headerlink" title="Covariate Shift扩展到子网络"></a>Covariate Shift扩展到子网络</h3><p>当学习系统的输入分布发生变化时，会发生<strong>协变量变化（Covariate Shift）</strong></p><blockquote><p>当一个学习系统的输入分布是变化的时（即训练集的样本分布和测试集的样本分布不一致），训练的模型就很难有较好的泛化能力，这叫做<strong>covariate shift</strong> (Shimodaira, 2000)，解决办法是domain adaptation (Jiang, 2008).和迁移学习。</p></blockquote><p>然而，协变变化的概念可以扩展到整个学习系统中，适用于其子部分，如子网络或层。考虑一个网络计算</p><p><strong>整个network损失函数为：</strong></p><script type="math/tex; mode=display">\ell=F_2(F_1(\mu,\theta_1), \theta_2)</script><p>学习$\theta_2$可以看作是输入$F_1(\mu,\theta_1)$被送入子网络<br><strong>sub-network的损失函数为：</strong></p><script type="math/tex; mode=display">\ell=F_2(x, \theta_2)</script><p>sub-network的SGD（$m$是batch_size，$\alpha$是学习率）：</p><script type="math/tex; mode=display">\theta_2\leftarrow\theta_2-\frac{\alpha}{m}\frac{\partial{F}_2(x_i,\theta_2)}{\partial\theta_2}</script><p>这与具有输入$x$的独立网络$F_2$完全等效。</p><p>因此，使训练更有效的输入分布特性，也适用于对子网络进行训练。<br><strong>将Covariate Shift扩展到子网络</strong>，即我们想要每个sub-network的输入分布都不要有太大的变化，这就引出了Batch Normalization的方法</p><p><strong>BN希望通过对每个子网络输入的归一化，来对子网络的输入分布进行一定程度的fixed。</strong></p><h2 id="Towards-Reducing-Internal-Covariate-Shift"><a href="#Towards-Reducing-Internal-Covariate-Shift" class="headerlink" title="Towards Reducing Internal Covariate Shift"></a>Towards Reducing Internal Covariate Shift</h2><p><strong>Internal Covariate Shift</strong> 算是将Covariate Shift扩展到每一个子网络的一个说法。希望每个子网络的输入都不要有太大的变化（在深度网络中，前一层一个参数的微小变化对于深层参数可能产生较大的影响）。</p><p>Batch Normalization就是为了降低Internal Covariate Shift。</p><h3 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h3><p>前面提到BN是想，在每次mini_batch的数据进行训练时，每一层的输入都进行归一化（子网络的思想就是：每个子网络的输入都进行处理）。</p><script type="math/tex; mode=display">\hat{x}^{(k)}=\frac{x^{(k)}-E(x^{(k)})}{\sqrt{Var(x^{(k)})}}</script><p>但是，简单地标准化每层的输入可能会改变该层可以表示的内容。例如，Sigmoid函数虽为一个非线性的函数，通过Normalization可能将输入压缩到其线性部分。</p><p><img src="http://static.zybuluo.com/BYWMM/5syb7ieqc57yqtl0md50sr03/saduhiasg.png" width="300"></p><p>我们希望，可以灵活地学习压缩范围，比如压缩到$[-5,5]$等某个更适合拟合训练集的某个区间。</p><p>所以针对每个层的激活引入了一对参数，使压缩区间可以移动和放缩：</p><script type="math/tex; mode=display">y^{(k)}=\gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}</script><p><strong>其中$\gamma$和$\beta$可在训练中学得。</strong></p><p>并且，当$\gamma^{(k)}=\sqrt{Var(x^())}$，$\beta^{(k)}=E(x^{(k)})$可还原出原先的分布。所以，这种拟合效果至少不会比原先的表示差！</p><h3 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h3><p><img src="http://static.zybuluo.com/BYWMM/lch9k8tgu9qrx5pymm44inz5/fkklaglk.png" width="500"></p><p>只要minibatch中的样本采样与同一分布，规范化后的输入 x 期望为0，方差为1，把规范后的 x 进行线性变换得到 y 作为后续层的输入，可以发现 后续层的输入具有固定的均值和方差的。尽管 规范化后的 x 的联合分布在训练过程中会改变(源于第一个简化，本文的规范化是把 x 向量中各个变量当作独立的，单独规范化的，所以他们的联合分布并不稳定，只是单独是稳定的)，但还是可以使训练加速。</p><h3 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h3><p>优化中也需要对BN的两个参数进行优化，链式法则求导就可以了：</p><p>具体推导可参见</p><p><img src="http://static.zybuluo.com/BYWMM/smfpv59hpra5ddhwtsyqw2s5/bpdjsflja.png" width="500"></p><p>BN是可微的，通过BN变换，可以减弱输入分布的 internal covariate shift ，并且学习到这个线性变换与网络本来的变换是等价的。</p><h2 id="其他几个问题总结"><a href="#其他几个问题总结" class="headerlink" title="其他几个问题总结"></a>其他几个问题总结</h2><h3 id="Batch-Normalization的好处"><a href="#Batch-Normalization的好处" class="headerlink" title="Batch Normalization的好处"></a>Batch Normalization的好处</h3><ul><li>保持各层输入的均值和方差稳定，来减弱 internal covariate shift；</li><li>也让梯度受参数及其初值的减小；</li><li>BN也算作正则项，减少对Dropout的依赖；</li><li>它让卡在饱和区域的概率降低，以便可以使用 saturating nonlinearities（如Sigmoid，tanh等激活函数）</li></ul><h3 id="BN层的位置"><a href="#BN层的位置" class="headerlink" title="BN层的位置"></a>BN层的位置</h3><p>原本按照BN的思想，当在每一层的激活函数之后。例如$ReLU=\max(Wx+b,0)$之后，对数据进行归一化。然而，文章中说这样做在训练初期，分界面还在剧烈变化时，计算出的参数不稳定，所以退而求其次，在$Wx+b$之后进行归一化。因为初始的$W$是从标准高斯分布中采样得到的，而$W$中元素的数量远大于$x$，$Wx+b$每维的均值本身就接近0、方差接近1，<strong>所以在$Wx+b$后使用Batch Normalization能得到更稳定的结果。</strong></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>实验和梯度推导可参见</p>]]></content>
    
    <summary type="html">
    
      Batch Normalization：Accelerating Deep Network Training by Reducing Internal Covariate Shift
    
    </summary>
    
      <category term="cs231n" scheme="https://bywmm.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="https://bywmm.github.io/tags/cs231n/"/>
    
      <category term="神经网络" scheme="https://bywmm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文笔记" scheme="https://bywmm.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>cs231n assignment2(FullyConnectedNets)</title>
    <link href="https://bywmm.github.io/2018/11/26/cs231n_assignment2(FullyConnectedNets)/"/>
    <id>https://bywmm.github.io/2018/11/26/cs231n_assignment2(FullyConnectedNets)/</id>
    <published>2018-11-26T11:06:37.000Z</published>
    <updated>2019-10-24T03:43:02.464Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment2-FullyConnectedNets"><a href="#cs231n-assignment2-FullyConnectedNets" class="headerlink" title="cs231n assignment2(FullyConnectedNets)"></a>cs231n assignment2(FullyConnectedNets)</h1><h2 id="层的模块化"><a href="#层的模块化" class="headerlink" title="层的模块化"></a>层的模块化</h2><p>在assignment1中的实验中，曾经实现了一个two-layers-net。用的方法，一个公式一个公式的写出来的，当网络规模变大，就不好实现，重用性也差。本节实验就将层进行模块化，每一层都会实现一个forward和backward的函数。</p><p>例如一个forward函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_forward</span><span class="params">(x, w)</span>:</span></span><br><span class="line">  <span class="string">""" Receive inputs x and weights w """</span></span><br><span class="line">  <span class="comment"># Do some computations ...</span></span><br><span class="line">  z = <span class="comment"># ... some intermediate value</span></span><br><span class="line">  <span class="comment"># Do some more computations ...</span></span><br><span class="line">  out = <span class="comment"># the output</span></span><br><span class="line"></span><br><span class="line">  cache = (x, w, z, out) <span class="comment"># Values we need to compute gradients</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure></p><p>例如一个backward函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Receive dout (derivative of loss with respect to outputs) and cache,</span></span><br><span class="line"><span class="string">  and compute derivative with respect to inputs.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Unpack cache values</span></span><br><span class="line">  x, w, z, out = cache</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Use values in cache to compute derivatives</span></span><br><span class="line">  dx = <span class="comment"># Derivative of loss with respect to x</span></span><br><span class="line">  dw = <span class="comment"># Derivative of loss with respect to w</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dx, dw</span><br></pre></td></tr></table></figure><p>实现完各种层的forward和backward函数之后，就可以把他们进行组装，来实现不同解构的网络了。</p><h3 id="Affine-layer-forward"><a href="#Affine-layer-forward" class="headerlink" title="Affine layer: forward"></a>Affine layer: forward</h3><p>Affine layer是全连接层，前向传播就是</p><script type="math/tex; mode=display">Z = X\cdot{W}+b</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_forward</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for an affine (fully-connected) layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N</span></span><br><span class="line"><span class="string">    examples, where each example x[i] has shape (d_1, ..., d_k). We will</span></span><br><span class="line"><span class="string">    reshape each input into a vector of dimension D = d_1 * ... * d_k, and</span></span><br><span class="line"><span class="string">    then transform it to an output vector of dimension M.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)</span></span><br><span class="line"><span class="string">    - w: A numpy array of weights, of shape (D, M)</span></span><br><span class="line"><span class="string">    - b: A numpy array of biases, of shape (M,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: output, of shape (N, M)</span></span><br><span class="line"><span class="string">    - cache: (x, w, b)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the affine forward pass. Store the result in out. You   #</span></span><br><span class="line">    <span class="comment"># will need to reshape the input into rows.                               #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N = x.shape[<span class="number">0</span>]</span><br><span class="line">    D = w.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    out = np.dot(x.reshape(N, D), w) + b</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = (x, w, b)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><h3 id="Affine-layer-backward"><a href="#Affine-layer-backward" class="headerlink" title="Affine layer: backward"></a>Affine layer: backward</h3><p>根据链式法则</p><script type="math/tex; mode=display">Z = X\cdot{W}+b</script><script type="math/tex; mode=display">\begin{align*}dX & = dout * \frac{\partial{Z}}{\partial{X}} = dout * W\\dW & = dout * \frac{\partial{Z}}{\partial{W}} = dout * X\\db & =dout * \frac{\partial{Z}}{\partial{b}} = dout * [1,...,1]\end{align*}</script><p>注意一下维度就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the backward pass for an affine layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivative, of shape (N, M)</span></span><br><span class="line"><span class="string">    - cache: Tuple of:</span></span><br><span class="line"><span class="string">      - x: Input data, of shape (N, d_1, ... d_k)</span></span><br><span class="line"><span class="string">      - w: Weights, of shape (D, M)</span></span><br><span class="line"><span class="string">      - b: Biases, of shape (M,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w, of shape (D, M)</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b, of shape (M,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x, w, b = cache</span><br><span class="line">    N = x.shape[<span class="number">0</span>]</span><br><span class="line">    D = w.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the affine backward pass.                               #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    dx = np.dot(dout, w.T).reshape(x.shape)</span><br><span class="line">    dw = np.dot(x.reshape(N, D).T, dout)</span><br><span class="line">    db = np.dot(np.ones((<span class="number">1</span>, N)), dout).flatten()</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure><h3 id="ReLU-layer-forward"><a href="#ReLU-layer-forward" class="headerlink" title="ReLU layer: forward"></a>ReLU layer: forward</h3><script type="math/tex; mode=display">A =ReLU(Z)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_forward</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for a layer of rectified linear units (ReLUs).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Inputs, of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output, of the same shape as x</span></span><br><span class="line"><span class="string">    - cache: x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the ReLU forward pass.                                  #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    out = np.maximum(<span class="number">0</span>, x)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = x</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><h3 id="ReLU-layer-backward"><a href="#ReLU-layer-backward" class="headerlink" title="ReLU layer: backward"></a>ReLU layer: backward</h3><p>根据链式求导</p><script type="math/tex; mode=display">A =\max(0,Z)</script><script type="math/tex; mode=display">dZ=dout\cdot\frac{\partial{A}}{\partial{Z}}=dout\cdot[Z>0]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the backward pass for a layer of rectified linear units (ReLUs).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of any shape</span></span><br><span class="line"><span class="string">    - cache: Input x, of same shape as dout</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, x = <span class="keyword">None</span>, cache</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the ReLU backward pass.                                 #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    dx = dout</span><br><span class="line">    dx[x &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><h3 id="Two-layer-network"><a href="#Two-layer-network" class="headerlink" title="Two-layer network"></a>Two-layer network</h3><p>有了之前实现的模块，实现一个两层网络就比较简单。贴一下关键代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># forward</span></span><br><span class="line">A1, cache1 = affine_relu_forward(X, W1, b1)</span><br><span class="line">scores, cache2 = affine_forward(A1, W2, b2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># If y is None then we are in test mode so just return scores</span></span><br><span class="line"><span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss</span></span><br><span class="line">loss, dscores = softmax_loss(scores, y)</span><br><span class="line">loss += <span class="number">0.5</span> * self.reg * (np.sum(W1 * W1) + np.sum(W2 * W2))</span><br><span class="line"><span class="comment"># backward</span></span><br><span class="line">dA1, dW2, db2 = affine_backward(dscores, cache2)</span><br><span class="line">dX, dW1, db1 = affine_relu_backward(dA1, cache1)</span><br></pre></td></tr></table></figure><h3 id="Multilayer-network"><a href="#Multilayer-network" class="headerlink" title="Multilayer network"></a>Multilayer network</h3><p>实现一个如下结构的多层网络</p><p>{affine - relu} x (L - 1) - affine - softmax</p><p>关键代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A0 = X</span></span><br><span class="line">A = X</span><br><span class="line">L = self.num_layers</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">    Z, params[<span class="string">'cache1'</span>+str(i)] = affine_forward(A, self.params[<span class="string">'W'</span>+str(i)], self.params[<span class="string">'b'</span>+str(i)])</span><br><span class="line">    A, params[<span class="string">'cache3'</span>+str(i)] = relu_forward(Z)</span><br><span class="line">scores, cache = affine_forward(A, self.params[<span class="string">'W'</span>+str(L)], self.params[<span class="string">'b'</span>+str(L)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># If test mode return early</span></span><br><span class="line"><span class="keyword">if</span> mode == <span class="string">'test'</span>:</span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line"><span class="comment"># cal the loss</span></span><br><span class="line">loss, dscores = softmax_loss(scores, y)</span><br><span class="line">sum_W_norm = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">    sum_W_norm += np.sum(self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)]**<span class="number">2</span>)</span><br><span class="line">loss += <span class="number">0.5</span> * self.reg * sum_W_norm</span><br><span class="line"><span class="comment"># backward</span></span><br><span class="line">dA, grads[<span class="string">'W'</span>+str(L)], grads[<span class="string">'b'</span>+str(L)] = affine_backward(dscores, cache)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">    dZ = relu_backward(dA, params[<span class="string">'cache3'</span>+str(i)])</span><br><span class="line">    dA, grads[<span class="string">'W'</span>+str(i)], grads[<span class="string">'b'</span>+str(i)] = affine_backward(dZ, params[<span class="string">'cache1'</span> + str(i)])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">    grads[<span class="string">'W'</span>+str(i+<span class="number">1</span>)] += self.reg * self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)]</span><br></pre></td></tr></table></figure></p><p>另外，初始化的时候注意，W是<strong>weight_scale*</strong>标准高斯分布的随机数。</p><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>公式及笔记参见：<a href="https://bywmm.github.io/2018/11/24/more%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">优化方法（more）</a></p><h3 id="SGD-Momentum"><a href="#SGD-Momentum" class="headerlink" title="SGD+Momentum"></a>SGD+Momentum</h3><p>关键代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v = config[<span class="string">'momentum'</span>] * v - config[<span class="string">'learning_rate'</span>] * dw</span><br><span class="line">next_w = w + v</span><br></pre></td></tr></table></figure></p><p>运行结果：<br>running with  sgd<br>(Epoch 5 / 5) train acc: 0.440000; val_acc: 0.322000</p><p>running with  sgd_momentum<br>(Epoch 5 / 5) train acc: 0.507000; val_acc: 0.384000</p><p><img src="http://static.zybuluo.com/BYWMM/as7iiq6omsyg5mszkrd3gft6/356adslfasdk%20%281%29.png"><br><img src="http://static.zybuluo.com/BYWMM/f781xwf5pmxg4pilhnp5bzxe/356adslfasdk%20%282%29.png"><br><img src="http://static.zybuluo.com/BYWMM/4nc7zjx7vbsghwuhvwl7v044/356adslfasdk%20%283%29.png"></p><h3 id="RMSProp-and-Adam"><a href="#RMSProp-and-Adam" class="headerlink" title="RMSProp and Adam"></a>RMSProp and Adam</h3><p><strong>RMSprop</strong>关键代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config[<span class="string">'cache'</span>] = config[<span class="string">'decay_rate'</span>] * config[<span class="string">'cache'</span>] +(<span class="number">1</span> - config[<span class="string">'decay_rate'</span>]) * dw**<span class="number">2</span></span><br><span class="line">next_w = w - config[<span class="string">'learning_rate'</span>] * dw / (np.sqrt(config[<span class="string">'cache'</span>]) + config[<span class="string">'epsilon'</span>])</span><br></pre></td></tr></table></figure></p><p><strong>Adam</strong>算是RMSprop和动量的结合。<br>这里采用了偏差修正，所以要乘一个$\frac{1}{1-\beta^t}$，详情可参见笔记。<br>关键代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">config[<span class="string">'t'</span>] += <span class="number">1</span></span><br><span class="line">config[<span class="string">'m'</span>] = config[<span class="string">'beta1'</span>] * config[<span class="string">'m'</span>] + (<span class="number">1</span> - config[<span class="string">'beta1'</span>]) * dw</span><br><span class="line">config[<span class="string">'v'</span>] = config[<span class="string">'beta2'</span>] * config[<span class="string">'v'</span>] + (<span class="number">1</span> - config[<span class="string">'beta2'</span>]) * (dw**<span class="number">2</span>)</span><br><span class="line">mb = config[<span class="string">'m'</span>] / (<span class="number">1</span> - config[<span class="string">'beta1'</span>] ** config[<span class="string">'t'</span>])</span><br><span class="line">vb = config[<span class="string">'v'</span>] / (<span class="number">1</span> - config[<span class="string">'beta2'</span>] ** config[<span class="string">'t'</span>])</span><br><span class="line">next_w = w - config[<span class="string">'learning_rate'</span>] * mb / (np.sqrt(vb) + config[<span class="string">'epsilon'</span>])</span><br></pre></td></tr></table></figure></p><p>对比结果：</p><p><img src="http://static.zybuluo.com/BYWMM/ap8jfanaoxhxkzcgvjk4nhl5/lingwai.png"><br><img src="http://static.zybuluo.com/BYWMM/uxkrp10efa9nglu09wn5946p/789127863fakjsdh%20%283%29.png"><br><img src="http://static.zybuluo.com/BYWMM/7mgqipql95f860q7ovn41j1c/789127863fakjsdh%20%281%29.png"></p>]]></content>
    
    <summary type="html">
    
      层的模块化；更多优化算法
    
    </summary>
    
      <category term="cs231n" scheme="https://bywmm.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="https://bywmm.github.io/tags/cs231n/"/>
    
      <category term="计算机视觉" scheme="https://bywmm.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="https://bywmm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>优化算法（more）</title>
    <link href="https://bywmm.github.io/2018/11/24/more%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    <id>https://bywmm.github.io/2018/11/24/more优化算法/</id>
    <published>2018-11-24T10:05:07.000Z</published>
    <updated>2019-10-24T03:45:01.501Z</updated>
    
    <content type="html"><![CDATA[<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="指数加权平均数"><a href="#指数加权平均数" class="headerlink" title="指数加权平均数"></a>指数加权平均数</h2><p>我想向你展示几个优化算法，它们比梯度下降法快，要理解这些算法，你需要用到<strong>指数加权平均（Exponentially weighted averages）</strong>，在统计中也叫做<strong>指数加权移动平均</strong></p><p>下面是某地一年中（时间-温度）的散点图。</p><p><img src="http://static.zybuluo.com/BYWMM/u5sb91yfkmnbz8dy64qtx5h8/image_1csvqpnf613o1ffh1iplibk1hhd9.png" width="350"></p><p>如果要计算趋势的话，也就是温度的<strong>局部平均值</strong>（或者说<strong>移动平均值</strong>）。</p><p>首先使$v_0=0$，每天需要使用$0.9$的加权数之前的数值加上当日温度$\theta_t$的0.1 倍，即</p><script type="math/tex; mode=display">v_t=0.9v_{t-1}+0.1\theta_t</script><p>用红线表示该平均值，便得到这样的结果。</p><p><img src="http://static.zybuluo.com/BYWMM/90ikz28h8iaz4vgyvz80dox1/eraawerewr.png" width="350"></p><p>我们把$0.9$这个常数变成$\beta$，将之前的$0.1$变成$(1-\beta)$，得到<strong>移动平均值</strong>的一般形式</p><script type="math/tex; mode=display">v_t=\beta{v}_{t-1}+(1-\beta)\theta_t</script><p><strong>可将$v_t$大概视为前$\frac{1}{1-\beta}$天的平均温度</strong></p><ul><li><p>例如$\beta=0.9$，$v_t$（上图红线部分）约为前10天的平均值。 </p></li><li><p>例如$\beta=0.98$，$v_t$（绿线部分）约为前50天的平均值。温度变化时，温度上下起伏小。</p></li></ul><p><img src="http://static.zybuluo.com/BYWMM/cocdsvvnpcg3qs3u3w0cl48y/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87.png" width="350"></p><ul><li>再例如$\beta=0.5$，$v_t$（黄线部分）约为前2天的平均值。温度变化时，温度上下起伏大。</li></ul><p><img src="http://static.zybuluo.com/BYWMM/abx24r8mymb6ur43wk2fu1z0/sjadkgkasghasf.png" width="350"></p><blockquote><p>为什么可视为前$\frac{1}{1-\beta}$天的平均？</p><p>下图为$\beta=0.9$是，前i天的温度在$v_t$中所占的比例</p><p><img src="http://static.zybuluo.com/BYWMM/b8ccntvoyqc4lpkf0ng2h4k1/gksa.png" width="300"></p><p>由于</p><script type="math/tex; mode=display">\lim_{x\mapsto0}(1-x)^{\frac{1}{x}}=\frac{1}{e}</script><p>这个例子中：10天后，$\theta_{t-i}$在$v_t$中所占的比例就不足$\theta_t$的$\frac{1}{e}$倍了，所以认为$v_t$约为近10天的平均温度。</p></blockquote><h2 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h2><script type="math/tex; mode=display">\frac{v_t}{1-\beta^t}</script><p>上文中，这个红色的线对应于$\beta=0.9$，绿色的线对应于$\beta=0.98$。</p><p>但如果执行的是这个公式的话，得到的$v_t$实际是紫色的线。</p><script type="math/tex; mode=display">v_t=0.9v_{t-1}+0.1\theta_t</script><p><img src="http://static.zybuluo.com/BYWMM/joirl7onn6xfukhous7yj3c5/6666666666fgsfasfsadg.png" width="350"></p><p>原因是我们初始化$v_0=0$，那么$v_1=0.98v_0+0.02\theta_1=0.02\theta_1$。所以对第一天的估计要小很多。</p><p><strong>偏差修正</strong>可以修改这一估测，让估测变得更准确，特别是在估测初期，也就是不用$v_t$，而是用$\frac{v_t}{1-\beta^t}$，$t$是当前的天数。</p><h2 id="动量梯度下降"><a href="#动量梯度下降" class="headerlink" title="动量梯度下降"></a>动量梯度下降</h2><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>普通的梯度下降<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通更新</span></span><br><span class="line">w += -learning_rate * dw</span><br></pre></td></tr></table></figure></p><p>动量更新则是<strong>计算梯度的指数加权平均数</strong>，并利用该梯度进行更新</p><script type="math/tex; mode=display">\begin{align*}& v_{dW}=\beta{v}_{dW}+(1-\beta)dW\\& W=W-\alpha*v_{dW}\end{align*}</script><p>想象在一个碗状的函数上进行优化，某个位置的梯度是该点的加速度，梯度影响速度，速度再影响位置。（在普通更新中，梯度直接影响位置。）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动量更新</span></span><br><span class="line">v = mu * v - learning_rate * dw <span class="comment"># 梯度影响速度</span></span><br><span class="line">w += v <span class="comment"># 速度影响位置</span></span><br></pre></td></tr></table></figure></p><blockquote><ul><li>在这里引入了一个初始化为0的变量<code>v</code>和一个超参数<code>mu</code>。<code>mu</code>在最优化的过程中被看做动量（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。</li><li>通过交叉验证，<code>mu</code>通常设为[0.5,0.9,0.95,0.99]中的一个。</li></ul></blockquote><p><strong>通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。</strong></p><p><img src="http://static.zybuluo.com/BYWMM/75lv5cjabd8sfc2zhsp6x11z/fksajghkljhkgasf.png" alt="fjagjalihg.png-215.3kB"></p><p>例如在上图中，普通SGD更新如图所示；而动量更新，方向上下变化的梯度在速度上变化不大，而持续向右的梯度方向，则可使速度持续向右增加，从而更快的下降。</p><p><img src="http://static.zybuluo.com/BYWMM/19i290dzgo3lmjznhnuviiw4/fasgjkahgfsd.png" alt="fasgjkahgfsd.png-177kB"></p><p>陷入<strong>极小点</strong>和<strong>鞍点</strong>是普通梯度下降法可能遇到的不好的情况。而动量下降，可以使在到达极小点和鞍点时，按惯性“冲出”这种不好的情况。(即使没有梯度，仍会有速度)</p><p><img src="http://static.zybuluo.com/BYWMM/yo8um5zo1pjsd4vlq5zfciem/%E6%9E%81%E5%B0%8F%E5%80%BC%E9%9E%8D%E7%82%B9.png" width="500"></p><h3 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h3><p>Nesterov动量的核心思路是，当参数向量位于某个位置$x$时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过<code>mu*v</code>稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置<code>x+mu*v</code>看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，计算x + mu * v的梯度而不是“旧”位置x的梯度就有意义了。</p><p><img src="http://static.zybuluo.com/BYWMM/djm6b90ny7miewa5s43ggdeo/412afb713ddcff0ba9165ab026563304_r.jpg" width="500"></p><p>如图，既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个<code>&quot;looked-ahead&quot;</code>的地方计算梯度。</p><script type="math/tex; mode=display">\begin{align*}& v_{t+1}=\beta{v}_t-\alpha\nabla{f}(w_t+\beta{v}_t)\\& w_{t+1}=w_t+v_{t+1}\end{align*}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_ahead = x + mu * v</span><br><span class="line"><span class="comment"># 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)</span></span><br><span class="line">v = mu * v - learning_rate * dx_ahead</span><br><span class="line">x += v</span><br></pre></td></tr></table></figure><p>令$\tilde{x}_t=x_t+\beta{v}_t$，就有另一个简单点的表示</p><script type="math/tex; mode=display">\begin{align*}& v_{t+1}=\beta{v}_t-\alpha\nabla{f}(\tilde{w}_t)\\& \tilde{w}_{t+1}=\tilde{w}_t-\beta{v}_t+(1+\beta)v_{t+1}\end{align*}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_prev = v <span class="comment"># 存储备份</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># 速度更新保持不变</span></span><br><span class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># 位置更新变了形式</span></span><br></pre></td></tr></table></figure><blockquote><p>来自cs231n：<br>在理论上Nesterov Momentum对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。</p><p>对于NAG（Nesterov’s Accelerated Momentum）的来源和数学公式推导，我们推荐以下的拓展阅读：</p><p>Yoshua Bengio的<a href="https://arxiv.org/pdf/1212.0901v2.pdf" target="_blank" rel="noopener">Advances in optimizing Recurrent Networks</a>，Section 3.5。<br><a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf" target="_blank" rel="noopener">Ilya Sutskever’s thesis</a> (pdf)在section 7.2对于这个主题有更详尽的阐述。</p></blockquote><h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>Adagrad是一个由Duchi等提出的适应性学习率算法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设有梯度和参数向量x</span></span><br><span class="line">cache += dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure></p><p>注意，变量<code>cache</code>跟踪了每个参数的梯度的平方和，用来归一化参数更新步长。<code>cache</code>使接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强。</p><p><img src="http://static.zybuluo.com/BYWMM/75lv5cjabd8sfc2zhsp6x11z/fksajghkljhkgasf.png" alt="fjagjalihg.png-215.3kB"></p><p>例如，这种情况下，纵向梯度值较高被，横向梯度较低，纵向梯度被削弱就会少走一些弯路。</p><p>eps（一般设为1e-4到1e-8之间）防止出现除以0的情况。</p><p><strong>缺点</strong>：随着梯度的累计，步长会越来越小，容易陷入极小点，在深度学习中单调的学习率被证明通常过于激进且过早停止学习。</p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop是Adagrad的一种变体。</p><p><img src="http://static.zybuluo.com/BYWMM/i5z1rkpnfvfgbyulw1ug3a56/image_1ct26t3uq1hqg1p171qcm1pt5bcn9.png" width="600"></p><p>RMSprop将<code>grad_squard</code>作为梯度平方的<strong>指数加权平均</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cache =  decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure><p>在上面的代码中，<code>decay_rate</code>是一个超参数，常用的值是[0.9,0.99,0.999]。</p><p>RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，同样效果不错。<strong>但是和Adagrad不同，其更新不会让学习率单调变小。</strong></p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam是最近才提出的一种更新方法，它看起来像是RMSProp的动量版。</p><p><strong>简易版Adam：</strong></p><p><img src="http://static.zybuluo.com/BYWMM/z69asbcmnoy55s6zeag1umpz/image_1ct28nnkfl1l1fi0lla1rb2aj1m.png" alt="image_1ct28nnkfl1l1fi0lla1rb2aj1m.png-327.3kB"></p><p>注意这个更新方法看起来是<strong>RMSProp+Momentum</strong>。<br>论文中推荐的参数值<code>eps</code>=1e-8，<code>beta1</code>=0.9，<code>beta2</code>=0.999<br>（<code>learning_rate</code>=1e-3或5e-4）。<br>（在实际操作中，推荐Adam作为默认的算法，一般而言跑起来比RMSProp要好一点。但是也可以试试SGD+Nesterov动量。）</p><p>完整的Adam算法包含一个<strong>偏置修正</strong>机制，因为<code>m</code>，<code>v</code>两个矩阵初始为0，指数加权平均中，初期前存在较大偏差：就算梯度很小，也很可能会产生很大的步长。</p><p><strong>完整版Adam：</strong></p><p><img src="http://static.zybuluo.com/BYWMM/bjjl6vahyf3e95wc0ev6sh3f/image_1ct2915981t551eqr1r6lurv2tq13.png" alt="image_1ct2915981t551eqr1r6lurv2tq13.png-520kB"></p>]]></content>
    
    <summary type="html">
    
      指数加权平均数；偏差修正；Momentum；Adagrad；RMSprop；Adam；
    
    </summary>
    
      <category term="机器学习" scheme="https://bywmm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="https://bywmm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="优化算法" scheme="https://bywmm.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>常用隐层激活函数</title>
    <link href="https://bywmm.github.io/2018/11/23/%E5%B8%B8%E7%94%A8%E9%9A%90%E5%B1%82%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>https://bywmm.github.io/2018/11/23/常用隐层激活函数/</id>
    <published>2018-11-23T05:12:16.000Z</published>
    <updated>2018-11-24T10:09:09.185Z</updated>
    
    <content type="html"><![CDATA[<h1 id="常用隐层激活函数"><a href="#常用隐层激活函数" class="headerlink" title="常用隐层激活函数"></a>常用隐层激活函数</h1><p>常见的<strong>隐藏单元</strong>激活函数如下：</p><p><img src="http://static.zybuluo.com/BYWMM/058atigazlzcezj92tcxaslb/manydjafljgkakl.png" alt="manydjafljgkakl.png-404.3kB"></p><h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><script type="math/tex; mode=display">\displaystyle\sigma(x)=\frac{1}{(1+e^{-x})}</script><p>Sigmoid将输入的实数值“挤压”到0到1范围内：</p><p><img src="http://static.zybuluo.com/BYWMM/gezg13xq4k7yfzkat1e1p1cx/abcdefghidmfalsmglang.png" width="300"></p><p>在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和（saturated）的激活（1）。</p><p>然而现在Sigmoid函数因为其两个主要缺点，实际已经很少使用了。</p><p><strong>缺点：</strong></p><ul><li><strong>Sigmoid函数饱和，使梯度消失</strong></li></ul><p>Sigmoid函数在其大部分定义域内都饱和（Z绝对值很大），只有输入接近0，它们才对输入强烈敏感。</p><p><img src="http://static.zybuluo.com/BYWMM/zda7tckyotomugcrkdihlimf/abcdkjglkaejrgakhgjoa.png" width="400"></p><p>如图，在反向传播的时候，$\frac{\partial{\sigma}}{\partial{x}}$将会与整个损失函数关于该门单元输出的梯度（即$\frac{\partial{L}}{\partial{\sigma}}$）相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。</p><ul><li><strong>Sigmoid函数的输出不是零中心的</strong></li></ul><p>这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在$f=w^Tx+b$中每个元素都$x&gt;0$），那么关于$w$的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式$f$而定）。这将会导致梯度下降权重更新时出现Z字型的下降。</p><p><strong>但Sigmoid常作为二值型的输出单元，将$z=\boldsymbol{w}^T\boldsymbol{h}+b$转化成概率。</strong></p><h2 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h2><p>注意tanh神经元是一个简单放大的sigmoid神经元，具体说来就是：</p><script type="math/tex; mode=display">\tanh(x)=2\sigma(2x)-1</script><p>tanh将实数值压缩到[-1,1]之间：</p><p><img src="http://static.zybuluo.com/BYWMM/k649lid4pc2eie9cr4xfg1nz/tanh.png" width="300"></p><p>和sigmoid神经元一样，它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎一些。</p><h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>极好的默认选择！！</p><script type="math/tex; mode=display">f(x)=max(0,x)</script><p>简单来说，ReLU就是一个关于0的阈值：</p><p><img src="http://static.zybuluo.com/BYWMM/gtkow3tvi0l15t1msfz7r30x/relu.png" width="300"></p><p>使用ReLU有以下一些优缺点：</p><p><strong>优点：</strong></p><ul><li><strong>收敛速度快</strong><br>相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用（ Krizhevsky 等的论文指出有6倍之多）。据称这是由它的线性，非饱和的公式导致的。</li><li><strong>计算速度快</strong><br>sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>在训练的时候，ReLU单元比较脆弱并且可能“died”。</strong><br>举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。<br>例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。<br><strong>当初始化参数的时候，可以将$b$设置成一个小的正值，例如0.1。</strong>这会使ReLU很可能初始化时就对训练集中的大多数输入呈现激活状态，并且允许导数通过。</li></ul><h2 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2><script type="math/tex; mode=display">f(x)=max(\alpha{x},x)</script><p>其中$\alpha$是一个小的常量，例如0.01。</p><p>Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x&lt;0时，函数值为0。而Leaky ReLU则是给出一个很小的负数梯度值，比如0.01。</p><p><img src="http://static.zybuluo.com/BYWMM/s8g7eb3o3rxq4z8f18ivvkpv/oifadjsklfjlasgfg.png" width="300"></p><blockquote><p>有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。<br>Kaiming He等人在2015年发布的论文<a href="http://link.zhihu.com/?target=http://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">Delving Deep into Rectifiers</a>中介绍了一种新方法<strong>PReLU</strong>，把负区间上的斜率当做每个神经元中的一个参数。然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰。</p></blockquote><h2 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h2><script type="math/tex; mode=display">\max(w^T_1x+b_1,w^T_2x+b_2)</script><p>Maxout是对ReLU和leaky ReLU的一般化归纳。</p><p>这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。</p><p>然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><strong>在同一个网络中混合使用不同类型的神经元是非常少见的</strong>，虽然没有什么根本性问题来禁止这样做。</p><p><strong>那么该用那种呢？</strong></p><p>用ReLU非线性函数。注意设置好学习率，监控网络中死亡的神经元占的比例；</p><p>如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout；</p><p>不要再用sigmoid了，也可以试试tanh，但是其效果应该不如ReLU或者Maxout。</p>]]></content>
    
    <summary type="html">
    
      Sigmoid；Tanh；ReLU；Leaky ReLU；Maxout；
    
    </summary>
    
      <category term="神经网络" scheme="https://bywmm.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://bywmm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="激活函数" scheme="https://bywmm.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>cs231n assignment1(Two_Layer_Net)</title>
    <link href="https://bywmm.github.io/2018/11/20/cs231n_assignment1(Two_Layer_Net)/"/>
    <id>https://bywmm.github.io/2018/11/20/cs231n_assignment1(Two_Layer_Net)/</id>
    <published>2018-11-20T11:39:52.000Z</published>
    <updated>2019-10-24T03:42:28.703Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment1-Two-Layer-Net"><a href="#cs231n-assignment1-Two-Layer-Net" class="headerlink" title="cs231n assignment1(Two_Layer_Net)"></a>cs231n assignment1(Two_Layer_Net)</h1><h2 id="两层网络（code）"><a href="#两层网络（code）" class="headerlink" title="两层网络（code）"></a>两层网络（code）</h2><p>首先来回顾一下我们的网络结结构：</p><p>输入层($D$个神经元)，全连接层-ReLu($H$)，softmax($C$)。<br>输入：$X [N\times{D}]$，输出 $y [N\times{1}]$<br>网络参数：$W1[D×H],b1[1×H],W2[H×C],b2[1×C]$</p><h3 id="Propagation"><a href="#Propagation" class="headerlink" title="Propagation"></a>Propagation</h3><script type="math/tex; mode=display">Z1=X⋅W1+b1</script><script type="math/tex; mode=display">A1=maximum(0,Z1)</script><script type="math/tex; mode=display">Z2=A1⋅W2+b2</script><script type="math/tex; mode=display">A2=softmax(Z2)</script><script type="math/tex; mode=display">P=A2</script><p>$Z2 \color{Blue}{[N\times{C}]}$就是每个样本在每个类别下的分数（score）<br>$A2 \color{Blue}{[N\times{C}]}$就是每个样本在每个类别下的概率（prob）</p><p>关键代码如下：<br>（Propagation中计算的值，Backpropogation的计算中要用到）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z1 = np.dot(X, W1) + b1</span><br><span class="line">A1 = np.maximum(<span class="number">0</span>, Z1)</span><br><span class="line">Z2 = np.dot(A1, W2) + b2</span><br><span class="line">scores = Z2</span><br><span class="line">scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">A2 = np.exp(scores) / np.sum(np.exp(scores), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) </span><br><span class="line">prob = A2</span><br></pre></td></tr></table></figure></p><h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h3><script type="math/tex; mode=display">L = \frac{1}{m}\sum_{i=1}^m-\log{P_{y_i}}+R(W)</script><p>关键代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最大似然项</span></span><br><span class="line">loss += -np.sum(np.log(prob[(range(N), y)])) / N</span><br><span class="line"><span class="comment"># 正则化项</span></span><br><span class="line">loss += reg * (np.sum(W1 * W1) + np.sum(W2 * W2))</span><br></pre></td></tr></table></figure></p><h3 id="Backpropogation"><a href="#Backpropogation" class="headerlink" title="Backpropogation"></a>Backpropogation</h3><p>反向传播的推导，主要还是依据链式法则，从后至前进行推导。</p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{Z2}}=A2−MaskMat\ \color{Blue}{[N×C]}</script><p>MaskMat参见<a href="https://bywmm.github.io/2018/11/14/cs231n_assignment1%28Softmax%29/#more">softmax_no_loop梯度推导</a></p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{W2}}=\frac{\partial{Z2}}{\partial{W2}}\cdot\frac{\partial{L}}{\partial{Z2}}=A1^T\cdot\frac{\partial{L}}{\partial{Z2}}</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{b2}}=\frac{\partial{Z2}}{\partial{b2}}\cdot\frac{\partial{L}}{\partial{Z2}}=[1...1]\ \color{Blue}{[1×H]}\cdot\frac{\partial{L}}{\partial{Z2}}</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{A1}}=\frac{\partial{L}}{\partial{Z2}}\cdot\frac{\partial{Z2}}{\partial{A1}}=\frac{\partial{L}}{\partial{Z2}}\cdot{W2^T}</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{Z1}}=\frac{\partial{L}}{\partial{A1}}\cdot\frac{\partial{A1}}{\partial{Z1}}=\left\{\begin{matrix}\frac{\partial{L}}{\partial{A1}}&if\ A1>0,\\ 0&otherwise.\end{matrix}\right.</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{W1}}=\frac{\partial{Z1}}{\partial{W1}}\cdot\frac{\partial{L}}{\partial{Z1}}=X^T\cdot\frac{\partial{L}}{\partial{Z1}}</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{b1}}=\frac{\partial{Z1}}{\partial{b1}}\cdot\frac{\partial{L}}{\partial{Z1}}=[1...1]\ \color{Blue}{[1×N]}\cdot\frac{\partial{L}}{\partial{Z1}}</script><p>关键代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># softmax函数求导</span></span><br><span class="line">prob[(range(N), y)] -= <span class="number">1</span></span><br><span class="line">dZ2 = prob / N</span><br><span class="line"></span><br><span class="line">dW2 = np.dot(A1.T, dZ2)</span><br><span class="line">db2 = np.ones((<span class="number">1</span>, A1.shape[<span class="number">0</span>])).dot(dZ2)</span><br><span class="line"></span><br><span class="line">dA1 = dZ2.dot(W2.T)</span><br><span class="line">dZ1 = dA1.copy()</span><br><span class="line">dZ1[A1 &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">dW1 = X.T.dot(dZ1)</span><br><span class="line">db1 = np.ones((<span class="number">1</span>, N)).dot(dZ1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># regulation</span></span><br><span class="line">dW1 += <span class="number">2</span> * reg * W1</span><br><span class="line">dW2 += <span class="number">2</span> * reg * W2</span><br></pre></td></tr></table></figure></p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="train函数"><a href="#train函数" class="headerlink" title="train函数"></a>train函数</h3><p>还是采用<strong>stochastic gradient descent (SGD)</strong>进行优化。</p><p>具体操作与前面的线性学习器相同。每次迭代随机一小部分样本进行梯度下降：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(num_iters):</span><br><span class="line">    index = np.random.choice(num_train, batch_size)</span><br><span class="line">    X_batch = X[index]</span><br><span class="line">    y_batch = y[index]</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><p>值得一提的是，随着优化的进行，我们离最小点越来越近，所以希望learning rate也越来越小，这样更可能不错过最小点的位置。<br>所以设置一个<strong>decay rate</strong>，每次Echo后与学习率相乘，使其相应变小。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">iterations_per_epoch = max(num_train / batch_size, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># Every epoch, decay learning rate.</span></span><br><span class="line"><span class="keyword">if</span> it % iterations_per_epoch == <span class="number">0</span>:</span><br><span class="line">    learning_rate *= learning_rate_decay</span><br></pre></td></tr></table></figure></p><h3 id="train-a-network"><a href="#train-a-network" class="headerlink" title="train a network"></a>train a network</h3><p>先用默认的参数，训练一个网络<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">input_size = <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span></span><br><span class="line">hidden_size = <span class="number">50</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line">net = TwoLayerNet(input_size, hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the network</span></span><br><span class="line">stats = net.train(X_train, y_train, X_val, y_val,</span><br><span class="line">            num_iters=<span class="number">1000</span>, batch_size=<span class="number">200</span>,</span><br><span class="line">            learning_rate=<span class="number">1e-4</span>, learning_rate_decay=<span class="number">0.95</span>,</span><br><span class="line">            reg=<span class="number">0.25</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict on the validation set</span></span><br><span class="line">val_acc = (net.predict(X_val) == y_val).mean()</span><br><span class="line">print(<span class="string">'Validation accuracy: '</span>, val_acc)</span><br></pre></td></tr></table></figure></p><p><img src="http://static.zybuluo.com/BYWMM/80jq2lwznnfjq9mde11kvhnh/ojgrhejsgkjlkfaf.png" width="320"></p><h3 id="debug-the-training"><a href="#debug-the-training" class="headerlink" title="debug the training"></a>debug the training</h3><p>可以看到我们用默认参数训练出的模型只有0.287的准确度。</p><p><strong>一种方法就是，画出优化中损失函数和准确度的变化</strong>（可能每隔100次梯度下降就记录一下，然后画出来），找一下问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the loss function and train / validation accuracies</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(stats[<span class="string">'loss_history'</span>])</span><br><span class="line">plt.title(<span class="string">'Loss history'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Iteration'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(stats[<span class="string">'train_acc_history'</span>], label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(stats[<span class="string">'val_acc_history'</span>], label=<span class="string">'val'</span>)</span><br><span class="line">plt.title(<span class="string">'Classification accuracy history'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Clasification accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>代码中的<code>loss_history</code>、<code>train_acc_history</code>、<code>val_acc_history</code>都是在模型训练中，每隔100次迭代记录下来的。目的就是为了这时候，可以将梯度下降的过程可视化。</p><p><img src="http://static.zybuluo.com/BYWMM/9w7bo1qx54pl5crn9lfzjsfb/RLVDUJVOWDOMV207MXC.png" width="500"></p><ul><li>损失曲线下降趋势有点直，问题应该是learning rate太小了。</li><li>train_acc和val_acc相差不大而且都挺低的，可能就是模型太小了，没有很好的拟合，应该增加模型的容量。</li><li>如果train_acc和val_acc相差挺大的话，可能是过拟合了。（当然这里没有）</li></ul><p><strong>另一个方法就是，把第一层的权重W1可视化。</strong>大多数网络中，W1可以看到一些直观的结构。</p><p>表现不好的$W1$：<br><img src="http://static.zybuluo.com/BYWMM/8sqeholdfyvze5g888l89ju4/zheshiyigebiaoxianbuhaodeW1.png" width="300"><br>表现较好的$W1$：<br><img src="http://static.zybuluo.com/BYWMM/j5gzymxys50d4e7ij6nu829i/zheshiyigebiaoxianhaibucuodeW1.png" width="300"><br>好吧，这个是要培养些直觉？</p><h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><p>和之前差不多，超参数有隐层神经元个数、学习率、正则化参数、梯度下降次数、dacayrate等等。</p><p>代码和之前差不多<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">best_net = <span class="keyword">None</span> <span class="comment"># store the best model into this </span></span><br><span class="line">best_acc = <span class="number">-1.0</span> <span class="comment"># store the best score</span></span><br><span class="line">best_stats = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">iput_size = <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">hidden_sizes = [<span class="number">300</span>]</span><br><span class="line">learning_rates = [<span class="number">1e-3</span>, <span class="number">1.2e-3</span>, <span class="number">1.4e-3</span>, <span class="number">1.6e-3</span>, <span class="number">1.8e-3</span>]</span><br><span class="line">regularization_strengths = [<span class="number">1e-4</span>, <span class="number">1e-3</span>, <span class="number">1e-2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hyperparameters = [(x, y, z) <span class="keyword">for</span> x <span class="keyword">in</span> hidden_sizes <span class="keyword">for</span> y <span class="keyword">in</span> learning_rates <span class="keyword">for</span> z <span class="keyword">in</span> regularization_strengths]</span><br><span class="line"><span class="keyword">for</span> hs, lr, reg <span class="keyword">in</span> hyperparameters:</span><br><span class="line">    print(<span class="string">'hs %e lr %e reg %e'</span> % (hs, lr, reg))</span><br><span class="line">    net_now = TwoLayerNet(iput_size, hs, num_classes)</span><br><span class="line">    <span class="comment"># Train the network</span></span><br><span class="line">    stats = net.train(X_train, y_train, X_val, y_val,</span><br><span class="line">                num_iters=<span class="number">2000</span>, batch_size=<span class="number">200</span>,</span><br><span class="line">                learning_rate=lr, learning_rate_decay=<span class="number">0.95</span>,</span><br><span class="line">                reg=reg, verbose=<span class="keyword">True</span>)</span><br><span class="line">    train_acc = (net.predict(X_train) == y_train).mean()</span><br><span class="line">    val_acc = (net.predict(X_val) == y_val).mean()</span><br><span class="line">    results[(hs, lr, reg)] = (train_acc, val_acc)</span><br><span class="line">    <span class="comment"># update the best net</span></span><br><span class="line">    <span class="keyword">if</span> val_acc &gt; best_acc:</span><br><span class="line">        best_net = net_now</span><br><span class="line">        best_acc = val_acc</span><br><span class="line">        best_stats = stats</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out results.        </span></span><br><span class="line"><span class="keyword">for</span> hs, lr, reg <span class="keyword">in</span> results:</span><br><span class="line">    train_acc, val_acc = results[(hs, lr, reg)]</span><br><span class="line">    print(<span class="string">'hs %e lr %e reg %e train accuracy: %f val accuracy: %f'</span> % (hs, lr, reg, train_acc, val_acc))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'best val accuracy is '</span>+ str(best_acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the loss function and train / validation accuracies</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(best_stats[<span class="string">'loss_history'</span>])</span><br><span class="line">plt.title(<span class="string">'Loss history'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Iteration'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(best_stats[<span class="string">'train_acc_history'</span>], label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(best_stats[<span class="string">'val_acc_history'</span>], label=<span class="string">'val'</span>)</span><br><span class="line">plt.title(<span class="string">'Classification accuracy history'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Clasification accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>结果<br><img src="http://static.zybuluo.com/BYWMM/ifywu12y2ifu2dlolobh25am/Y9BP4I664PODQL88.png" alt="fansdkgnjknheh.png-12.9kB"><br><img src="http://static.zybuluo.com/BYWMM/d9qht9lyqitvo28bi46hjja3/kgrklsgmldhdsf.png" alt="kgrklsgmldhdsf.png-33.7kB"></p>]]></content>
    
    <summary type="html">
    
      Backpropogation；learning decay rate；debug the training；
    
    </summary>
    
      <category term="cs231n" scheme="https://bywmm.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="https://bywmm.github.io/tags/cs231n/"/>
    
      <category term="计算机视觉" scheme="https://bywmm.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="https://bywmm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>cs231n assignment1(Softmax)</title>
    <link href="https://bywmm.github.io/2018/11/14/cs231n_assignment1(Softmax)/"/>
    <id>https://bywmm.github.io/2018/11/14/cs231n_assignment1(Softmax)/</id>
    <published>2018-11-14T07:38:29.000Z</published>
    <updated>2019-10-24T03:42:21.260Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment1-Softmax"><a href="#cs231n-assignment1-Softmax" class="headerlink" title="cs231n assignment1(Softmax)"></a>cs231n assignment1(Softmax)</h1><p>实验相关cs231n课程教程：<a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="noopener">Linear classification: Support Vector Machine, Softmax</a></p><p>这个实验是，用线性分类器<code>Softmax</code> ，在<code>CIFAR-10</code>数据集上做图像分类。</p><a id="more"></a><p>关于<code>CIFAR-10</code>数据集的介绍，移步这里。<br>（！！此处应该有个超链接！！）</p><p>上篇<code>SVM</code>实验介绍过线性模型，这里不再赘述。<a href="https://bywmm.github.io/2018/11/13/cs231n_assignment1%28SVM%29/#more">cs231n_assignment1(SVM)</a></p><h2 id="多分类Softmax的损失"><a href="#多分类Softmax的损失" class="headerlink" title="多分类Softmax的损失"></a>多分类Softmax的损失</h2><p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器</strong>，它的损失函数与SVM的损失函数不同。Softmax分类器就可以理解为<strong>逻辑回归分类器面对多个分类的一般化归纳</strong>。</p><h3 id="softmax函数及损失"><a href="#softmax函数及损失" class="headerlink" title="softmax函数及损失"></a>softmax函数及损失</h3><p>SVM将输出$f(x_i,W)$作为每个分类的评分。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释。在Softmax分类器中，函数映射$f(x_i;W)=Wx_i$保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<code>hinge loss</code>替换为<strong>交叉熵</strong>损失<code>cross-entropy loss</code>。公式如下：</p><script type="math/tex; mode=display">\displaystyle Li=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})</script><p>等价于下式</p><script type="math/tex; mode=display">L_i=-f_{y_i}+log(\sum_je^{f_j})</script><p>在上式中，使用$f_j$来表示分类评分向量$f$中的第$j$个元素，</p><p>其中$f_i(z)=\frac{e^{z_j}}{\sum_ke^{z_k}}$被称作<strong>softmax函数</strong>。可以看出通过这样处理之后，元素大小的单调性没有改变，每个元素被压缩到0~1之间，且所有元素的和为1。</p><p>数据集上的损失，和之前一样，就是数据集中所有样本数据的损失值$L_i$的均值与正则化损失$R(W)$之和。</p><script type="math/tex; mode=display">L=\frac{1}{m}L_i+\lambda{R}(W)</script><h3 id="信息理论解释"><a href="#信息理论解释" class="headerlink" title="信息理论解释"></a>信息理论解释</h3><p>在经验分布$p$和估计分布$q$之间的<strong>交叉熵</strong>定义如下：</p><script type="math/tex; mode=display">\displaystyle H(p,q)=-\sum_xp(x) logq(x)</script><p>因此，<code>Softmax</code>分类器所做的就是最小化在估计分类概率（就是上面的$\frac{e^{f_{y_i}}}{\sum_je^{f_j}}$）和经验分布之间的交叉熵。</p><p>交叉熵还可以写成熵和相对熵（Kullback-Leibler divergence）</p><script type="math/tex; mode=display">H(p,q)=H(p)+D_{KL}(p||q)</script><p>因为$H(p)$是固定的（我们是要优化$q$，使交叉熵最小），于是最小化<code>交叉熵</code>等价于最小化<code>相对熵</code>。即让$q$和$p$越相似越好，交叉熵最小化的终极目标，想让预测分布的所有概率密度都在正确分类上。</p><blockquote><p>信息论相关知识，可移步：<a href="https://bywmm.github.io/2018/11/13/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80/">信息论基础</a></p></blockquote><h3 id="概率论解释"><a href="#概率论解释" class="headerlink" title="概率论解释"></a>概率论解释</h3><p>先看下面的公式：</p><script type="math/tex; mode=display">P(y_i|x_i,W)=\frac{e^{f_{y_i}}}{\sum_je^{f_j}}</script><p>我们的<code>softmax函数</code>可看为$x_i$正确分类的后验概率。</p><p>从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE）。</p><p>该解释的另一个好处是，损失函数中的正则化部分R(W)可以被看做是权重矩阵W的高斯先验，这里进行的是最大后验估计（MAP）而不是最大似然估计。</p><blockquote><p>最大似然估计相关知识，可移步：<a href="https://bywmm.github.io/2018/10/29/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/">最大似然估计</a></p><p>最大后验概率相关知识，可移步：<a href>最大后验概率</a><br>(= = 欠一个超链接 = =)</p></blockquote><h3 id="实操事项：数值稳定"><a href="#实操事项：数值稳定" class="headerlink" title="实操事项：数值稳定"></a>实操事项：数值稳定</h3><p>编程实现softmax函数计算的时候，中间项$e^{f_{y_i}}$和$\sum_j e^{f_j}$因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数$C$，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p><script type="math/tex; mode=display">\frac{e^{f_{y_i}}}{\sum_je^{f_j}}=\frac{Ce^{f_{y_i}}}{C\sum_je^{f_j}}=\frac{e^{f_{y_i}+logC}}{\sum_je^{f_j+logC}}</script><p>$C$的值可自由选择，不会影响计算结果，通过使用这个技巧可以提高计算中的数值稳定性。通常将$C$设为 $logC=-max_jf_j$。代码实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure></p><blockquote><p>Softmax分类器的命名是从softmax函数那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。<br>注意从技术上说<code>“softmax loss”</code>是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p></blockquote><h2 id="损失的梯度推导"><a href="#损失的梯度推导" class="headerlink" title="损失的梯度推导"></a>损失的梯度推导</h2><p>有了SVM的梯度推导经验，Softmax应该不太难。</p><p>正则化向的导数</p><script type="math/tex; mode=display">\frac{\partial{R}}{\partial{W_{k,l}}}=2W_{k,l}</script><h3 id="with-loops梯度推导"><a href="#with-loops梯度推导" class="headerlink" title="with_loops梯度推导"></a>with_loops梯度推导</h3><p>前面每个样本的得分损失</p><script type="math/tex; mode=display">L_i=-x_iw_{y_i}^T+log(\sum_je^{x_iw_j^T})</script><p>前一项求导比较简单，只对$w_{y_i}^T$有贡献。</p><script type="math/tex; mode=display">\frac{\partial{L_{i1}}}{\partial{w^T_{y_i}}}=-x_i</script><p>后一项对$W$的每一项$w_k^T$都有贡献。通过链式求导可得出</p><script type="math/tex; mode=display">\frac{\partial{L_{i2}}}{\partial{w^T_{k}}}=\frac{x_i\times{e}^{x_iw_k^T}}{\sum_je^{x_iw_j^T}}</script><h3 id="no-loop梯度推导"><a href="#no-loop梯度推导" class="headerlink" title="no_loop梯度推导"></a>no_loop梯度推导</h3><p>上面求导数过程是把 Loss 对于 W 的导数显示的写出来，然后直接对 W 求导数，在这个简单的例子中可以这样，但是一旦网络变得复杂了，就很难直接写出Loss 对于要求的表达式的导数了。<br>一种比较好的方式是利用<strong>链式法则</strong>逐级的求导数：</p><p><strong>对于一个样本向量$x_i$</strong></p><script type="math/tex; mode=display">p_k=\frac{e^{f_k}}{\sum_je^{f_j}}</script><script type="math/tex; mode=display">L_i=-\log{p}_{y_i}=-f_{y_i}+\log\sum_{j}e^{f_j}</script><p>$L_i$对$f_j$的导数为</p><script type="math/tex; mode=display">\frac{\partial{L_i}}{\partial{f_k}}=p_k-1(k==y_i)</script><p>改写成向量形式，$L_i$对$f$的导数为</p><script type="math/tex; mode=display">\frac{\partial{L_i}}{\partial{f}}=p-[0...1...0](第y_i维为1)</script><p>由$f=x_iW$可已看出，$f$对$W$的导数为</p><script type="math/tex; mode=display">\frac{\partial{f}}{\partial{W}}=x_i^T</script><p>根据链式法则，可知</p><script type="math/tex; mode=display">\frac{\partial{L_i}}{\partial{W}}=\frac{\partial{L_i}}{\partial{f}}\cdot\frac{\partial{f}}{\partial{W}}</script><p><strong>m个样本的情况</strong></p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{f}}=p\ \color{Blue}{[N×C] }−MaskMat\ \color{Blue}{[N×C]}</script><script type="math/tex; mode=display">\frac{\partial{f}}{\partial{W}}=X^T\ \color{Blue}{[D×N]}</script><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{W}}=\frac{\partial{f}}{\partial{W}}\cdot\frac{\partial{L}}{\partial{f}}\ \color{Blue}{[D\times{C}]}</script><p>（其中MaskMat矩阵[i, y_i]位置为1，其他位置为0）</p><h2 id="计算损失-amp-梯度（code）"><a href="#计算损失-amp-梯度（code）" class="headerlink" title="计算损失&amp;梯度（code）"></a>计算损失&amp;梯度（code）</h2><h3 id="with-loops"><a href="#with-loops" class="headerlink" title="with_loops"></a>with_loops</h3><p>with_loops的公式上文已经推出，下面是实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs:</span></span><br><span class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">  - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns a tuple of:</span></span><br><span class="line"><span class="string">  - loss as single float</span></span><br><span class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros_like(W)</span><br><span class="line">  </span><br><span class="line">  num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</span><br><span class="line">      <span class="comment"># 初试分数</span></span><br><span class="line">      scores = np.dot(X[i], W)</span><br><span class="line">      <span class="comment"># 加一个常数C</span></span><br><span class="line">      scores -= np.max(scores)</span><br><span class="line">      <span class="comment"># softmax函数</span></span><br><span class="line">      prob = np.exp(scores) / np.sum(np.exp(scores))</span><br><span class="line">      <span class="comment"># 计算损失</span></span><br><span class="line">      loss += -np.log(prob[y[i]])</span><br><span class="line">      <span class="comment"># 前一项求导</span></span><br><span class="line">      dW[:,y[i]] -= X[i].T</span><br><span class="line">      <span class="comment"># 后一项求导</span></span><br><span class="line">      dW += np.dot(X[i].reshape(<span class="number">-1</span>,<span class="number">1</span>), prob.reshape(<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">  loss /= num_train</span><br><span class="line">  dW /= num_train</span><br><span class="line">  <span class="comment"># Regularization</span></span><br><span class="line">  loss += reg * np.sum(W * W)</span><br><span class="line">  dW += <span class="number">2</span> * reg * W</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h3 id="no-loop"><a href="#no-loop" class="headerlink" title="no_loop"></a>no_loop</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros_like(W)</span><br><span class="line"> </span><br><span class="line">  num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">  num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 初试分数</span></span><br><span class="line">  scores = np.dot(X, W)</span><br><span class="line">  <span class="comment"># 加一个常数C</span></span><br><span class="line">  scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">  <span class="comment"># softmax函数</span></span><br><span class="line">  prob = np.exp(scores) / np.sum(np.exp(scores), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">  <span class="comment"># 计算损失</span></span><br><span class="line">  loss += -np.sum(np.log(prob[(range(num_train), y)]))</span><br><span class="line">  <span class="comment"># 预处理：前一项+到后一项中</span></span><br><span class="line">  prob[(range(num_train),y)] += <span class="number">-1</span></span><br><span class="line">  <span class="comment"># 求梯度</span></span><br><span class="line">  dW += np.dot(X.T, prob)</span><br><span class="line"></span><br><span class="line">  loss /= num_train</span><br><span class="line">  dW /= num_train</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Regularization</span></span><br><span class="line">  loss += reg * np.sum(W * W)</span><br><span class="line">  dW += <span class="number">2</span> * reg * W</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h2 id="步长-amp-正则化常数调参"><a href="#步长-amp-正则化常数调参" class="headerlink" title="步长&amp;正则化常数调参"></a>步长&amp;正则化常数调参</h2><p>（这一部分和SVM几乎一样）</p><p>步长的范围是$[1e-7,5e-5]$，一般是等比增加的；<br>正则化的范围是$[2.5e4,5e4]$，这里采用等差增加。<br>最后在validation中的分数大概是0.349。（据说用心调可以到0.35左右）</p><p>代码不贴了，几乎一样的。</p><h2 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h2><p>SVM分类器使用的是折叶损失<code>hinge loss</code>;<br>softmax分类器使用的是交叉熵损失<code>corss-entropy loss</code>。</p><h3 id="分值解释不同"><a href="#分值解释不同" class="headerlink" title="分值解释不同"></a>分值解释不同</h3><p>下图是针对一个数据点，SVM和Softmax分类器的不同处理方式的一个例子。</p><p><img src="http://static.zybuluo.com/BYWMM/xv9alm7cnzx529syex5en40q/a90ce9e0ff533f3efee47473053820hd.png" alt="a90ce9e0ff533f3efee47473053820hd.png-112.4kB"></p><p>两个分类器都计算了同样的分值向量$f$（本节中是通过矩阵乘来实现）。不同之处在于对$f$中<strong>分值的解释</strong>：</p><ul><li>SVM分类器将它们看做是<strong>分类评分</strong>，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。</li><li>Softmax分类器将这些数值看做是每个分类<strong>没有归一化的对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。</li></ul><p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。</p><p>“可能性”分布的集中或离散程度是由正则化参数λ直接决定的。</p><p>举个例子，假设3个分类的原始分数是$[1,-2,0]$，那么<code>softmax函数</code>就会计算：</p><p>$[1,-2,0]\to[e^1,e^{-2},e^0]=[2.71,0.14,1]\to[0.7,0.04,0.26]$</p><p>现在，如果正则化参数λ更大，那么权重$W$就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧$[0.5,-1,0]$，那么softmax函数的计算就是：</p><p>$[0.5,-1,0]\to[e^{0.5},e^{-1},e^0]=[1.65,0.73,1]\to[0.55,0.12,0.33]$</p><p>现在看起来，概率的分布就更加分散了。</p><p>随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。</p><h3 id="优化方向不同"><a href="#优化方向不同" class="headerlink" title="优化方向不同"></a>优化方向不同</h3><p>考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。</p><ul><li>那么一个SVM（$\Delta=1$）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</li><li>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。</li><li>换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;cs231n-assignment1-Softmax&quot;&gt;&lt;a href=&quot;#cs231n-assignment1-Softmax&quot; class=&quot;headerlink&quot; title=&quot;cs231n assignment1(Softmax)&quot;&gt;&lt;/a&gt;cs231n assignment1(Softmax)&lt;/h1&gt;&lt;p&gt;实验相关cs231n课程教程：&lt;a href=&quot;http://cs231n.github.io/linear-classify/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linear classification: Support Vector Machine, Softmax&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个实验是，用线性分类器&lt;code&gt;Softmax&lt;/code&gt; ，在&lt;code&gt;CIFAR-10&lt;/code&gt;数据集上做图像分类。&lt;/p&gt;
    
    </summary>
    
      <category term="cs231n" scheme="https://bywmm.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="https://bywmm.github.io/tags/cs231n/"/>
    
      <category term="计算机视觉" scheme="https://bywmm.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
</feed>
